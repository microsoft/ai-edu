<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# Preface

**"What I cannot create, I do not understand." (American physicist Richard Feynman)**

In today's technologically advanced world, a lot of knowledge can be found on the Internet. However, the quality of blogs/articles is questionable, its focus is not clear, or it is directly copied from other people’s blogs. This situation makes it difficult for most beginners to learn. They may even be led astray, increasing the steepness of the learning curve. Of course, there are creators who are very responsible. The quality of their articles may be very high, but its scope is not enough. When you are enjoying yourself, there are no follow-up chapters and so it cannot be a good learning system.

Beginners can choose to read textbooks or books on theory, but the chicken and the egg problem arises: If you don’t understand, then after reading you still won’t understand the theory; if you understand, then you don’t need to read the theory. This is a shortcoming of some textbooks and theoretical books.

The author has seen professor Andrew Ng's (Wu Enda) classes. The theoretical knowledge is simple yet deep, and is very clear. Although there are very few code examples, I still strongly recommend looking into Andrew Ng's classes. The author's experience is that Andrew Ng's videos can be saved on a mobile device, and you can watch and learn on your own time.

There are external reasources that you can use to study deep learning online. The author took part in several group purchases. The teachers and teaching assistants are generally very responsible. Lastly, you can rewatch the videos and download the powerpoint course materials. These courses mainly focus on engineering projects, and explain the use of deep learning frameworks and tools. That is, teaching you how to use helpful tools for modeling, training, etc. But for beginners, understanding a new concept may require a lot of previously attained knowledge. An extremely steep learning curve can be frustrating. Maybe someone knows X but does't know why. They could end up as an assistant engineer, and their career development ends up being restricted.

还是应了那句古话：授人以鱼不如授人以渔。经历了以上那些学习经历，程序员出身的笔者迫切感觉到应该有一种新的学习体验，在“做中学”，用写代码的方式把一些基础的理论复现一遍，可以深刻理解其内涵，并能扩充其外延，使读者得到举一反三的泛化能力。

笔者总结了自身的学习经历后，把深度学习的入门知识归纳成了9个步骤，简称为9步学习法：

1. 基本概念
2. 线性回归
3. 线性分类
4. 非线性回归
5. 非线性分类
6. 模型的推理与部署
7. 深度神经网络
8. 卷积神经网络
9. 循环神经网络

笔者看到过的很多书籍是直接从第7步起步的，其基本假设是读者已经掌握了前面的知识。但是对于从零开始的初学者们，这种假设并不正确。

在后面的讲解中，我们一般会使用如下方式进行：

1. 提出问题：先提出一个与现实相关的假想问题，为了由浅入深，这些问题并不复杂，是实际的工程问题的简化版本。
2. 解决方案：用神经网络的知识解决这些问题，从最简单的模型开始，一步步到复杂的模型。
3. 原理分析：使用基本的物理学概念或者数学工具，理解神经网络的工作方式。
4. 可视化理解：可视化是学习新知识的重要手段，由于我们使用了简单案例，因此可以很方便地可视化。

原理分析和可视化理解也是本书的一个特点，试图让神经网络是可以解释的，而不是盲目地使用。

还有一个非常重要的地方，我们还有配套的Python代码，除了一些必要的科学计算库和绘图库，如NumPy和Matplotlib等，我们没有使用任何已有的深度学习框架，而是带领大家从零开始搭建自己的知识体系，从简单到复杂，一步步理解深度学习中的众多知识点。

对于没有Python经验的朋友来说，通过阅读示例代码，也可以起到帮助大家学习Python的作用，一举两得。随着问题的难度加深，代码也会增多，但是前后都有继承关系的，最后的代码会形成一个小的框架，笔者称之为Mini-Framework，可以用搭积木的方式调用其中的函数来搭建深度学习的组件。

这些代码都是由笔者亲自编写调试的，每章节都可以独立运行，得到相关章节内所描述的结果，包括打印输出和图形输出。

另外，为了便于理解，笔者绘制了大量的示意图，数量是同类书籍的10倍以上。一图顶万字，相信大家会通过这些示意图快速而深刻地理解笔者想要分享的知识点，使大家能够从真正的“零”开始，对神经网络、深度学习有基本的了解，并能动手实践。

对于读者的要求：

1. 学过高等数学中的线性代数与微分
2. 有编程基础，可以不会Python语言，因为可以从示例代码中学得
3. 思考 + 动手的学习模式

可以帮助读者达到的水平：

1. 可以判断哪些任务是机器学习可以实现的，哪些是科学幻想，不说外行话
2. 深刻了解神经网络和深度学习的基本理论
3. 培养举一反三的解决实际问题的能力
4. 得到自学更复杂模型和更高级内容的能力
5. 对于天资好的读者，可以培养研发新模型的能力

## 符号约定

|符号|含义|
|---|---|
|$x$|训练用样本值|
|$x_1$|第一个样本或样本的第一个特征值，在上下文中会有说明|
|$x_{12},x_{1,2}$|第1个样本的第2个特征值|
|$X$|训练用多样本矩阵|
|$y$|训练用样本标签值|
|$y_1$|第一个样本的标签值|
|$Y$|训练用多样本标签矩阵|
|$z$|线性运算的结果值|
|$Z$|线性运算的结果矩阵|
|$Z1$|第一层网络的线性运算结果矩阵|
|$\sigma$|激活函数|
|$a$|激活函数结果值|
|$A$|激活函数结果矩阵|
|$A1$|第一层网络的激活函数结果矩阵|
|$w$|权重参数值|
|$w_{12},w_{1,2}$|权重参数矩阵中的第1行第2列的权重值|
|$w1_{12},w1_{1,2}$|第一层网络的权重参数矩阵中的第1行第2列的权重值|
|$W$|权重参数矩阵|
|$W1$|第一层网络的权重参数矩阵|
|$b$|偏移参数值|
|$b_1$|偏移参数矩阵中的第1个偏移值|
|$b2_1$|第二层网络的偏移参数矩阵中的第1个偏移值|
|$B$|偏移参数矩阵（向量）|
|$B1$|第一层网络的偏移参数矩阵（向量）|
|$X^T$|X的转置矩阵|
|$X^{-1}$|X的逆矩阵|
|$loss,loss(w,b)$|单样本误差函数|
|$J, J(w,b)$|多样本损失函数|
