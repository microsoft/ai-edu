<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 3.1 Mean Square Error

MSE - Mean Square Error.

This function is the most intuitive loss function - calculating the Euclidean distance between predicted and real observations. The mean square error decreases with the approximation of the predicted value and actual value.

The mean square error function is often used for function fitting in linear regression. Its formula is as follows:

$$
loss = {1 \over 2}(z-y)^2 \tag{Single examples}
$$

$$
J=\frac{1}{2m} \sum_{i=1}^m (z_i-y_i)^2 \tag{Multi examples}
$$

### 3.1.1 Working principle

The simplest way to get the difference between the predicted value $a$ and real value $y$ is to use $Error=a_i-y_i$.

This works for single examples, but when multiple samples are accumulated, $a_i-y_i$ may be positive or negative and cancel out during summation calculation. The idea of absolute difference, namely $Error=|a_i-y_i|$, seems like a simple and ideal solution to this problem. So, why introduce a mean square error function? A comparison of these two loss functions is shown in Table 3-1.

Table 3-1 A comparison of absolute error function and mean square error function

|Label of example|Predicted value of example|Absolute error function|MSE function|  
|------|------|------|------|
|$[1,1,1]$|$[1,2,3]$|$(1-1)+(2-1)+(3-1)=3$|$(1-1)^2+(2-1)^2+(3-1)^2=5$|
|$[1,1,1]$|$[1,3,3]$|$(1-1)+(3-1)+(3-1)=4$|$(1-1)^2+(3-1)^2+(3-1)^2=8$|
|||$4/3=1.33$|$8/5=1.6$|

As shown above, the MSE loss of 5 is higher than the absolute error of 3, and 8 is twice as large as 4. The MSE loss of 8 also magnifies the impact of the local loss of a sample in overall training compared to 5. In technical terms, MSE is “Sensitive to samples with large deviation”, which draws attention to the monitoring training process to help backpropagate errors.

### 3.1.2 Practical cases

Assuming a set of data like Figure 3-3, we are looking for a line of best fit.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/mse1.png" ch="500" />

Figure 3-3 The scatter plot of sample data distribution

The first three images in Figure 3-4 show a gradual process of finding the line of best fit.

- In the first plot, calculated with the mean square error function, $loss = 0.53 $;  
- In the second plot, the line of best fit shifted upward, and the error is calculated as $loss = 0.16 $, which is much lower than the error shown in Figure 1;  
- In the third plot, the line shifted further upward, and the error is calculated as $loss = 0.048 $. You can then try either shifting the line(adjust the value of $b $) or rotating the line(change the value of $W $) to get a lower loss value;  
- In the fourth plot, the line of best fit deviates from the optimal position, and the error value is $loss = 0.18 $. In this case, the algorithm will attempt to shift downward.  

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/mse2.png" ch="500" />

Fig. 3-4 The relationship between the loss value and the position of the fitting line

The third diagram shows the case where the loss is minimal. Comparing the loss value in the second and fourth graphs, how do we decide whether to shift upward or downward when the MSE loss are both positive?

It is unnecessary to calculate the loss value in the actual training process since the loss will be reflected in the back-propagation process. Let’s look at the derivative of the mean square error function:

$$
\frac{\partial{J}}{\partial{a_i}} = a_i-y_i
$$

Although $(a_i-y_i)^2$ is always a positive number,  $a_i-y_i$ can be either positive (when the line is below the points) or negative (when the line is above the points). This positive or negative number is propagated back to the previous calculation, which will guide the training process in the right direction.

In the example above, we have two variables, $W and b $, whose changes affect the final loss value.

Assume the equation for the line of best fit is $y = 2x + 3 $. When we vary the value of $b$ from $2$ to $4$ and fix $W = 2$, the changes of the loss is shown in figure 3-5.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/LossWithB.png" ch="500" />

Figure 3-5 Changes in the loss value when $W$ is fixed and $b$ is changing

Assume the equation for the line of best fit is $y = 2x + 3 $. When we vary $w$ from $1$ to $3$ and fix $b = 3$, the changes of the loss is shown in Figure 3-6.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/LossWithW.png" ch="500" />

Figure 3-5 Changes in the loss value when $b$ is fixed and $W$ is changing

### 3.1.3 Loss Function Visualization

####  3D representation of loss value

The x-axis is $w$, and the y-axis is $b$. For each combination of $(W, B)$, a loss value represented by the height of the three-dimensional graph is computed. As shown in Figure 3-7 below, the bottom of the graph is not a plane but rather a concave curve surface with low curvature.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/lossfunction3d.png" ch="500" />  

Figure 3-7 A 3D loss curve formed by the simultaneous variation of $W and $b$

####  2D representation of loss value

We often use contour lines to represent altitude on a plane map. The figure below is the projection of Figure 3-7 on a plane, which is the contour map of loss value, as shown in Figure 3-8.
 
<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/lossfunction_contour.png" ch="500" />

Figure 3-8 Contour map of loss value

If that doesn’t make sense, we’ll draw a diagram using the following code:

```Python
    s = 200
    W = np.linspace(w-2,w+2,s)
    B = np.linspace(b-2,b+2,s)
    LOSS = np.zeros((s,s))
    for i in range(len(W)):
        for j in range(len(B)):
            z = W[i] * x + B[j]
            loss = CostFunction(x,y,z,m)
            LOSS[i,j] = round(loss, 2)
```

The above code calculates a LOSS value for each combination of $(W, B)$, places in the ‘LOSS’ matrix，keeping 2 decimal places. The matrix is shown as follows:

```
[[4.69 4.63 4.57 ... 0.72 0.74 0.76]
 [4.66 4.6  4.54 ... 0.73 0.75 0.77]
 [4.62 4.56 4.5  ... 0.73 0.75 0.77]
 ...
 [0.7  0.68 0.66 ... 4.57 4.63 4.69]
 [0.69 0.67 0.65 ... 4.6  4.66 4.72]
 [0.68 0.66 0.64 ... 4.63 4.69 4.75]]
```

We traverse the loss values in the matrix and color in points with the same value with the same color. For example, points with a value of 0.72 are drawn in as red and points with a value of 0.75 are drawn in as blue... The resulting diagram is shown in Figure 3-9.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/lossfunction2d.png" ch="500" />

Figure 3-9 A simple way of plotting contour graphs

This diagram is equivalent to the contour plot, but since the contour plot is concise and clear enough, we will use the contour plot to illustrate the problem.

### Code location

ch03, Level1
