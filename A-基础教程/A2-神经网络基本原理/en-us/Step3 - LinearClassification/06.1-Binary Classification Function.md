<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 6.1 Binary classification function

This function is suitable for both linear and non-linear binary classification.

### 6.1.1 Binary classification function

The logistic function can be used as an activation function or as a binary classification Function. In many informal texts, these two concepts are mixed up, such as the following statement: "We use the Sigmoid activation function at the end to do two classifications." which is inappropriate. This book will distinguish between the activation function, and the classification function according to the different tasks. In the binary classification task, this is called the Logistic function, and when used as an activation function, it is called the Sigmoid function.

- Logistic function equation

$$Logistic(z) = \frac{1}{1 + e^{-z}}$$

denoted as $a=Logistic(z)$。

- Derivative

$$Logistic'(z) = a(1 - a)$$

The specific derivation process can be found in section 8.1.

- Input range

$$(-\infty, \infty)$$

- Output Range 

$$(0,1)$$

- Function plot（Figure6-2）

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/logistic.png" ch="500" />

Figure6-2 Logistic function plot

- How to use

This function is simply a measure of probability. It compresses every number between $(-\infty, \infty)$ to $(0,1)$, and returns a probability value. When the value is close to $1$, it is regarded as a positive example; Otherwise, it is considered a negative example.

During training, a sample $x$ is used as input $z$ after matrix operation through the last layer of the neural network. A prediction value between $(0,1)$ is output after Logistic calculation. We assume that the label value of this sample $0$ belongs to the negative category. If the predicted value is closer to $0$, it approaches the label value, representing the smaller error and the lower intensity of the backpropagation.

In the process of inferring, we preset a threshold such as $0.5$, which is considered positive if the result is greater than $0.5$, negative if the output is less than $0.5$; when it is equal to $0.5$, classify it according to the situation. The threshold is not necessarily $0.5$, it can also be $0.65$ or other values. The larger the threshold is, the higher the accuracy and the lower the recall rate are; In contrast, the smaller threshold will cause lower accuracy and increase the recall rate.

For example:

- when input=2，output=0.88，0.88>0.5，considered as a positive case
- when input=-1，output=0.27，0.27<0.5，considered as a negative case

### 6.1.2 Feed-forward propagation

#### Matrix operations

$$
z=x \cdot w + b \tag{1}
$$

#### Classification calculation

$$
a = Logistic(z)=\frac{1}{1 + e^{-z}} \tag{2}
$$

#### Loss function calculation

Binary classification Cross-entropy loss function:

$$
loss(w,b) = -[y \ln a+(1-y) \ln(1-a)] \tag{3}
$$

### 6.1.3 Back Propagation

#### Calculate the partial derivatives of the loss function with respect to $a$

$$
\frac{\partial loss}{\partial a}=-\left[\frac{y}{a}-\frac{1-y}{1-a}\right]=\frac{a-y}{a(1-a)} \tag{4}
$$

#### Calculate the partial derivatives of $a$ with respect to $z$

$$
\frac{\partial a}{\partial z}= a(1-a) \tag{5}
$$

#### Calculate the partial derivatives of $loss$ with respect to $z$

apply Chain Rule to Eq4 and Eq5:

$$
\frac{\partial loss}{\partial z}=\frac{\partial loss}{\partial a}\frac{\partial a}{\partial z}=\frac{a-y}{a(1-a)} \cdot a(1-a)=a-y \tag{6}
$$

We were surprised to discover that the denominator obtained by the derivatives of the cross-entropy function exactly offset with the derivatives of the Logistic classification function, and leaving only the term $a-y $. Is there such a coincidence? It actually relies on the ingenuity of scientists to find this matching relationship, which fulfilled the following conditions:

1. The loss function meets the requirements of binary classification; both positive and negative examples are monotonous
2. The loss function is differentiable to facilitate the use of backpropagation algorithm;
3. The computational procedure is straightforward, can be done with a simple subtraction.

#### Multi-sample cases

We instantiate the deduction with three samples:

$$Z=
\begin{pmatrix}
  z_1 \\\\ z_2 \\\\ z_3
\end{pmatrix},
A=Logistic\begin{pmatrix}
  z_1 \\\\ z_2 \\\\ z_3
\end{pmatrix}=
\begin{pmatrix}
  a_1 \\\\ a_2 \\\\ a_3
\end{pmatrix}
$$

$$
\begin{aligned}
J(w,b)=&-[y_1 \ln a_1+(1-y_1)\ln(1-a_1)]  \\\\
&-[y_2 \ln a_2+(1-y_2)\ln(1-a_2)]  \\\\
&-[y_3 \ln a_3+(1-y_3)\ln(1-a_3)]  \\\\
\end{aligned}
$$
Apply the result of Formula 6: 
$$ 
\frac{\partial J(w,b)}{\partial Z}=
\begin{pmatrix}
  \frac{\partial J(w,b)}{\partial z_1} \\\\
  \frac{\partial J(w,b)}{\partial z_2} \\\\
  \frac{\partial J(w,b)}{\partial z_3}
\end{pmatrix}
=\begin{pmatrix}
  a_1-y_1 \\\\
  a_2-y_2 \\\\
  a_3-y_3 
\end{pmatrix}=A-Y
$$

Therefore, the matrix operation can be simplified to the form of matrix subtraction：$A-Y$.

### 6.1.4 The origin of the log odds

After the mathematical deduction, it is clear that the neural network actually does the same thing: after adjusting the values of $w$ and $b$, all samples of positive cases are grouped into a range greater than $0.5$, and all negative cases are less than $0.5$. However, if we just say it is greater than or less than, we cannot do an accurate quantitative calculation, so we use a log-odds function to simulate it.

One more question about the logistic function is why it's called the "log-odds" probability function?

Let's take an example: Suppose a coin is tossed and the probability of getting heads is $0.5$, and for tails is $0.5$. If you divide the probability of heads by the probability of tails, $0.5/0.5=1$, this value is called $odds$, which is the probability.

To generalize, if the probability of a head is $a$, then the probability of a tail is $1-a$, and the probability is equal to:

$$odds = \frac{a}{1-a} \tag{9}$$

In the above formula, if $a$ is the probability that predicts sample $x$ as a positive, then $1-a$ is the probability of its negative example. Accordingly,  $\frac{a}{1-a}$ is the ratio of positive to negative, called odds (odds), which reflects the relative probability of $x$ as a positive case. Computing the logarithm of the odds is called log-odds (logit).

Suppose the probabilities are as shown in table 6-3.

Table 6-3 A comparison table of probability and log-odds

|Probability$a$|0|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1|
|--|--|--|--|--|--|--|--|--|--|--|--|
|Inverse probability$1-a$|1|0.9|0.8|0.7|0.6|0.5|0.4|0.3|0.2|0.1|0|
|Odds $odds$ |0|0.11|0.25|0.43|0.67|1|1.5|2.33|4|9|$\infty$|
|Log-odds $\ln(odds)$|N/A|-2.19|-1.38|-0.84|-0.4|0|0.4|0.84|1.38|2.19|N/A|

It can be seen that the value of probability is not linear, which is not conducive to analyzing the problem. Therefore, taking the logarithm of the odds in the fourth row of the table to obtain a value that constitutes a linear relationship, which can be represented by a linear equation, namely:
$$
\ln(odds) = \ln \frac{a}{1-a}= xw + b \tag{10}
$$

Taking the natural exponent for both sides of Equation 10.

$$
\frac{a}{1-a}=e^{xw+b} \tag{11}
$$

$$
a=\frac{1}{1+e^{-(xw+b)}}
$$

Let $z=xw+b$：

$$
a=\frac{1}{1+e^{-z}} \tag{12}
$$

Formula 12 is Formula 2! The function of log-odds can be considered to be obtained in this way.

The above deduction process is actually using the prediction result of the linear regression model to approximate the log-odds of the sample classification. This is why it is called logistic regression, but it is actually a classification learning method. The advantages of this method are as follows:

- Introducing the successful experience of linear regression into the classification problem. It is equivalent to modelling the "boundary line" prediction, which is a straight line in two-dimensional space, without considering the distribution of specific samples (e.g., coordinates in two-dimensional space). This avoids the problem of inaccurate assumptions about the distribution;
- Not only predicts the category (0/1) but also yields an approximate probability value (such as 0.31 or 0.86), which is helpful for many tasks that require probability to assist decision-making;
- The log-odds function is a derivable convex function of any order. It has good mathematical properties and can be used directly by many numerical optimization algorithms to obtain the optimal solution.