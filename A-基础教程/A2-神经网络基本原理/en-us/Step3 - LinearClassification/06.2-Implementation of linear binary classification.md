<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->
  
## 6.2 Linear binary classification with neural network

Let's first look at using a neural network to draw a clear dividing line between two sets of samples with different labels. The line can be a straight line or a curve. This is the binary classification problem. If we only draw a dividing line, whether it is a straight line or a curve, we can use an imaginary pen (a neuron) to achieve the goal. To put it another way, the direction of the pen depends entirely on this one neuron judging from the input signal.


Looking at the diagram of Chu-Han city again, there seems to be a straight dividing line between the two colour regions, i.e., linearly separable.

1. We can use a single-layer neural network if the diagram is visually linearly separable 
2. The input features are longitude and latitude, so we set two inputs in the input layer. Where $x_1=$longitude, $x_2=$latitude;
3. The final output is a binary classification result, which is Chu-Han city and can be regarded as a binary classification problem in which the outcome is either 0 or 1. Therefore, we only need one output unit.

### 6.2.1 Define the neural network structure

Based on the previous guesses, it seems that we only need a neuron with two inputs and one output. This network has only input and output layers, and since the input layer is not counted, it is a one-layer network, see Figure 6-3.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/BinaryClassifierNN.png" ch="500" />

Figure 6-3 The neuron structure of the binary classification task

The difference from the network diagram in the previous chapter is that we used the classification function for the neuron output, so the result is $a$ instead of the $z$.

#### Input layer

Input two features: Longitude $X_1$ and latitude $X_2$,

$$
X=\begin{pmatrix}
x_{1} & x_{2}
\end{pmatrix}
$$

#### Weighting Matrix

If input is two features, and the output is a number, the size of $W$ is $2\times 1$:

$$
W=\begin{pmatrix}
w_{1} \\\\ w_{2}
\end{pmatrix}
$$

The size of $B$ is $1\times 1$, the number of rows is always 1, and the number of columns is constantly the same as $W$.

$$
B=\begin{pmatrix}
b
\end{pmatrix}
$$

#### Output layer

$$
\begin{aligned}    
z &= X \cdot W + B
=\begin{pmatrix}
    x_1 & x_2
\end{pmatrix}
\begin{pmatrix}
    w_1 \\\\ w_2
\end{pmatrix} + b \\\\
&=x_1 \cdot w_1 + x_2 \cdot w_2 + b 
\end{aligned}
\tag{1}
$$
$$a = Logistic(z) \tag{2}$$

#### Loss function

Binary classification cross-entropy loss function:

$$
loss(W,B) = -[y\ln a+(1-y)\ln(1-a)] \tag{3}
$$

### 6.2.2 Back-propagation

We have derived the partial derivative of $loss$ with respect to $z$ in Section 6.1, and the result is $A-Y$. Next, we find the derivative of $loss$ with respect to $W$. In this example, the form of $W$ is a vector with 2 rows and 1 column. Therefore, when seeking the partial derivative of $W$, we need to calculate the derivative of the vector:

$$
\frac{\partial loss}{\partial w}=
\begin{pmatrix}
    \frac{\partial loss}{\partial w_1} \\\\ 
    \frac{\partial loss}{\partial w_2}
\end{pmatrix}
$$
$$
=\begin{pmatrix}
 \frac{\partial loss}{\partial z}\frac{\partial z}{\partial w_1} \\\\
 \frac{\partial loss}{\partial z}\frac{\partial z}{\partial w_2}   
\end{pmatrix}
=\begin{pmatrix}
    (a-y)x_1 \\\\
    (a-y)x_2
\end{pmatrix}
$$
$$
=(x_1 \ x_2)^{\top} (a-y) \tag{4}
$$

In the above formula, $x_1,x_2$ are two features of a sample. In multiple samples, formula 4 will become its matrix form, taking 3 samples as an example:

$$
\frac{\partial J(W,B)}{\partial W}=
\begin{pmatrix}
    x_{11} & x_{12} \\\\
    x_{21} & x_{22} \\\\
    x_{31} & x_{32} 
\end{pmatrix}^{\top}
\begin{pmatrix}
    a_1-y_1 \\\\
    a_2-y_2 \\\\
    a_3-y_3 
\end{pmatrix}
=X^{\top}(A-Y) \tag{5}
$$

### 6.2.3 Code implementation

We can copy some classes that have been written in the `HelperClass5` in Chapter 5, which will meet our needs with a few changes.

Since our neural network can only perform regression tasks before, we now have one more skill, classification.  Therefore, we add an enumeration that allows the caller to control the functionalities of the neural network by specifying the parameter.

```Python
class NetType(Enum):
    Fitting = 1,
    BinaryClassifier = 2,
    MultipleClassifier = 3,
```

Then add this new parameter to the initialization function in the HyperParameters class:

```Python
class HyperParameters(object):
    def __init__(self, eta=0.1, max_epoch=1000, batch_size=5, eps=0.1, net_type=NetType.Fitting):
        self.eta = eta
        self.max_epoch = max_epoch
        self.batch_size = batch_size
        self.eps = eps
        self.net_type = net_type
```

Next,add a `Logistic` classification function:

```Python
class Logistic(object):
    def forward(self, z):
        a = 1.0 / (1.0 + np.exp(-z))
        return a
```

There was only a mean-square-error function, and now we’ve added a cross-entropy function. We created a new class to manage the functions efficiently:

```Python
class LossFunction(object):
    def __init__(self, net_type):
        self.net_type = net_type
    # end def

    def MSE(self, A, Y, count):
        ...

    # for binary classifier
    def CE2(self, A, Y, count):
        ...
```
The above class determines when to call the mean square error function (MSE) and when to call the cross-entropy function (CE2) through the network type during initialization.

Next, modify the feed-forward calculation function in the `NeuralNet` class, and determine whether to call the `Logistic` classification function after linear transformation by judging the current network type:

```Python
class NeuralNet(object):
    def __init__(self, params, input_size, output_size):
        self.params = params
        self.W = np.zeros((input_size, output_size))
        self.B = np.zeros((1, output_size))

    def __forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.W) + self.B
        if self.params.net_type == NetType.BinaryClassifier:
            A = Sigmoid().forward(Z)
            return A
        else:
            return Z
```

Finally, the main process: 

```Python
if __name__ == '__main__':
    ......
    params = HyperParameters(eta=0.1, max_epoch=100, batch_size=10, eps=1e-3, net_type=NetType.BinaryClassifier)
    ......
```

The difference is that we set the network type in the hyperparameter to be `BinaryClassifier.`


### 6.2.4 Result

The loss function values shown in Figure 6-4 record a very smooth decline, indicating that the network converged.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/binary_loss.png" ch="500" />

Figure 6-4 Change of loss values during training

Printout of the last few lines:

```
......
99 19 0.20742586902509108
W= [[-7.66469954]
 [ 3.15772116]]
B= [[2.19442993]]
A= [[0.65791301]
 [0.30556477]
 [0.53019727]]
```

1. When the relative value of latitude and longitude is (0.58,0.92), the probability is 0.65, the city belongs to Han;
2. When the relative value of latitude and longitude is (0.62, 0.55), the probability is 0.30, which belongs to Chu;
3. When the relative value of latitude and longitude is (0.39, 0.29), the probability is 0.53, which belongs to Han.

The classification method is to specify that when $A>0.5$ the example is positive, and when $A\leq 0.5$ it is a negative sample. Sometimes the ratio of positive and negative examples is different, or there are special requirements. You can also use a number other than $0.5$ as the threshold.
   
### Code location

ch06, Level1
