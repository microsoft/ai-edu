<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 6.5 Implementing logic AND OR NOT gates

Single-layer neural networks, also called perceptron, can easily implement logic AND, OR, and NOT gates. As the logic AND and OR gates need to have two variable inputs, while the logic NOT gates have only one variable input. However, their common feature is that the input is 0 or 1, which can be considered two categories of positive and negative.

Therefore, after learning the binary classification, we can use the idea of classification to implement the following 5 logic gates:

- AND gate
- NAND gate
- OR gate
- NOR gate
- NOT gate
 
Taking the logic AND as an example, the four dots in Figure 6-12 represent the four sample data. The blue dots indicate the negative cases ($y=0$), and the red triangles represent the positive cases ($y=1$).

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/LogicAndGateData.png" ch="500" />

Figure 6-12 Multiple dividing lines that can solve logic AND problems

Suppose we use the classification idea, according to what we learned earlier, we should draw a dividing line between the red and blue points, which can precisely separate the positive cases from the negative ones. Since the sample data is sparse, the angle and position of this split line can be relatively arbitrary, such as the three straight lines in the figure, which can all be the solution to this problem. Let's see if the neural network can give us a surprise.

### 6.5.1 Implementing logic NOT gate

Many reading materials will introduce the model $y=wx+b$ such that $w=-1,b=1$, then:

- When $x=0$，$y = -1 \times 0 + 1 = 1$
- When $x=1$，$y = -1 \times 1 + 1 = 0$

Thus, there is a neuronal structure as shown in Figure 6-13.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/LogicNot.png" width="400"/>

Figure 6-13 Neuron implementation of incorrect logic NOT gate

However, this becomes a fitting problem rather than a classification problem. For example, let $x=0.5$, substituting into the formula:

$$
y=wx+b = -1 \times 0.5 + 1 = 0.5
$$

That is, when $x=0.5$, $y=0.5$, and the values of $x$ and $y$ do not have the meaning of "NOT". Therefore, the neuron shown in Figure 6-14 should be defined to solve the problem, and the sample data is also straightforward, as shown in Table 6-6, with only two rows of data.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/LogicNot2.png" width="500" />

Figure 6-14 Neuron implementation of the correct logic NOT gate

Table 6-6 Sample data for the logic NOT problems

|Sample number|Sample value$x$|Label value$y$|
|:---:|:---:|:---:|
|1|0|1|
|2|1|0|

The code for establishing the sample data is as follows:

```Python
    def Read_Logic_NOT_Data(self):
        X = np.array([0,1]).reshape(2,1)
        Y = np.array([1,0]).reshape(2,1)
        self.XTrain = self.XRaw = X
        self.YTrain = self.YRaw = Y
        self.num_train = self.XRaw.shape[0]
```

In the main program, let:
```Python
num_input = 1
num_output = 1
```
Execute the training process and end up with the classification results shown in Figure 6-16 and the printout below.

```
......
2514 1 0.0020001369266925305
2515 1 0.0019993382569061806
W= [[-12.46886021]]
B= [[6.03109791]]
[[0.99760291]
 [0.00159743]]
```

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/LogicNotResult.png" width="400" />

Figure 6-15 Classification results of the logic NOT gate

From Figure 6-15, it can be understood that the neural network draws a straight line between the left and right sample points to separate the two classes of samples, and the equation of this line is the line represented by the W and B values in the printout.

$$
y = -12.468x + 6.031
$$

The result shows that this line is not perpendicular to the $x$-axis, but slightly "skewed". This reflects the limitations of the capabilities of the neural network, which only "simulates" a result but cannot accurately obtain a perfect mathematical formula. The precise mathematical formula for this problem is a vertical line, equivalent to $w=\infty$, which is impossible to train.

### 6.5.2 Implementing logic AND,OR gates

#### The neuron model

The neuron model from Section 6.2 is still used, as in Figure 6-16.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/BinaryClassifierNN.png" ch="500" />

Figure 6-16 Neuron implementation of logic AND OR gate

Since there are only two input feature values and one binary output, the model is the same as in the previous section.

#### Training samples

Each type of logic gate has only four training samples, as shown in Table 6-7.

Table 6-7 Sample and label data for four types of logic gates

|Sample|$x_1$|$x_2$|Logic AND $y$|Logic NAND $y$|Logic OR $y$|Logic NOR $y$|
|:---:|:--:|:--:|:--:|:--:|:--:|:--:|
|1|0|0|0|1|0|1|
|2|0|1|0|1|1|0|
|3|1|0|0|1|1|0|
|4|1|1|1|0|1|0|

#### Retrieving data
  
```Python
class LogicDataReader(SimpleDataReader):
    def Read_Logic_AND_Data(self):
        X = np.array([0,0,0,1,1,0,1,1]).reshape(4,2)
        Y = np.array([0,0,0,1]).reshape(4,1)
        self.XTrain = self.XRaw = X
        self.YTrain = self.YRaw = Y
        self.num_train = self.XRaw.shape[0]

    def Read_Logic_NAND_Data(self):
        ......

    def Read_Logic_OR_Data(self):
        ......

    def Read_Logic_NOR_Data(self):        
        ......
```

Taking the logic AND as an example, we derive our own class `LogicDataReader` from `SimpleDataReader` and add a specific data reading method `Read_Logic_AND_Data()`. Several other logic gates have similar approaches, and only the method names are listed here.

#### The Test Function

```Python
def Test(net, reader):
    X,Y = reader.GetWholeTrainSamples()
    A = net.inference(X)
    print(A)
    diff = np.abs(A-Y)
    result = np.where(diff < 1e-2, True, False)
    if result.sum() == 4:
        return True
    else:
        return False
```

We know that a neural network can only give approximate solutions, but the extent to which this "approximation" can be made is something that we need to specify during training. For example, if the input is $(1, 1)$, the result of AND is $1$. However, the neural network can only give a probability value of $0.721$, which is not sufficient for the accuracy requirement, and the error must be less than `1e-2` for all 4 samples.

#### The Train function

```Python
def train(reader, title):
    ...
    params = HyperParameters(eta=0.5, max_epoch=10000, batch_size=1, eps=2e-3, net_type=NetType.BinaryClassifier)
    num_input = 2
    num_output = 1
    net = NeuralNet(params, num_input, num_output)
    net.train(reader, checkpoint=1)
    # test
    print(Test(net, reader))
    ......
```
A maximum of `epoch` of 10,000 times, a learning rate of 0.5, and a stopping condition when the value of the loss function is as low as `2e-3` are specified in the hyperparameter. At the end of the training, the test function must be called first, and `True` needs to be returned to meet the requirements. The classification results are displayed graphically.

#### Compiling results

The printout of the results of the logic AND is as follows:

```
......
epoch=4236
4236 3 0.0019998012999365928
W= [[11.75750515]
 [11.75780362]]
B= [[-17.80473354]]
[[9.96700157e-01]
 [2.35953140e-03]
 [1.85140939e-08]
 [2.35882891e-03]]
True
```
The precision $loss<1e-2$ is achieved after 4236 iterations. When four combinations of $(1,1), (1,0), (0,1), and (0,0)$ are input, all the outputs meet the accuracy requirements.

### 6.5.3 Results Comparison

Put the 5 sets of data into Table 6-8 for a comparison.

Table 6-8 Results comparison of five logic gates

|Logic gate|Classification result|Parameter|
|---|---|---|
|NOT|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\6\LogicNotResult.png" width="300" height="300">|W=-12.468<br/>B=6.031|
|AND|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\6\LogicAndGateResult.png" width="300" height="300">|W1=11.757<br/>W2=11.757<br/>B=-17.804|
|NAND|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\6\LogicNandGateResult.png" width="300" height="300">|W1=-11.763<br/>W2=-11.763<br/>B=17.812|
|OR|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\6\LogicOrGateResult.png" width="300" height="300">|W1=11.743<br/>W2=11.743<br/>B=-11.738|
|NOR|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\6\LogicNorGateResult.png" width="300" height="300">|W1=-11.738<br/>W2=-11.738<br/>B=5.409|


We can draw two conclusions from the values and graphs:

1. the values of `W1` and `W2` are essentially the same and have the same sign, indicating that the partition line must have a slope of 135°
2. the higher the accuracy, the closer the starting and ending points of the dividing line are to the midpoint of the four sides at 0.5

The above two points show that the neural network is intelligent and will find the dividing line as gracefully and robustly as possible.


### Code Location

ch06, Level4

### Thinking and Exercises

1. Decrease the value of `max_epoch` and observe the training result of the neural network.
2. Why do the logic OR and NOR use only about 2000 epochs to achieve the same accuracy, while the logic AND and NAND need more than 4000 epochs?
