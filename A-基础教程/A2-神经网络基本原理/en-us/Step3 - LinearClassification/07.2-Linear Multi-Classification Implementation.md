<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 7.2 Neural network implementation of linear multi-classification  

### 7.2.1 Defining the neural network structure

Figure 7-1 shows two relatively distinct dividing lines between the three color groups. These lines are straight lines, which means these categories are linearly separable. How can we precisely find these two dividing lines using a neural network?

- Visually, they are linearly separable, so we can use a single layer neural network
- There are two input features, namely longitude and latitude
- The final output is three categories, which are Wei, Shu and Wu, so the output layer has three neurons

If more than three categories co-exist, we need to assign a neuron to each class. The role of this neuron is to perform a linear process ($Z=WX+B$) based on various data input from the front-end and then perform a non-linear process to calculate the prediction probability of each sample in each category. Next, compare it with the labeled type to see if the prediction is accurate. If accurate, the prediction is rewarded, and positive feedback is given; if not, the prediction is penalized, and negative feedback is given. Both types of feedback are back-propagated to the neural network system to adjust the parameters.

This network has only an input layer and an output layer, and since the input layer is not counted, it is a one-layer network, as shown in Figure 7-7.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/MultipleClassifierNN.png" ch="500" />

Figure 7-7 Multi-input and multi-output single-layer neural network

Unlike the previous single-layer network, the rightmost output layer in Figure 7-7 also has an additional Softmax classification function, which is standard in multi-classification tasks and can be seen as the activation function of the output layer. It does not become a separate layer, just as the logistic function in binary classification.

#### Input layer 

Enter two features longitude $x_1$ and latitude $x_2$:

$$
x=\begin{pmatrix}
x_1 & x_2
\end{pmatrix}
$$

#### weighing matrix

The size of the $W$ weight matrix can be viewed from front to back, e.g., if the input layer has 2 features and the output layer has 3 neurons, the size of $W$ is $2\times 3$.

$$
W=\begin{pmatrix}
w_{11} & w_{12} & w_{13}\\\\
w_{21} & w_{22} & w_{23} 
\end{pmatrix}
$$

The size of $B$ is 1x3, the number of columns is always the same as the number of neurons, and the number of rows is always 1.

$$
B=\begin{pmatrix}
b_1 & b_2 & b_3 
\end{pmatrix}
$$

#### Output layer

The output layer of three neurons, together with a Softmax calculation, ends up with three outputs of $A1, A2, A3$, written as:

$$
Z = \begin{pmatrix}z_1 & z_2 & z_3 \end{pmatrix}
$$
$$
A = \begin{pmatrix}a_1 & a_2 & a_3 \end{pmatrix}
$$

where $Z=X \cdot W+B, A = Softmax(Z)$

### 7.2.2 Sample Data

After retrieving the data with the `SimpleDataReader` class, observe the basic properties of the data:

```Python
reader.XRaw.shape
(140, 2)
reader.XRaw.min()
0.058152279749505986
reader.XRaw.max()
9.925126526921046

reader.YRaw.shape
(140, 1)
reader.YRaw.min()
1.0
reader.YRaw.max()
3.0
```

- Training data X, 140 records, two features, minimum value 0.058, maximum value 9.925
- Labeled data Y, 140 records, one category value, taking values in the range of $\\{1,2,3\\\}$

#### Sample Label Data

In general, when labelling samples, we use a label like $1,2,3$ to indicate which category it is. So the sample data looks like this:
$$
Y = 
\begin{pmatrix}
y_1 \\\\ y_2 \\\\ \vdots \\\\ y_{140}
\end{pmatrix}=
\begin{pmatrix}3 \\\\ 2 \\\\ \vdots \\\\ 1\end{pmatrix}
$$

When there is a Softmax multi-classification calculation, we use the following equivalence, commonly known as OneHot.
$$
Y = 
\begin{pmatrix}
y_1 \\\\ y_2 \\\\ \vdots \\\\ y_{140}
\end{pmatrix}=
\begin{pmatrix}
0 & 0 & 1 \\\\
0 & 1 & 0 \\\\
... & ... & ... \\\\
1 & 0 & 0
\end{pmatrix}
$$

OneHot means that in this column of data, there is only a single 1 and all others are 0. The number of columns where 1 is located is the classified category of this sample. The label data corresponds to each sample data with column alignment, and there are only three possibilities of $(1,0,0),(0,1,0),(0,0,1)$, which mean the first category, the second category and the third category respectively.

Implement the `ToOneHot()` method in the `SimpleDataReader` to convert the original label into a One-Hot encoding:

```Python
class SimpleDataReader(object):
    def ToOneHot(self, num_category, base=0):
        ......
```

### 7.2.3 Code Implementation

Most of the code can be copied from `HelperClass` in the previous chapter, but we need to slightly modify it for the specific needs of this chapter.

#### Adding the classification function

Add Softmax implementation in `Activators.py` and add unit tests.

```Python
class Softmax(object):
    def forward(self, z):
        ......
```

#### Feed-forward calculation

The Feed-forward calculation requires adding the following classification function calls:

```Python
class NeuralNet(object):
    def forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.W) + self.B
        if self.params.net_type == NetType.BinaryClassifier:
            A = Logistic().forward(Z)
            return A
        elif self.params.net_type == NetType.MultipleClassifier:
            A = Softmax().forward(Z)
            return A
        else:
            return Z
```

#### Backpropagation

The deduction process of backpropagation is described in detail in the section of the multi-classification function, and the result of the deduction is surprisingly a simple subtraction, with the same result as the algorithm of the fitting and binary classification learned earlier.

```Python
class NeuralNet(object):
    def backwardBatch(self, batch_x, batch_y, batch_a):
        ......
```

#### Calculate the value of the loss function

The loss function is no longer the Mean Square Error and binary cross-entropy, but the cross-entropy function for multi-classification and a conditional branch is added to determine that this loss function is called only when the network type is multi-classification.

```Python
class LossFunction(object):
    # fcFunc: feed forward calculation
    def CheckLoss(self, A, Y):
        m = Y.shape[0]
        if self.net_type == NetType.Fitting:
            loss = self.MSE(A, Y, m)
        elif self.net_type == NetType.BinaryClassifier:
            loss = self.CE2(A, Y, m)
        elif self.net_type == NetType.MultipleClassifier:
            loss = self.CE3(A, Y, m)
        #end if
        return loss
    # end def

    # for multiple classifier
    def CE3(self, A, Y, count):
        ......
```

#### Inference functions

```Python
def inference(net, reader):
    xt_raw = np.array([5,1,7,6,5,6,2,7]).reshape(4,2)
    xt = reader.NormalizePredicateData(xt_raw)
    output = net.inference(xt)
    r = np.argmax(output, axis=1)+1
    print("output=", output)
    print("r=", r)
```
Note that normalization is done before inference because the original data is in the range [0,10].

The function `np.argmax` is used to compare the values of several data in `output` and return the number of rows or columns of the largest one, 0-based. For example, when output=(1.02,-3,2.2), it will return 2 because 2.2 is the largest, so we add 1 again to make the return value one of 1,2,3.

The parameter `axis=1` of the `np.argmax` function, because there are 4 samples involved in the prediction, needs to be distinguished in the second dimension and the argmax values for each sample calculated separately.

#### Main program

```Python
if __name__ == '__main__':
    num_category = 3
    ......
    num_input = 2
    params = HyperParameters(num_input, num_category, eta=0.1, max_epoch=100, batch_size=10, eps=1e-3, net_type=NetType.MultipleClassifier)
    ......
```

### 7.2.4 Runtime results

#### History of loss function

The trend shown in Figure 7-8 shows that the loss function value may further decrease to improve the model accuracy. Interested readers can train a few more rounds to see the effect.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/loss.png" ch="500" />

Figure 7-8 Variation of loss function values during training

The following are the last few lines of the printout:

```
......
epoch=99
99 13 0.25497053433985734
W= [[-1.43234109 -3.57409342  5.00643451]
 [ 4.47791288 -2.88936887 -1.58854401]]
B= [[-1.81896724  3.66606162 -1.84709438]]
output= [[0.01801124 0.73435241 0.24763634]
 [0.24709055 0.15438074 0.59852871]
 [0.38304995 0.37347646 0.24347359]
 [0.51360269 0.46266935 0.02372795]]
r= [2 3 1 1]
```
Notice that r is the classified prediction result, and the output for each test sample is viewed by row, i.e., the first row is the classified result for the first test sample.

1. The relative latitude and longitude is $(5,1)$, the maximum probability is 0.734, and it belongs to 2, Shu
2. The relative value of latitude and longitude is $(7,6)$, the probability 0.598 is the largest, and it belongs to 3, Wu
3. The relative value of latitude and longitude is $(5,6)$, with the maximum probability of 0.383, belongs to 1, Wei
4. The probability of 0.513 is the highest when the relative latitude and longitude are $(2,7)$ and belongs to 1, Wei

### Code Location

ch07, Level1

### Thinking and Exercises

1. From the inference results of the 4 samples, all the classifications are correct, but only the first sample result has the absolute leading probability value of 0.734, and the other classifications are not so assertive. How can one make the difference in the probability value of correct classifications larger?
