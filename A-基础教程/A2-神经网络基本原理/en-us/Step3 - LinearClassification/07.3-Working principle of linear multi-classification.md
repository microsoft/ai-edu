<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 7.3 Principle of linear multi-classification

This principle is applicable to both linear and non-linear multi-classification.

### 7.3.1 Multi-Classification Process

Here we take an example of a triple classification with two feature values. It can be extended to more classifications or arbitrary feature values. For instance, in an image classification task of ImageNet, the last fully connected layer outputs thousands of feature values and 1000 categories to the classifier.

1. Linear calculations

$$z_1 = x_1 w_{11} + x_2 w_{21} + b_1 \tag{1}$$
$$z_2 = x_1 w_{12} + x_2 w_{22} + b_2 \tag{2}$$
$$z_3 = x_1 w_{13} + x_2 w_{23} + b_3 \tag{3}$$

2. Classification Calculations

$$
a_1=\frac{e^{z_1}}{\sum_i e^{z_i}}=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{4}
$$
$$
a_2=\frac{e^{z_2}}{\sum_i e^{z_i}}=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{5}
$$
$$
a_3=\frac{e^{z_3}}{\sum_i e^{z_i}}=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{6}
$$

3. Loss function calculations

For single samples, $n$ denotes the number of categories, and $j$ represents the ordinal category number:

$$
\begin{aligned}
loss(w,b)&=-(y_1 \ln a_1 + y_2 \ln a_2 + y_3 \ln a_3) \\\\
&=-\sum_{j=1}^{n} y_j \ln a_j 
\end{aligned}
\tag{7}
$$

For batch samples, $m$ denotes the number of samples, and $i$ indicates the sample serial number.

$$
\begin{aligned}
J(w,b) &=- \sum_{i=1}^m (y_{i1} \ln a_{i1} + y_{i2} \ln a_{i2} + y_{i3} \ln a_{i3}) \\\\
&=- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \ln a_{ij}
\end{aligned}
 \tag{8}
$$

The loss function calculation is described in detail in the section on the cross-entropy function.

### 7.3.2 Example of numerical calculation

Suppose the calculated $z$ value obtained from predicting a sample is:

$$z=[z_1,z_2,z_3]=[3,1,-3]$$

Then the calculation by equations 4, 5 and 6 leads to the probability distribution of Softmax as:

$$a=[a_1,a_2,a_3]=[0.879,0.119,0.002]$$

#### If the label value indicates that this sample belongs to the first category

that is：

$$y=[1,0,0]$$

then the loss function is:

$$
loss_1=-(1 \times \ln 0.879 + 0 \times \ln 0.119 + 0 \times \ln 0.002)=0.123
$$

The backpropagation error matrix is given by:

$$a-y=[-0.121,0.119,0.002]$$

Since $a_1=0.879$ is the largest of the three and correctly classified, none of the three values of $a-y$ is large.

#### If the label value indicates that this sample belongs to the second category

that is：

$$y=[0,1,0]$$

Then the loss function is：

$$
loss_2=-(0 \times \ln 0.879 + 1 \times \ln 0.119 + 0 \times \ln 0.002)=2.128
$$

It can be seen that the value of $loss_2$ is much larger than the value of $loss_1$ due to misclassification.

The backpropagation error matrix is:

$$a-y=[0.879,0.881,0.002]$$

It was supposed to be the second category and misclassified as the first, so the first two elements have large values, and the backpropagation is strong.

### 7.3.3 Geometric principles of multi-classification

The previous binary classification principle is easily understood as separating two sections with a straight line. For the multi-classification problem, is it possible to follow a similar geometric interpretation? The answer is yes, except that each category needs to be determined separately.

Suppose there are three kinds of samples in total, blue is 1, red is 2, and green is 3. Then the Softmax should be of the form:

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^3 e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+^{z_3}}
$$

#### When the sample belongs to the first category

Separate the blue dots from the other color dots.

If a point is determined to belong to the first category, the $a_1$ probability value must be larger than $a_2,a_3$, expressed as the equation:

$$a_1 > a_2 且 a_1 > a_3 \tag{9}$$

Due to the unique form of Softmax where the denominators are the same, it is sufficient to only compare the numerators. Moreover, the numerator is a natural exponent with an output value domain that is greater than zero and monotonically increasing, so comparing only the exponents is enough. Therefore, Equation 9 is equivalent to the following equation:

$$z_1 > z_2, z_1 > z_3 \tag{10}$$

Applying equations 1, 2 and 3 to 10:

$$x_1 w_{11}  + x_2 w_{21} + b_1  > x_1 w_{12} + x_2 w_{22} + b_2 \tag{11}$$
$$x_1 w_{11}  + x_2 w_{21} + b_1  > x_1 w_{13} + x_2 w_{23} + b_3 \tag{12}$$

Transforms into:

$$(w_{21} - w_{22})x_2 > (w_{12} - w_{11})x_1 + (b_2 - b_1) \tag{13}$$

$$(w_{21} - w_{23})x_2 > (w_{13} - w_{11})x_1 + (b_3 - b_1) \tag{14}$$

Let us first assume that:

$$w_{21} > w_{22}, w_{21}> w_{23} \tag{15}$$

So the coefficients on the left side of Eqs. 13 and 14 are both greater than zero, and both sides are simultaneously divided by the coefficients of:

$$x_2 > \frac{w_{12} - w_{11}}{w_{21} - w_{22}}x_1 + \frac{b_2 - b_1}{w_{21} - w_{22}} \tag{16}$$

$$x_2 > \frac{w_{13} - w_{11}}{w_{21} - w_{23}} x_1 + \frac{b_3 - b_1}{w_{21} - w_{23}} \tag{17}$$

The simplification, that is, to simplify the factors in the fractional form of Eqs. 16 and 17 into one variable $W$ and $B$, respectively, and to make $y = x_2,x = x_1$, facilitates the understanding of:

$$y > W_{12} \cdot x + B_{12} \tag{18}$$

$$y > W_{13} \cdot x + B_{13} \tag{19}$$

At this point, y represents the blue dot of the first category.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/z1.png" ch="500" />

Figure 7-9 How to separate the blue samples from the other two colours

Borrowing the concept from binary classification, the geometric meaning of Equation 18 is that there is a straight line that separates the first class (blue points) and the second class (red points) such that all blue points are above the line and all red points are below the line. So we can draw that green straight line in Figure 7-9.

And the geometric meaning of Equation 19 is that there is a line that separates the first class (blue points) and the third class (green points) so that all blue points are above the line and all green points are below the line. Therefore, we can draw that red straight line in Figure 7-9.

That is to say, draw two straight lines in the figure with all the blue points above both the red and green lines at the same time.

#### When the sample belongs to the second category

That is, how should one separate the red dots from the other two colours?

$$z_2 > z_1,z_2 > z_3 \tag{20}$$

Similarly, we can obtain:

$$y < W_{12} \cdot x + B_{12} \tag{21}$$

$$y > W_{23} \cdot x + B_{23} \tag{22}$$

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/z2.png" ch="500" />

Figure 7-10 How to separate the red sample from the other two colours

At this time, $y$ represents the second class of red points.

Equation 21 and Equation 18 have the same geometric meaning with opposite inequality signs, representing the partitioning effect of the green line in Figure 7-10, the red points below the green line.

The geometric meaning of Equation 22 is that there is a blue line that can separate the second category (red points) and the third category (green points) so that all red points are above the line and all green points are below the line.

#### When the sample belongs to the third category

That is, how to separate the green dots from the other two colour?

$$z_3 > z_1,z_3 > z_2 \tag{22}$$

This finally leads to.

$$y < W_{13} \cdot x + B_{13} \tag{23}$$

$$y < W_{23} \cdot x + B_{23} \tag{24}$$

At this time, $y$ represents the green point of the third category.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/z3.png" ch="500" />

Figure 7-11 How to separate the green sample from the other two colours

Equation 23 and Equation 19 have opposite inequality signs and the same geometric meaning, representing the partitioning effect of the red line in Figure 7-11, with the green point below the red line.

Equation 24 and Equation 22 have the opposite inequality signs, the same geometric meaning, representing the role of the blue line in Figure 7-11, the green point below the blue line.

#### Comprehensive effect

Combining the three diagrams, it should appear as Figure 7-12.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/z123.png" ch="500" />

Figure 7-12 Three lines separating three types of samples

### Thinking and Exercises

1. Is there any basis for our assumption that $w_{21} > w_{22} > w_{23}$? Suppose $w_{22} > w_{21} > w_{23}$, would the positions of the straight lines change?
2. Shouldn't the three straight lines in the last figure intersect at one point?
