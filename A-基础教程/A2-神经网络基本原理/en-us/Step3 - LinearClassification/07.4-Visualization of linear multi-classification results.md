<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->
  
## 7.4 Visualization of multi-classification results

Is the neural network a one-to-one approach or a one-to-many approach? The Softmax formula seems to be a one-to-many approach because only one maximum value is taken, so ideally, the one-to-many approach should look like the one shown in Figure 7-13.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/OneVsOthers.png" ch="500" />

Figure 7-13 The ideal dividing line for the one-to-many approach

Let's walkthrough the following analysis to see what it actually looks like.

### 7.4.1 Display raw data plot

We face the same problem as in binary classification: how can one intuitively understand the results of multi-classification? Triple classification is a bit more complicated, so let's examine the raw data first.

```Python
def ShowData(X,Y):
    for i in range(X.shape[0]):
        if Y[i,0] == 1:
            plt.plot(X[i,0], X[i,1], '.', c='r')
        elif Y[i,0] == 2:
            plt.plot(X[i,0], X[i,1], 'x', c='g')
        elif Y[i,0] == 3:
            plt.plot(X[i,0], X[i,1], '^', c='b')
        # end if
    # end for
    plt.show()
```

which will draw Figure 7-1.

### 7.4.2 Display dividing line chart of classification results

The following data are the results of the weights and bias values trained by the neural network:

```
......
epoch=98
98 1385 0.25640040547970516
epoch=99
99 1399 0.2549651316913006
W= [[-1.43299777 -3.57488388  5.00788165]
 [ 4.47527075 -2.88799216 -1.58727859]]
B= [[-1.821679    3.66752583 -1.84584683]]
......
```

In fact, when explaining the multi-classification principle in Section 7.2, we have discussed its geometric explanations. The deduction of those formulas can guide us to draw multi-classification dividing lines. Let's make a few valuable conclusions first.

From Equation 16 in Section 7.2, turning the inequality sign into an equal sign, i.e., $z_1=z_2$, represents the green dividing line for dividing the first and second categories:

$$x_2 = \frac{w_{12} - w_{11}}{w_{21} - w_{22}}x_1 + \frac{b_2 - b_1}{w_{21} - w_{22}} \tag{1}$$

$$which is：y = W_{12} \cdot x + B_{12}$$

Since the `Python` array starts at 0, all subscripts in equation 1 are subtracted by 1 and written as code:

```Python
b12 = (net.B[0,1] - net.B[0,0])/(net.W[1,0] - net.W[1,1])
w12 = (net.W[0,1] - net.W[0,0])/(net.W[1,0] - net.W[1,1])
```

From Equation 17 in Section 7.2, turning the inequality sign into an equal sign, i.e., $z_1=z_3$, represents the red dividing line for dividing the first and third categories:

$$x_2 = \frac{w_{13} - w_{11}}{w_{21} - w_{23}} x_1 + \frac{b_3 - b_1}{w_{21} - w_{23}} \tag{2}$$
$$which is：y = W_{13} \cdot x + B_{13}$$

Written as code:

```Python
b13 = (net.B[0,0] - net.B[0,2])/(net.W[1,2] - net.W[1,0])
w13 = (net.W[0,0] - net.W[0,2])/(net.W[1,2] - net.W[1,0])
```

From Equation 24 in Section 7.2, turning the inequality sign into an equal sign, i.e., $z_2=z_3$, represents the blue dividing line for dividing the second and third categories of:

$$x_2 = \frac{w_{13} - w_{12}}{w_{22} - w_{23}} x_1 + \frac{b_3 - b_2}{w_{22} - w_{23}} \tag{3}$$
$$which is：y = W_{23} \cdot x + B_{23}$$

Written as code:

```Python
b23 = (net.B[0,2] - net.B[0,1])/(net.W[1,1] - net.W[1,2])
w23 = (net.W[0,2] - net.W[0,1])/(net.W[1,1] - net.W[1,2])
```

See the python file in ch07 Level2 for the complete code.

Modify the main function to add two function calls to`ShowData()` and `ShowResult()`, and we can finally see the classification result graph shown in Figure 7-14. Note that this result graph is the same as the one we analyzed in 7.2, except that the slope of the blue line is different.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/result.png" ch="500" />

Figure 7-14 The classification result plotted by the neural network

The four large points of the triangle in Figure 7-14 are the four coordinate values we need to predict. Three of the points are classified relatively clearly, and only the classification of the blue point is ambiguous. This can be observed by zooming in locally on the actual run result graph.

### 7.4.3 Understand the classification of neural networks

The actual result in Figure 7-14 is quite different from our conjecture in Figure 7-13:

- The blue line is the boundary of 2|3, without considering class 1
- The green line is the boundary of 1|2, not considering class 3
- The red line is the boundary of 1|3 without considering class 2

Let's just look at class 1 in blue. When it comes to distinguishing between 1|2 and 1|3, the neural network actually uses two straight lines (green and red) as boundaries at the same time. So is this a one-to-one approach or a one-to-many approach?

The partition line in Figure 7-14 is obtained by making the three equations $z_1=z_2, z_2=z_3, z_3=z_1$, but in fact, the neural network does not work this way. It does not compare two categories alone, but three categories simultaneously, which can be understood from the fact that Softmax outputs three probability values at the same time. For example, when we want to get the dividing line of the first category, we need to satisfy two conditions at the same time:

$$z_1=z_2，and：z_1=z_3 \tag{4}$$

That is, find the boundary between the first category and the third category at the same time.

This means that Equation 4 is actually a linear segmentation function, not a function composed of two straight lines, the red and green rays in Figure 7-15.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/multiple_result_true.png" ch="500" />

Figure 7-15 Segmentation effect of piecewise linear

Similarly, the segmentation lines used to separate red points from the other two classes are the blue and green rays, and the segmentation lines used to separate green points from the other two classes are the red and blue rays.

When training a one-to-many classifier, the blue samples are treated as one class, and the red and green samples are mixed together as another class. When training a one-to-one classifier, the green samples are thrown away and only the blue samples and the red samples are considered. We did not do that here, where all three classes of samples were involved in the training at the same time. So we can only say that the neural network is a one-to-many approach in terms of results, and we will further explore its fundamental character later when discussing non-linear classification.

### Code Location

ch07, Level2

### Thinking and Exercises

1. Use a one-to-one approach to train three binary classifiers to solve this problem.

