<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 10.1 Why it is necessary to use a two-layer neural network

### 10.1.1 Classification

Let us recall the meaning of the various classifications:

- In terms of complexity, there are linear/nonlinear classifications;
- In terms of sample class, there are  binary/multiple classifications.

From an intuitive understanding, these concepts should match the examples in Table 10-2.

Table 10-2 Combined relationships of various classifications

| |Binary classification|Multiple classification|
|---|---|---|
|Linear|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/linear_binary.png"/>|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/linear_multiple.png"/>|
|Non-linear|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/non_linear_binary.png"/>|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/non_linear_multiple.png"/>|

We learned about linear classification in step 3, and if used here, we might get the green dividing line shown in Table 10-3.

Table 10-3 Linear classification results

|XOR problem|Curve problem|
|---|---|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_data_line.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_data_line.png'/>|
|It is impossible to split the blue points on one side while the red points are on the other side for either of the two straight lines in the figure| For the linear technique, it has done its best to make the two types of samples distributed on both sides of the line as much as possible|

### 10.1.2 Simple proof of the impossibility of XOR problems

Is it possible to accomplish the XOR task with a single perceptron or a single-layer neural network? Let's make a simple proof ourselves. Let's first look at the sample data, as in Table 10-4.

Table 10-4 Sample data for the XOR task

|Sample|$x_1$|$x_2$|$y$|
|---|---|---|---|
|1|0|0|0|
|2|0|1|1|
|3|1|0|1|
|4|1|1|0|

In terms of a single neuron (perceptron), it is a combination of the two techniques in Table 10-5.

Table 10-5 Neuron structure and binary classification function

|Neuron|Classification function Logistic|
|--|--|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_prove.png' width="400"/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/sigmoid_seperator.png' width="430"/>|

Feed forward calculation equation:

$$z = x_1  w_1 + x_2  w_2 + b \tag{1}$$
$$a = Logistic(z) \tag{2}$$

- For the first sample data

$x_1=0,x_2=0,y=0$. If $a=y$ is needed, from the logistic function curve, $z<0$ is needed, so we have
$$x_1 w_1 + x_2  w_2 + b < 0$$

Since $x_1=0,x_2=0$, only $b$ terms remain:

$$b < 0 \tag{3}$$

- For the second sample data

$x_1=0,x_2=1,y=1$. If $a=y$ is needed, then $z>0$ is required and the inequality is:

$$x_1w_1 + x_2w_2+b=w_2+b > 0 \tag{4}$$

- For the third sample data

$x_1=1,x_2=0,y=1$. If $a=y$ is needed, then $z>0$ is required and the inequality is:

$$x_1w_1 + x_2w_2+b=w_1+b > 0 \tag{5}$$

- For the fourth sample data

$x_1=1,x_2=1,y=0$. If $a=y$ is needed, then $z<0$ is required and the inequality is:

$$x_1w_1 + x_2w_2+b=w_1+w_2+b < 0 \tag{6}$$

Add $b$ to both sides of Equation 6 and connect Equation 3:

$$(w_1 + b) + (w_2 + b) < b < 0 \tag{7}$$

Looking at Equations 4 and 5 again, the two factors in the left bracket of the inequality are greater than 0, and their sum must also be greater than 0. It cannot be less than $b$. Therefore Equation 7 does not hold, and in any case, it cannot satisfy the condition of all 4 samples, so it is impossible for a single neuron to do an XOR operation.

### 10.1.3 Possibility of non-linearity

We learned earlier how to implement AND, NAND, OR, or NOR. Let's see how to build an XOR gate with existing logic, as shown in Figure 10-5.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_gate.png" />

Figure 10-5 Building an XOR operation unit with a basic logic unit

Table 10-6 Procedure of Combination Operation

|Sample and Calculation|1|2|3|4|
|----|----|----|----|----|
|$x_1$|0|0|1|1|
|$x_2$|0|1|0|1|
|$s_1=x_1$ NAND $x_2$|1|1|1|0|
|$s_2=x_1$ OR $x_2$|0|1|1|1|
|$y=s_1$ AND $s_2$|0|1|1|0|

After the combination operation shown in Table 10-6, you can see that the output of $y$ is an XOR logic compared with the input of $x_1,x_2$. So, it is proved that the two-layer logic circuit can solve the problem. In addition, we learned about non-linear regression in step 4, and using two-layer neural networks can accomplish some amazing things, such as fitting complex curves with only 6 or 7 parameters.  We can simulate this idea and build a model with a two-layer neural network to solve the non-linear classification problem.

