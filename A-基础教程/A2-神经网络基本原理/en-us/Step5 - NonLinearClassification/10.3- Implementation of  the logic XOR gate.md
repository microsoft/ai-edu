<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 10.3 Implementation of the logic XOR gate

### 10.3.1 Code Implementation

#### Preparing data

The XOR data is relatively simple, with only 4 records, so it's hardcoded here, no need to create a dataset. This also gives the reader a chance to derive a whole new subclass `XOR_DataReader` from the `DataReader` class.

For example, in the following code, we override three methods from the parent class:

- `init()` initialization method: the initialization method of the parent class requires two parameters, representing the train/test data file
- `ReadData()` method: the parent class method read the data file directly, generate the sample data in memory, and make the training set equal to the original data set (without normalization) so that the test set is equivalent to the training set
- `GenerateValidationSet()` method, because there are only 4 samples, so directly make the validation set equivalent to the training set

Since the code in `NeuralNet2` requires an entire dataset with the training set, validation set and test set, we set the validation set and test set to be the same as the training set to have the code running smoothly, which has no impact on solving this XOR problem.

```Python
class XOR_DataReader(DataReader):
    def ReadData(self):
        self.XTrainRaw = np.array([0,0,0,1,1,0,1,1]).reshape(4,2)
        self.YTrainRaw = np.array([0,1,1,0]).reshape(4,1)
        self.XTrain = self.XTrainRaw
        self.YTrain = self.YTrainRaw
        self.num_category = 1
        self.num_train = self.XTrainRaw.shape[0]
        self.num_feature = self.XTrainRaw.shape[1]
        self.XTestRaw = self.XTrainRaw
        self.YTestRaw = self.YTrainRaw
        self.XTest = self.XTestRaw
        self.YTest = self.YTestRaw
        self.num_test = self.num_train

    def GenerateValidationSet(self, k = 10):
        self.XVld = self.XTrain
        self.YVld = self.YTrain
```

#### The Test function

As with the logic AND and OR gates in Chapter 6, we need the results of the neural network operations to achieve a certain accuracy, that is, very close to 0 and 1, instead of saying that it is approximate to 1 when it is barely greater than 0.5, so the accuracy requirement is that the absolute value of the error is less than `1e-2`.

```Python
def Test(dataReader, net):
    print("testing...")
    X,Y = dataReader.GetTestSet()
    A = net.inference(X)
    diff = np.abs(A-Y)
    result = np.where(diff < 1e-2, True, False)
    if result.sum() == dataReader.num_test:
        return True
    else:
        return False
```

#### The main process code

```Python
if __name__ == '__main__':
    ......
    n_input = dataReader.num_feature
    n_hidden = 2
    n_output = 1
    eta, batch_size, max_epoch = 0.1, 1, 10000
    eps = 0.005
    hp = HyperParameters2(n_input, n_hidden, n_output, eta, max_epoch, batch_size, eps, NetType.BinaryClassifier, InitialMethod.Xavier)
    net = NeuralNet2(hp, "Xor_221")
    net.train(dataReader, 100, True)
    ......
```

The code here has a few details that need to be highlighted:

- `n_input = dataReader.num_feature`, the value is 2, and it must be 2 because there are only two feature values
- `n_hidden=2`, which is the number of hidden layer neurons set manually and can be any integer greater than 2
- `eps` precision=0.005 is the posterior knowledge. The stopping condition obtained by testing is used to facilitate case explanation
- The network type is `NetType.BinaryClassifier`, specifying that it is a binary classification network
- Finally, the ‘Test’function is called to verify the accuracy

### 10.3.2 Runtime result

After quick iterations, the training process is displayed, as shown in Figure 10-10.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_loss.png" />

Figure 10-10 Variation of loss function value and accuracy value during training

It can be seen that the trend of both is ideal.

Also some information is printed in the console and the last few lines are as follows:

```
......
epoch=5799, total_iteration=23199
loss_train=0.005553, accuracy_train=1.000000
loss_valid=0.005058, accuracy_valid=1.000000
epoch=5899, total_iteration=23599
loss_train=0.005438, accuracy_train=1.000000
loss_valid=0.004952, accuracy_valid=1.000000
W= [[-7.10166559  5.48008579]
 [-7.10286572  5.48050039]]
B= [[ 2.91305831 -8.48569781]]
W= [[-12.06031599]
 [-12.26898815]]
B= [[5.97067802]]
testing...
1.0
None
testing...
A2= [[0.00418973]
 [0.99457721]
 [0.99457729]
 [0.00474491]]
True
```
A total of 5900 `epoch`s were used to reach the specified `loss` precision (0.005), and the `loss_valid` was 0.004991, just under 0.005 when the iteration was stopped.

We print out the `A2` values, the network inference results, as shown in Table 10-7.

Table 10-7 Comparison of XOR computed values and neural network inference values

|x1|x2|XOR|Inference|diff|
|---|---|---|---|---|
|0|0|0|0.0041|0.0041|
|0|1|1|0.9945|0.0055|
|1|0|1|0.9945|0.0055|
|1|1|0|0.0047|0.0047|

The inference value in the fourth column of the table is very close to the `XOR` result in the third column, and higher accuracy can be obtained with further training, but this is generally not necessary. Thus, we once again realize that neural networks can only obtain approximate solutions that are infinitely close to the true value.

### Code location

ch10, Level1
