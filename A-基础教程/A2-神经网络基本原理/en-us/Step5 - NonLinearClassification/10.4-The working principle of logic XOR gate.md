<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 10.4 The working principle of logic XOR gate

The previous lesson demonstrated practically that two-layer neural networks can solve the XOR problems. Let us understand how neural networks work on this XOR problem. This principle can be extended to more complex problem spaces, but it presents difficulties to our understanding because the high-dimensional space cannot be visualized.

### 10.4.1 Visualization of classification results

To assist in understanding the process of XOR classification, we add some visualization functions to help understand this process.

#### Display raw data

```Python
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt

from Level1_XorGateClassifier import *

def ShowSourceData(dataReader):
    DrawSamplePoints(dataReader.XTrain[:,0],dataReader.XTrain[:,1],dataReader.YTrain, "XOR Source Data", "x1", "x2")

def DrawSamplePoints(x1, x2, y, title, xlabel, ylabel, show=True):
    assert(x1.shape[0] == x2.shape[0])
    fig = plt.figure(figsize=(6,6))
    count = x1.shape[0]
    for i in range(count):
        if y[i,0] == 0:
            plt.scatter(x1[i], x2[i], marker='^', color='r', s=200, zorder=10)
        else:
            plt.scatter(x1[i], x2[i], marker='o', color='b', s=200, zorder=10)
        #end if
    #end for
    plt.grid()
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    if show:
        plt.show()
```

1. First, import all the content from Level_XorGateClassifier, which saves us the trouble of rewriting the code for the data preparation part
2. Obtain all the training samples with category 1 and display them on the panel with red crosses
3. Obtain all the training samples with category 0 and display them on the panel with blue dots

From this, we will get the samples as shown in Figure 10-11.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_source_data.png" ch="500" />

Figure 10-11 sample data for the XOR problem 

The four points of the XOR problem are distributed on the four corners of the [0,1] space, the red points belong to the positive class, and the blue points belong to the negative class.

#### Show intermediate results of inference

Since it is a two-layer neural network, recall its formula:$Z1 = X \cdot W1 +B1,A1=Sigmoid(Z1),Z2=A1 \cdot W2+B2,A2=Logistic(A2)$，So there will be intermediate results of operations such as $Z1,A1,Z2,A2$. We display them graphically to help the reader understand the inference process.

```Python
def ShowProcess2D(net, dataReader):
    net.inference(dataReader.XTest)
    # show z1    
    DrawSamplePoints(net.Z1[:,0], net.Z1[:,1], dataReader.YTest, "net.Z1", "Z1[0]", "Z1[1]")
    # show a1
    DrawSamplePoints(net.A1[:,0], net.A1[:,1], dataReader.YTest, "net.A1", "A1[0]", "A1[1]")
    # show sigmoid
    DrawSamplePoints(net.Z2, net.A2, dataReader.YTrain, "Z2->A2", "Z2", "A2", show=False)
    x = np.linspace(-6,6)
    a = Sigmoid().forward(x)
    plt.plot(x,a)
    plt.show()
```

1. first, make an inference with the test sample.
2. Z1 is the result of the linear transformation of the first layer of the neural network, and since Z1 is an array of 4 rows and 2 columns, we draw 4 points using the 1st column of Z1 as x1 and the 2nd column of Z1 as x2.
3. A1 is the result of Z1 after the activation function and is drawn as 4 points as in Z1.
4. Z2 results from the linear transformation of the second layer of the neural network, A2 is the result of the Logistic Function of Z2, using Z2 as x1 and A2 as x2 to draw 4 points, and superimposing the Logistic Function image to see if they match.

So we get the following three diagrams, which are put into Table 10-8 (putting the original diagram in the first position as a comparison).

Table 10-8 Inference process of the XOR problem

|||
|---|---|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_source_data.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_z1.png'/>|
|The raw sample|Z1 is the linear calculation result of the first layer network|  
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_a1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_z2_a2.png'/>|
|A1 is the calculation result of activation function of Z1|Z2 is the calculation result of second layer linearity, A2 is the result of binary classification| 

- Z1: By linear transformation, blue points of the original data are moved to the two opposite corners, and the red points are moved towards the center, close to overlapping. The red points in the figure look as if it is one dot but is actually two points overlapping. You can see the details by zooming in on the original drawing board
- A1: Through Sigmoid operation, the value of Z1 is compressed into [0,1] space, making the blue point coordinates close to [0,1] and [1,0], and the red point coordinates approach to [0,0]
- Z2->A2: Again, through linear transformation, map both types of points to the horizontal axis, and move the blue points to the negative direction and the red points to the positive direction, and then perform Logistic classification, separating the two types of sample points far away to the two ends of [0,1], thus completing the classification task

The intermediate results are shown in table 10-9 for easy comparison.

Table 10-9 Intermediate calculation results

||1（Blue point 1）|2（Red point 1）|3（Red point 2）|4（Blue point2）|
|---|---|---|---|---|
|x1|0|0|1|1|
|x2|0|1|0|1|
|y|0|1|1|0|
|Z1|2.868856|-4.142354|-4.138914|-11.150125|
||-8.538638|-3.024127|-3.023451|2.491059|
|A1|0.946285|0.015637|0.015690|0.000014|
||0.000195|0.046347|0.046377|0.923512|
|Z2|-5.458510|5.203479|5.202473|-5.341711|
|A2|0.004241|0.994532|0.994527|0.004764|

#### Display final results

So far, all we know is that the neural network completes the XOR problem, but how does it actually draw the dividing line?

Perhaps the reader remembers that when learning linear classification in step 4, we successfully drew the dividing line by formula deduction. However, this time it is different, a two-layer neural network is used here, and it is difficult to explain the meaning of W and B weight matrices by formula deduction, so let's try a different approach.

Think about it, in what way does the neural network finally determine the category of the sample? In the feed-forward calculation process, the last formula is the Logistic function, which compresses $(-\infty, +\infty)$ to between (0,1), equivalent to calculating a probability value, and then determines whether it belongs to the positive class by whether the probability value is greater than 0.5 or not. Although the XOR problem has only 4 sample points, if:

1. we do a uniform grid sampling in the [0,1] square interval so that each point will have coordinate values.
2. the coordinate values are then substituted into the neural network for inference, and the outcome obtained should be a grid-like result.
3. each result is a probability value that must be between (0,1), so it is either greater than 0.5 or less than 0.5.
4. we paint the grids greater than 0.5 into pink and the grids less than 0.5 into yellow, and we should be able to draw the dividing line.

Great, with this exciting idea in mind, we immediately implement the following:

```Python
def ShowResult2D(net, dr, title):
    print("please wait for a while...")
    DrawSamplePoints(dr.XTest[:,0], dr.XTest[:,1], dr.YTest, title, "x1", "x2", show=False)
    count = 50
    x1 = np.linspace(0,1,count)
    x2 = np.linspace(0,1,count)
    for i in range(count):
        for j in range(count):
            x = np.array([x1[i],x2[j]]).reshape(1,2)
            output = net.inference(x)
            if output[0,0] >= 0.5:
                plt.plot(x[0,0], x[0,1], 's', c='m', zorder=1)
            else:
                plt.plot(x[0,0], x[0,1], 's', c='y', zorder=1)
            # end if
        # end for
    # end for
    plt.title(title)
    plt.show()
```

In the above code, 50 points are taken horizontally and vertically to form a 50x50 grid and then inferred to get the output value and colour it. Since there are 2,500 calculations, which takes a little longer, we print "please wait for a while..." and let the program run. Finally, we get Figure 10-12.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_2d.png" ch="500" />

Figure 10-12 Segmentation diagram of classification results

Isn't it exciting to see this chart for the first time! We don't live by drawing lines anymore, but rise to the level of colouring! Please ignore the jaggedness in the figure because we took a 50x50 grid, so there will be mosaics. If we take more dense grid points, it will alleviate this problem, but the calculation will be much slower.

We can see that the two types of sample points are divided into different coloured regions, which makes us realize that the neural network can draw two dividing lines simultaneously, or more accurately, "can draw two classification regions".

### 10.4.2 More intuitive visualization of results

#### 3D Diagram

Can a neural network really draw two segmentation lines simultaneously? This subverts my understanding since I have always believed that the last layer of the neural network is just a linear unit. It can do a limited number of things, so its behaviour is linear, draw a line to do fitting or dividing,......, wait a minute, why can only be a line? Couldn’t it be a plane?

This reminds me that in Chapter 5, a plane was used to fit the sample points in space, as shown in Table 10-10.

Table 10-10 Visualization results of plane fitting

|Forward|Lateral|
|---|---|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/5/level3_result_1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/5/level3_result_2.png'/>|

So is it possible that the solution to this XOR problem is a three-dimensional space? With this more exciting idea in mind, we immediately wrote the code:

```Python
def Prepare3DData(net, count):
    x = np.linspace(0,1,count)
    y = np.linspace(0,1,count)
    X, Y = np.meshgrid(x, y)
    Z = np.zeros((count, count))
    input = np.hstack((X.ravel().reshape(count*count,1),Y.ravel().reshape(count*count,1)))
    output = net.inference(input)
    Z = output.reshape(count,count)
    return X,Y,Z

def ShowResult3D(net, dr):
    fig = plt.figure(figsize=(6,6))
    ax = Axes3D(fig)
    X,Y,Z = Prepare3DData(net, 50)
    ax.plot_surface(X,Y,Z,cmap='rainbow')
    ax.set_zlim(0,1)
    # draw sample data in 3D space
    for i in range(dr.num_train):
        if dataReader.YTrain[i,0] == 0:
            ax.scatter(dataReader.XTrain[i,0],dataReader.XTrain[i,1],dataReader.YTrain[i,0],marker='^',c='r',s=200)
        else:
            ax.scatter(dataReader.XTrain[i,0],dataReader.XTrain[i,1],dataReader.YTrain[i,0],marker='o',c='b',s=200)

    plt.show()
```

The function Prepare3DData() is used to prepare the data in a 3D coordinate system:

1. x coordinate is divided into 50 parts in [0,1] space
2. y coordinate in [0,1] space divided into 50 parts
3. np.meshgrid(x,y) forms a mesh grid of points X and Y, each with 2500 records, and each line of X must be combined with the corresponding sequence of Y to form a mesh grid
4. np.hstack() combines X,Y into a sample matrix of 2500x2
5. net.inference() does inference and gets the resulting output
6. convert the result into a 50x50 shape and assign it to Z, and match the 50x50 grid points of X, Y
7. finally, return the three-dimensional point matrix XYZ
8. the function ShowResult3D() use ax.plot_surface() function to draw the space surface
9. then plot 4 sample points in space, X and Y values are the original sample values x1 and x2, Z value is the actual label value y, that is, 0 or 1

Finally, the results in Table 10-11 are obtained.

Table 10-11 Visualization of XOR classification results

|Oblique side view|Top view|
|---|---|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_3D_1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_3D_2.png'/>|

Now we immediately understand what the neural network does: it goes through the sample points, extrapolates the probability of the classification result for each coordinate point in the plane, forms a spatial surface, and then intercepts it (a tangent plane) so that the neural network can draw a plane at Z=0.5 out, perfectly separating the diagonal vertices. If we look at the top view, it is highly similar to the 2D region colouring map we generated earlier, which has a probability value approaching 1 for the red part and 0 for the blue area, with the colour between red and blue, representing a gradient value from 0 to 1.

The straight line that splits the two classes on the plane is just our imagination: using 0.5 as a threshold separates the two parts of the data like a national border. But in reality, the output of a neural network is a probability, i.e., it can tell you what the likelihood of a point belonging to a particular class is, and we set it to belong to the positive class when the probability is greater than 0.5 and to the negative class when it is less than 0.5. In the spatial surface, the transition zone can be shown as well for better understanding.

#### 2.5D Diagram

The 3D diagram is interesting, but the 2D chart can express the meaning of the classification, but it is just not perfect, so let's find a way to create a 2.5D diagram.

```Python
def ShowResultContour(net, dr):
    DrawSamplePoints(dr.XTrain[:,0], dr.XTrain[:,1], dr.YTrain, "classification result", "x1", "x2", show=False)
    X,Y,Z = Prepare3DData(net, 50)
    plt.contourf(X, Y, Z, cmap=plt.cm.Spectral)
    plt.show()
```

In the two-dimensional plane, the contour map of colouring can be drawn by the plt.contourf() function, with Z as the contour height, which can be obtained in Figure 10-13.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_25d.png" ch="500" />

Figure 10-13 Contour plot of classification results

The 2.5D plot indicates the probability values of the different regions by colour, and it can be seen that the red and blue regions are the regions with probabilities close to 0 and 1, respectively, corresponding to the two types of sample points. We will continue to study the classification problem using this approach later.

But can neural networks really be smart enough to solve problems with dimension-raising? We have only found a reasonable explanation, but we cannot be sure that this is how neural networks work. In the following, we will explore the training process of a neural network to understand how it actually learns.

### 10.4.3 Explore the training process

As the number of iterations increases, the classification results for the XOR binary classification problem become more accurate. We may observe several stages in the training process to understand the training process of the neural network.

In the following experiments, we specify three iterations of 500, 2000, and 6000 to see the classification at each stage.

Table 10-12 Evolution of the values of Z1 and A1 during the training process of XOR classification

|number of iterations|evolution of Z1|evolution of A1|
|---|---|---|
|500 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_z1_500.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_a1_500.png'/>|
|2000 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_z1_2000.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_a1_2000.png'/>|
|6000 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_z1_6000.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_a1_6000.png'/>|

From the evolution of Z1 above, the neural network tries to make the two red points overlap, while the two blue points are as far away as possible but centrosymmetric.

The evolution of A1 is similar to Z1, but the final goal is to make the red points at the two vertices of the [0,1] space (at the same position as the original data) and the blue points overlapped in one corner. From the last picture of the evolution of A1, the two red points have been squeezed together, so it is absolutely possible to have a dividing line to separate the two, as shown in Figure 10-14.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_a1_6000_line.png" ch="500" />

Figure 10-14 Two types of sample data after spatial transformation

In other words, at this point, the neural network no longer needs to do the dimension-raising calculation, and the classification problem can be solved in the two-dimensional plane. From the author's personal perspective, I prefer to believe that this is how neural networks work.

Let's look at the evolution of the final classification results, as shown in Table 10-13.

Table 10-13 Evolution of classification function values and results during the training process of XOR classification

|number of iterations|evolution of classification function values|evolution of classification results|
|---|---|---|
|500 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_logistic_500.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_500.png'/>|
|2000 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_logistic_2000.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_2000.png'/>|
|6000 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_logistic_6000.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_result_6000.png'/>|

From the case of the classification function, the two categories of points are completely indistinguishable at the beginning. As the learning process deepens, the two categories of points gradually move toward the two ends until they are finally as far apart as possible. The 2.5D plot of the classification result shows the probability change of each point in this square region, which eventually forms a banded probability distribution plot due to the symmetric distribution of the sample points.

### 10.4.4 Effect of the number of neurons in the hidden layer

In general, the number of neurons in the hidden layer is greater than or equal to the number of input features. In this case, the number of feature values is 2. For research purposes, the author used six configurations of the number of neurons to test the operation of the neural network. See the comparison graph in Table 10-14.

Table 10-14 Effect of the number of neurons in the hidden layer on the classification results

|||
|---|---|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_n1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_n2.png'/>|
|1 neuron, unable to complete the classification task|2 neurons, 6200 iterations to reach the required accuracy|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_n3.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_n4.png'/>|
|3 neurons, 4900 iterations to reach the accuracy requirement|4 neurons, 4300 iterations to reach the accuracy requirement|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_n8.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_n16.png'/>|
|8 neurons, 4400 iterations to reach the required accuracy|16 neurons, 4500 iterations to reach the required accuracy|

The number of iterations for each case above is the value obtained by testing once with Xavier initialized. It does not mean that more neurons are better, but the appropriate number is better. It can be summarized as follows:

- 2 neurons are definitely sufficient.
- 4 neurons are definitely more straightforward, using the least number of iterations.
- However, more neurons are not easier, such as 8 neurons, which appear as a classification boundary of the curve due to overpowering.
- And 16 neurons divide 4 samples onto 4 regions, get half the result with double the effort. This also hints that neural networks can do more powerful things!
- The angle of the separating bands in Figure 3 of the table is opposite to the previous figures, but the red sample points are still in the blue zone, and the blue sample points are still in the red area; this property has not changed. This is just multiple solutions of the neural network caused by different initialization parameters and has nothing to do with the number of neurons.

### Code Location

ch10, Level2
