<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 10.5 Realization of hyperbolic binary classification

The successful solution of the logic XOR problem can bring us some confidence, but after all, with only four samples, it cannot yet exploit the actual ability of the two-layer neural network. Let's solve Problem 2, the complex binary classification problem.

### 10.5.1 Code Implementation

#### The main process code

```Python
if __name__ == '__main__':
    ......
    n_input = dataReader.num_feature
    n_hidden = 2
    n_output = 1
    eta, batch_size, max_epoch = 0.1, 5, 10000
    eps = 0.08

    hp = HyperParameters2(n_input, n_hidden, n_output, eta, max_epoch, batch_size, eps, NetType.BinaryClassifier, InitialMethod.Xavier)
    net = NeuralNet2(hp, "Arc_221")
    net.train(dataReader, 5, True)
    net.ShowTrainingTrace()
```

The code here has a few details that need to be highlighted:

- `n_input = dataReader.num_feature`, the value is 2, and it must be 2 because there are only two feature values
- `n_hidden=2`, which is the pre-set number of hidden layer neurons and can be any integer greater than 2
- `eps` precision=0.08 is the a posteriori knowledge, the author got the stopping condition by testing, used to facilitate case explanation
- The network type is `NetType.BinaryClassifier`, specifying that it is a binary classification network

### 10.5.2 Runtime results

After rapid iterations, the loss function curve and the accuracy curve are displayed after the training is completed, as in Figure 10-15.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_loss.png" />

Figure 10-15 Variation of the loss function and accuracy values during the training process

The blue line is the curve of the small-batch training samples with relatively large fluctuations, which can be ignored because the small batch is inevitably causing variation. The red curve is the trend of the validation set, and you can see that the trend of both is ideal. After a short break-in period, starting from the 200th `epoch,` both curves suddenly find the breakthrough direction, and then it only takes 50 `epoch`s to quickly reach the specified accuracy.

At the same time, some information is printed in the console, and the last few lines are as follows:

```
......
epoch=259, total_iteration=18719
loss_train=0.092687, accuracy_train=1.000000
loss_valid=0.074073, accuracy_valid=1.000000
W= [[ 8.88189429  6.09089509]
 [-7.45706681  5.07004428]]
B= [[ 1.99109895 -7.46281087]]
W= [[-9.98653838]
 [11.04185384]]
B= [[3.92199463]]
testing...
1.0
```
A total of 260 `epoch` were used, and the iteration was stopped when the specified loss accuracy (0.08) was reached. Looking at the test set, the accuracy is 1.0, i.e. 100% correct classification.

### Code location

ch10, Level3
