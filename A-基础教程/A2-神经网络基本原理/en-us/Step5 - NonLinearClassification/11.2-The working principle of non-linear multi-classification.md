<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 11.2 The working principle of non-linear multi-classification

### 11.2.1 Effect of the number of neurons in the hidden layer

The cases listed below have 2, 4, 8, 16, 32, and 64 hidden layer neurons. The maximum number of epochs is set to 10000 to compare the maximum accuracy values they can achieve. Each configuration is tested for only one round, so the measured data has some randomness.

Table 11-3 shows the relationship between the number of neurons in the hidden layer and the classification results.

Table 11-3 Relationship between the number of neurons and network capability and classification results

|number of neurons|loss function|classification result|
|---|---|---|
|2|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss_n2.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result_n2.png'/>|
||The test set accuracy was 0.618, took 49 seconds, and the loss function value was 0.795. In the case of curves like this, the loss function value does not go down, and the accuracy value does not go up, mainly because of the lack of network capability. |Failure to complete the classification task|
|4|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss_n4.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result_n4.png'/>|
||The test set accuracy was 0.954, took 51 seconds, and the loss function value was 0.132. Although the classification task is completed, the network capacity is still insufficient. |Basically complete, but the border is not clear enough.|
|8|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss_n8.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result_n8.png'/>|
||The test accuracy was 0.97, took 52 seconds, and the loss function value was 0.105. We can try to decrease the learning rate at a later stage first and consider increasing the network capacity if there is no improvement in another 5000 rounds of training. |Basically completed, but the boundaries are not clear enough|
|16|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss_n16.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result_n16.png'/>|
||The test set accuracy was 0.978, took 53 seconds, and the loss function value was 0.094. Same as above, try to use the optimization algorithm to see if it converges faster. |The classification task is well completed|
|32|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss_n32.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result_n32.png'/>|
||The test accuracy was 0.974, took 53 seconds, and the loss function value was 0.085. The network capability was sufficient, and more iterations might be needed judging from the decreasing trend of the loss value and the increasing trend of the accuracy value. |The classification task was well accomplished|
|64|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss_n64.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result_n64.png'/>|
||The test accuracy was 0.972, took 64 seconds, and the loss function value was 0.075. The network capability was sufficient|The classification task is well completed|

### 11.2.2 The transformation process in three-dimensional space

From the above comparison, it is clear that more than three neurons must be used for the hidden layer. Using three neurons can accomplish the essential classification task in this example, but the accuracy is worse. However, if more neurons had to be used to meet the basic requirements, we would not be able to proceed to the next step of the experiment, so the difficulty of this example would be appropriate for us.

Using what we learned in Section 10.6, if the hidden layer is two neurons, we can treat the output data of the two neurons as horizontal and vertical coordinates and plot the intermediate results in the two-dimensional plane. Since three neurons are used, making the hidden layer output three columns per sample row, we must plot the intermediate results in the three-dimensional space.

```Python
def Show3D(net, dr):
    ......
```

The above code first does an inference on the trained network using the test set (500 sample points) to get the computed results of the hidden layer. Then plots the 3D point plots using the three columns of data from `net.Z1` and `net.A1` as the XYZ coordinate values, respectively, listed in Table 11-4 for comparison.

Table 11-4 Visualization of working principle

| |Front view|Side view|
|---|---|---|
|z1|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/bank_z1_1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/bank_z1_2.png'/>|
| |Linear plane in 3D space obtained by linear transformation|Linear plane from side view|
|a1|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/bank_a1_1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/bank_a1_2.png'/>|
| |by the non-linear variation of the activation function, it is spatially squeezed into a triangle| from the side view, the three colours are divided into three layers|

The dot plot of `net.Z1` shows the result of a linear transformation of the input data. As you can see, since it is only a linear transformation, it still looks like a two-dimensional plane from the side view.

The dot plot of `net.A1` shows that the graph after a nonlinear transformation of the activation function. Since the green point is closer to the edge, each value in the 3D coordinates will have at least a one-dimensional coordinate close to 1 after the Sigmoid activation function calculation. Thus it is scattered more widely and forms a triangular region on the periphery. The blue point is just the opposite. The 3D coordinate values all approach 0, so they are finally concentrated in the triangular region at the origin of the 3D coordinates. The red point is in between the first two since there are many intermediate values.

Observing the side view of net.A1, it seems to have been stratified, with blue dots deposited down, green dots floating up, and red dots in the middle, divided into three layers like a cocktail. This creates the conditions for the second layer of the neural network to do a linear triple classification, and only two planes are needed to separate the three easily.

#### 3D classification result map

Higher-dimensional spaces cannot be shown, so when the number of hidden layer neurons is 4 or 8 or more, it is basically impossible to understand the spatial transformation. But there is a way to approximate the higher dimensional case: when in 3D space, blue dots will be squeezed into a corner to form a triangle, then in N (N>3) dimensional space, blue dots will also be packed into a corner. Since N is large, a polygon will approximate a circle, which these three-dimensional figures we will generate below will look like.

We continue the 3D effect experience from Section 9.2. However, the actual implementation of multi-categorization is 1-to-many, so we can only show the categorization effect map one category at a time, as listed in Table 11-5.

Table 11-5 Classification Effect Map

|||
|---|---|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_3d_c1_1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_3d_c2_1.png'/>|
|red: category 1 sample area|red: category 2 sample area|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_3d_c3_1.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_3d_c1_c2_1.png'/>|
|red: category 3 sample area|red: category 1, cyan: category 2, purple: category 3|

The image in the last row of the table above shows the cumulative effect of categories 1 and 2. Since the final results are all normalized by Softmax to between $[0,1]$, we can simply multiply the data of category 1 by 2 and add the data of category 2.

### Code Location

ch11, Level2
