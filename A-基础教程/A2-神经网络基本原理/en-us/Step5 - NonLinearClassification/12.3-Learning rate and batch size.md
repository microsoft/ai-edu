<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 12.3 Learning rate and batch size

In the gradient descent equation:

$$
w_{t+1} = w_t - \frac{\eta}{m} \sum_i^m \nabla J(w,b) \tag{1}
$$

Where $\eta$ is the learning rate, and m is the batch size. Therefore, the learning rate and batch size are the two factors that significantly impact gradient descent.

### 12.3.1 Challenges regarding learning rates

A common saying in the industry is that if only one parameter needs to be adjusted in all hyperparameters, it is the learning rate. This shows how important the learning rate is. Suppose the reader has done the experiment in 9.6 carefully. In that case, he will find that no matter how much you change the batch size or the number of neurons in the hidden layer, you will always find a suitable learning rate to fit the above modifications and finally get the optimal training results.

However, the learning rate is a complicated parameter to adjust, and the details are given below.

The general gradient descent method, as studied in the previous chapters, contains three forms：

1. single sample
2. whole batch sample
3. small-batch samples

We usually refer 1 and 3 collectively as SGD (Stochastic Gradient Descent). When the batch is not very large, the entire batch can be included in this range. Large means: a data volume of 10,000 or more.

When using these forms of gradient descent, we usually face the following challenges:

1. It is difficult to choose a suitable learning rate
   
   A learning rate that is too small will cause the network to converge too slowly, while a learning rate that is too large may affect convergence and cause the loss function to fluctuate on the minimum or even gradient divergence.
   
2. The same learning rate does not apply to all parameter updates
   
   If the training set data is sparse and the features are very different in frequency, they should not all be updated to the same level, but a larger update rate should be used for attributes that occur rarely.
   
3. Avoid getting trapped in multiple local minimums.
   
   In fact, the problem does not arise from local minimum but from saddle points, i.e., points where one dimension tilts upward, and the other dimension slopes downward. These saddle points are usually surrounded by planes of the same error value, making it difficult to detach the SGD algorithm because the gradient is close to zero in all dimensions.

Table 12-1 Saddle points and stationary points

|Saddle points|stationary points|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\12\saddle_point.png" width="640">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\9\sgd_loss_8.png">|

The left graph in Table 12-1 defines the saddle point, near which the gradient descent algorithm often gets bogged down, resulting in the same history curve as the right graph: there is a period when the Loss value decreases slowly with the number of iterations, as if looking for a breakthrough, and then suddenly it finds it and goes all the way down, eventually converging.

Why is there a large flat section between 3000 and 6000 epochs where the Loss value does not drop significantly? This reflects the shape of the actual loss function of this problem, where the gradient is relatively flat over this region. The gradient descent algorithm does not find a proper breakthrough direction to find the optimal solution but wanders in place. This flat region is the saddle point of the loss function.

### 12.3.2 Selection of initial learning rate

We have been using a fixed learning rate, such as 0.1 or 0.05, instead of using a high learning rate like 0.5 or 0.8. This is because the gradient of the loss function also becomes smaller as it approaches the minimum, and there is no concern about crossing the minimum with too large a step when using a small learning rate.

A sufficient condition to guarantee the convergence of SGD is that:

$$\sum_{k=1}^\infty \eta_k = \infty \tag{2}$$

and： 

$$\sum_{k=1}^\infty \eta^2_k < \infty \tag{3}$$ 

Figure 12-5 shows the effect of different choices of learning rates on the training results.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/learning_rate.png" ch="500" />

Figure 12-5 Effect of learning rate on training

- Yellow: the learning rate is too high, the loss value increases, and the network diverges
- Red: learning rate can make the network converge, but the value is relatively large, and the loss value drops quickly at the beginning but jumps back and forth near the optimal solution when it reaches the extreme value point
- Green: the correct learning rate setting
- Blue: the learning rate is too low, the loss value decreases slowly, the training times are long, and the convergence is slow

There is a way to help us quickly find the right initial learning rate.

A great way to find initial learning rates is described in a paper by Leslie N. Smith in 2015 [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186).

This method is used in the paper to estimate the minimum and maximum learning rates allowed for the network. We can also use it to find our optimal initial learning rate straightforwardly.

1. first, we set a very small initial learning rate, say `1e-5`.
2. then update the network after each `batch`, calculate the loss function value and increase the learning rate simultaneously.
Finally, we can plot the curve of the learning rate and the curve of the loss, from which we can find the best learning rate.

Table 12-2 is the curve of increasing learning rate as the number of iterations increases and the curve of the loss corresponding to different learning rates (the ideal curve).

Table 12-2 Experimental optimal learning rate

|Learning rate increases with the number of iterations| Observe the relationship between Loss value and learning rate|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\12\lr-select-1.jpg">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\12\lr-select-2.jpg">|

As you can see from the right graph in Table 12-2, the learning rate performs best at around 0.3, and any larger than that is likely to diverge. Let's apply this method to our code to try if it works.

First, design a data structure to make Table 12-3.

Table 12-3 Experimental design of learning rate and number of iterations

|Learning Rate Segment|0.0001~0.0009|0.001~0.009|0.01~0.09|0.1~0.9|1.0~1.1|
|----|----|----|----|---|---|
|Step length|0.0001|0.001|0.01|0.1|0.01|
|Iterations|10|10|10|10|10|

For each learning rate segment, iterate 10 times at each point, and then:

$$ current learning rate + step length \rightarrow next learning rate $$

Taking the first segment as an example, it will iterate 100 times on 0.1, 100 times on 0.2, ..., and 100 iterations on 0.9. The step length and the number of iterations can be set in segments to obtain Figure 12-6.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/LR_try_1.png" ch="500" />

Figure 12-6 Learning rate test of the first round

The `np.log10()` function was used for the horizontal coordinates to show the logarithmic values, so the correspondence between the horizontal coordinates and the learning rate is shown in Table 12-4.

Table 12-4 Correspondence between horizontal coordinates and learning rate

|x-coordinate|-1.0|-0.8|-0.6|-0.4|-0.2|0.0|
|---|---|---|---|---|---|---|
|Learning rate|0.1|0.16|0.25|0.4|0.62|1.0|

The first large segment decreases, indicating that the learning rate is too low at 0.1, 0.16, 0.25, and 0.4. Then we continue to explore the segment after -0.4 and get the second round of test results in Figure 12-7.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/LR_try_2.png" ch="500" />

Figure 12-7 Learning rate test of the second round

By the time -0.13 (corresponding to a learning rate of 0.74) starts, the loss value rises, so a reasonable initial learning rate should be about 0.7, so we again narrow the range of 0.6, 0.7, 0.8 to do the test and get the third round of test results, as shown in Figure 12-8.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/LR_try_3.png" ch="500" />

Figure 12-8 Learning rate test of the third round

The final best initial learning rate is about 0.8. Since the loss value is gradually changing from decreasing to increasing, there is an accumulation process. A smaller value than 0.8, such as 0.75, can be used as the initial learning rate to avoid the impact due to the first few iterations.

### 12.3.3 Late revision of learning rate

Using the MNIST example of 12.1 with a fixed batch size of 128, we compare the learning curves using learning rates of 0.2, 0.3, 0.5, and 0.8, respectively.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/acc_bs_128.png" ch="500" />

Figure 12-9 The number of iterations corresponding to different learning rates and the accuracy

The learning rate of 0.5 works best. Although the learning rate of 0.8 rises quickly initially, the curve of 0.5 overcomes up by 10 `epochs` and finally stabilizes above the curve of 0.8.

This gives us a hint: we can set the learning rate larger at the beginning so that the accuracy rate rises quickly and the loss value falls quickly; after a particular stage, we can switch to a lower learning rate to continue training. Expressed in the formula:

$$
LR_{new}=LR_{current} * DecayRate^{GlobalStep/DecaySteps} \tag{4}
$$

For example

- Current LR = 0.1
- DecayRate = 0.9
- DecaySteps = 50

The equation becomes:

$$lr = 0.1 * 0.9^{GlobalSteps/50}$$

It means that the initial learning rate is 0.1, and a new $lr$ is calculated every 50 training rounds, which is $0.9^n$ times the current one, where $n$ is a positive integer because the result of $GlobalSteps/50$ is generally rounded, so $n=1,2,3,\ldots$

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/lr_decay.png" ch="500" />

Figure 12-10 Step Decay Learning Rate Method

If we calculate the specific values of decay for every 50 rounds, see Table 12-5.

Table 12-5 Learning rate decay value calculation

|Iterations|0|50|100|150|200|250|300|...|
|---|---|---|---|---|---|---|---|---|
|Learning rate|0.1|0.09|0.081|0.073|0.065|0.059|0.053|...|

In this way, it is possible to converge quickly at the beginning and become cautious later on, carefully approaching the extrema to avoid jumping over due to large step size.

The algorithm described above is called the STEP algorithm, and there are some other algorithms as follows.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/lr_policy.png" ch="500" />

Figure 12-11 Various other learning rate decrease algorithms

#### fixed

Use a fixed learning rate, e.g., 0.1 throughout, and note that this value should not be large. Otherwise, it will not converge quickly when approaching the extrema at a later stage.

#### step

Adjust the learning rate down after every preset number of iterations (e.g. 500 steps). Discrete, simple and practical.

#### multistep

Preset a few iterations and adjust the learning rate down when it reaches. Unlike STEP, the number of iterations here can be uneven, e.g. 3000, 5500, 8000. Discrete, practical and straightforward.

#### exp

The continuous exponentially varying learning rate, with the equation:

$$lr_{new}=lr_{base} * \gamma^{iteration} \tag{5}$$

Since the general iteration is extensive (training requires many iterations), the learning rate decays quickly. The $\gamma$ can take values close to 1, such as 0.9, 0.99, etc. The larger the value, the slower the decay of the learning rate.

#### inv

The inverse type variation, equation is:

$$lr_{new}=lr_{base} * \frac{1}{( 1 + \gamma * iteration)^{p}} \tag{6}$$

$\gamma$ controls the descent rate. The larger the value, the faster the descent rate; $p$ controls the minimum value of the limit. The larger the value, the smaller the minimum value. We can use 0.5 to do the default value.

#### poly

The polynomial decay, the equation is:

$$lr_{new}=lr_{base} * (1 - {iteration \over iteration_{max}})^p \tag{7}$$

When $p=1$, it is a linear decline; when $p>1$, the downward trend protrudes upward; the downward trend is concave when $p<1$. $p$ can be set to 0.9.

### 12.3.4 The relationship between learning rate and batch size

#### Experimental results

Let's go back to the MNIST example and continue the experiment. When the batch size is 32, the best learning rate is still 0.5, as shown in Figure 12-12.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/acc_bs_32.png" ch="500" />

Figure 12-12 Comparison of several learning rates for a batch size of 32

Is 0.5 a global best learning rate? Stay calm and continue to reduce the batch size to 16 and observe the accuracy curve again. Since the batch size is reduced by half, there will be twice as many iterations in Figure 12-13 as in Figure 12-12 to complete the same `epoch`.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/acc_bs_16.png" ch="500" />

Figure 12-13 Comparison of several learning rates when the batch size is 16

There is a significant change this time, and the best learning rate is 0.1, which shows that when the batch size is small to a specific order of magnitude, the learning rate should match the batch size. A larger learning rate matches a larger batch, and vice versa.

#### Reason explanation

We got this intuition from the experiment: a large batch value should correspond to a high learning rate. Otherwise, the convergence is very slow; a small batch value should correspond to a low learning rate. Otherwise, it will not converge to the optimal point.

An extreme case is when the batch size is 1, i.e., a single sample, where we are not sure that the gradient's direction is the correct direction due to the presence of noise. Still, we cannot ignore the role of this sample, so a minimal learning rate is often used. This situation is well suited to the online-learning scenario, i.e., streaming training.

The advantage of using a Mini-batch is that it can overcome the noise of a single sample. Due to the sample noise problem, it uses a slightly larger learning rate to make the convergence faster without deviating. From the perspective of bias-variance, the probability of bias is larger for single samples and smaller for multiple samples. Due to the assumption of I.I.D. (independent and identical distribution), the variance of multiple samples will not change much, i.e., the variance of 16 examples and 32 examples should be similar, and the variance of the gradients they produce should be similar.

Usually, when we increase the batch size to N times the original one, the learning rate should be increased to m times the original one to ensure that the updated weights are equal after the same samples, according to the linear scaling rule. But if we want to ensure that the gradient variance of the weights remains the same, the learning rate should be increased to $\sqrt m$ times of the original one.

It is shown that decaying the learning rate can be achieved similarly by increasing the batch size, which is equivalent to the weight update equation of SGD. For a fixed learning rate, an optimal batch size maximizes the test accuracy, which is positively related to the learning rate and the size of the training set. There are actually two proposals for this:

1. If the learning rate is increased, then the batch size should be increased so that the convergence is more stable.
2. try to use a large learning rate because many studies have shown that a larger learning rate improves generalization. If you really want to decay, you can try other approaches to increase the batch size. The learning rate has a significant impact on the convergence of the model, so adjust it carefully.

#### Numerical comprehension

If some of the above text is not easy to understand, we use a straightforward example to illustrate the positive relationship between learning rate and batch size.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/lr_bs.png" />

Figure 12-14 Numerical comprehension of the relationship between learning rate and batch size

Let's first look at the left panel in Figure 12-14: Suppose there are three blue sample points and the correct fitting line is shown as the green line, but the current fitting result is a red line with a slope of 0.5. Let's calculate its loss function value, assuming the unit value of the grid composed of dashed lines is 1.

$$loss = \frac{1}{2m} \sum_i^m (z-y)^2 = (1^2 + 0^2 + 1^2)/2/3=0.333$$

The loss function value can be interpreted as the error backpropagation strength. At this point, a backpropagation strength of 0.333 is needed to bring the red straight line closer to the position of the green straight line, i.e., to make the $W$ value smaller and the slope change from 0.5 to 0.

Suppose we use a less accurate calculation below to illustrate the learning rate versus sample. In that case, this calculation is not really in the neural network but only a numerical and intuitive interpretation.

We need a learning rate $\eta_1$ such that:

$$w = w - \eta_1 * 0.333 = 0.5 - \eta_1 * 0.333 = 0$$

then：

$$\eta_1 = 1.5 \tag{8}$$

Looking at the right panel of Figures 12-14, the sample points become 5, with two more orange sample points, equivalent to a change in batch size from 3 to 5. Calculate the loss function value:

$$loss = (1^2+0.5^2+0^2+0.5^2+1^2)/2/5=0.25$$

The number of samples has increased, and since the samples obey the I.I.D. distribution, the new orange samples are located between the blue samples. Also, the loss function value does not increase but instead decreases from 0.333 for three sample points to 0.25 for five sample points. At which point, if one wants to return the same error strength so that the slope of w becomes 0, then:

$$w = w - \eta_2 * 0.25 = 0.5 - \eta_2 * 0.25 = 0$$

then：

$$\eta_2 = 2 \tag{9}$$

Comparing the results of Equation 8 and Equation 9, the number of samples increases, and the learning rate needs to increase accordingly.

The large batch size can reduce the number of iterations and thus the training time; on the other hand, the gradient calculation is more stable, and the curve is smooth for large batch sizes. Within a specific range, increasing the batch size helps the stability of convergence, but an excessive batch size will make the generalization ability of the model decrease and the error of validation or testing increase.

The increase of batch size can be relatively arbitrary, such as from 16 to 32, 64, 128, etc., while the learning rate has an upper limit. From the equation 2 and 3, know that the learning rate can not be greater than 1.0. It is the same as the Sigmoid function, the input value can vary greatly, but a large input value will get close to the output value of 1. Therefore the relationship between batch size and learning rate can be roughly summarized as follows:

1. when you increase the batch size, you need to increase the learning rate to adapt. You can use the linear scaling rules proportional to the scaling
2. At a certain point, the increase in learning rate will shrink to $\sqrt m$ times the batch size
3. to a more extreme situation, no matter how much the batch size is increased, the learning rate cannot be increased
