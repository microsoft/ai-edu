<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# Chapter 14 Building a DNN (Deep Neural Network) Framework

## 14.0 Deep Neural Network Framework Design

### 14.0.1 Function/Mode Analysis

Compared to the code of the three-layer neural network in Chapter 12, we see lots of similarities, such as in the forward calculation:

```Python
def forward3(X, dict_Param):
    ...
    # layer 1
    Z1 = np.dot(W1,X) + B1
    A1 = Sigmoid(Z1)
    # layer 2
    Z2 = np.dot(W2,A1) + B2
    A2 = Tanh(Z2)
    # layer 3
    Z3 = np.dot(W3,A2) + B3
    A3 = Softmax(Z3)
    ...    
```

There is a repeating pattern in layers 1, 2, and 3: a matrix operation and an activation/classification function.

Let us reconsider backpropagation：

```Python
def backward3(dict_Param,cache,X,Y):
    ...
    # layer 3
    dZ3= A3 - Y
    dW3 = np.dot(dZ3, A2.T)
    dB3 = np.sum(dZ3, axis=1, keepdims=True)
    # layer 2
    dZ2 = np.dot(W3.T, dZ3) * (1-A2*A2) # tanh
    dW2 = np.dot(dZ2, A1.T)
    dB2 = np.sum(dZ2, axis=1, keepdims=True)
    # layer 1
    dZ1 = np.dot(W2.T, dZ2) * A1 * (1-A1)   #sigmoid
    dW1 = np.dot(dZ1, X.T)
    dB1 = np.sum(dZ1, axis=1, keepdims=True)
    ...
```
The pattern of each layer is also very similar: calculate `dZ` of a layer, and then calculate `dW` and `dB` according to `dZ`.

As the three-layer network has one more layer than the two-layer network. So, the initialization, forward calculation, backward propagation, and updating parameters stages are slightly different. In addition, in the previous chapters, in order to achieve some auxiliary functions, we have written a lot of classes. So, now you can build a mini-framework for deep learning.

### 14.0.2 Abstraction and Design

Figure 14-1 is the modular design of the mini frame. The following is an explanation of the functional points of each module.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/class.png" />

Figure 14-1 Mini frame design

#### NeuralNet

First, a `NeuralNet` class is needed to package the basic neural network structure and functions:

- `Layers` - the containers of each layer of the neural network, maintains a list in the order of addition
- `Parameters` - basic parameters, including common parameters and hyperparameters
- `Loss Function` - provides the function of calculating loss function value, storing historical records, and finally drawing it
- `LayerManagement()` - addd a neural network layer
- `ForwardCalculation()` - calls the forward calculation method on each layer
- `BackPropagation()` - calls back propagation method on each layer
- `PreUpdateWeights()` - pre-updates the weight parameters for each layer
- `UpdateWeights()` - updates the weight parameters for each layer
- `Train()` - training
- `SaveWeights()` - saves the weight parameters for each layer
- `LoadWeights()` - loads the weight parameters for each layer

#### Layer

It is an abstract class, and the actual class requires more, including:

- Fully Connected Layer
- Classification Layer
- Activator Layer
- Dropout Layer
- Batch Norm Layer

In the future, it will also include:

- Convolution Layer
- Max Pool Layer

Each Layer includes the following basic methods：
 - `ForwardCalculation()` - calls the forward calculation method of this layer
 - `BackPropagation()` - calls the backpropagation method of this layer
 - `PreUpdateWeights()` - pre-updates the weight parameters of this layer
 - `UpdateWeights()` - updates the weight parameters of this layer
 - `SaveWeights()` - saves the weight parameters of this layer
 - `LoadWeights()` - loads the weight parameters of this layer

#### Activator Layer

Activation function and classification function:

- `Identity` - Direct transfer function, so no activation processing
- `Sigmoid`
- `Tanh`
- `Relu`

#### Classification Layer

Classification functions, including:

-`Sigmoid` - two categories
-`Softmax` - multi-category


 #### Parameters

Basic neural network operating parameters:

-Learning rate
-Maximum epoch
-`batch size`
-Loss function definition
-Initialization method
-Optimizer type
-Stop condition
-Regular types and conditions

#### LossFunction

Loss function and helper methods:

-Mean square error function
-Binary cross-entropy function
-Multi-class cross-entropy function
-Record loss function
-Display loss function history
-The weight parameter when obtaining the minimum function value

#### Optimizer

Optimizer:

- `SGD`
- `Momentum`
- `Nag`
- `AdaGrad`
- `AdaDelta`
- `RMSProp`
- `Adam`

#### WeightsBias

The weight matrix is only used by the fully connected layer:

- Initialization
   - `Zero`, `Normal`, `MSRA` (`HE`), `Xavier`
   - Saves the initialization value
   - Loads initialization value
- `Pre_Update` - Pre-updates
- `Update` - Updates
- `Save` - Saves the training result value
- `Load` - Loads training result value

#### DataReader

Sample data reader:

- `ReadData` - Reads data from a file
- `NormalizeX` - Normalizes sample value
- `NormalizeY` - Normalizes label value
- `GetBatchSamples` - Gets batch data
- `ToOneHot` - The tag value becomes OneHot code for multi-category
- `ToZeroOne` - The tag value becomes 0/1 code for binary classification
- `Shuffle` - Shuffles the order of samples

Two data readers are derived from it:

- `MnistImageDataReader` - Reads MNIST data
- `CifarImageReader` - Reads Cifar10 data
