<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# Chapter 14 Building a Deep Neural Network Framework

## 14.0 Deep Neural Network Framework Design

### 14.0.1 Function/Mode Analysis

Compared to code of the three-layer neural network in Chapter 12, we can see lots of repetition, such as in the forward propagation:

```Python
def forward3(X, dict_Param):
    ...
    # layer 1
    Z1 = np.dot(W1,X) + B1
    A1 = Sigmoid(Z1)
    # layer 2
    Z2 = np.dot(W2,A1) + B2
    A2 = Tanh(Z2)
    # layer 3
    Z3 = np.dot(W3,A2) + B3
    A3 = Softmax(Z3)
    ...    
```

There is a repeating pattern in layers 1, 2, and 3: matrix operation + activation/classification function.

Let us reconsider backpropagation：

```Python
def backward3(dict_Param,cache,X,Y):
    ...
    # layer 3
    dZ3= A3 - Y
    dW3 = np.dot(dZ3, A2.T)
    dB3 = np.sum(dZ3, axis=1, keepdims=True)
    # layer 2
    dZ2 = np.dot(W3.T, dZ3) * (1-A2*A2) # tanh
    dW2 = np.dot(dZ2, A1.T)
    dB2 = np.sum(dZ2, axis=1, keepdims=True)
    # layer 1
    dZ1 = np.dot(W2.T, dZ2) * A1 * (1-A1)   #sigmoid
    dW1 = np.dot(dZ1, X.T)
    dB1 = np.sum(dZ1, axis=1, keepdims=True)
    ...
```
The pattern of each layer is also very similar: calculate `dZ` of a layer, and then calculate `dW` and `dB` according to `dZ`.

As the three-layer network has one more layer than the two-layer network, the four stages of initialization, forward propagation, backward propagation, and updating parameters is slightly different. In addition, in the previous chapters, in order to achieve some auxiliary functions, we have written a lot of classes. So, now you can build a mini-framework for deep learning.

### 14.0.2 Abstraction and Design

Figure 14-1 is the modular design of the mini frame. The following is an explanation of the functional points of each module.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/class.png" />

Figure 14-1 Mini frame design

#### NeuralNet

First, a `NeuralNet` class is needed to package the basic neural network structure and functions:

- `Layers` - the containers of each layer of the neural network, maintains a list in the order of addition
- `Parameters` - basic parameters, including common parameters and hyperparameters
- `Loss Function` - provides the function of calculating loss function value, storing historical records, and finally drawing it
- `LayerManagement()` - addd a neural network layer
- `ForwardCalculation()` - calls the forward propagation method on each layer
- `BackPropagation()` - calls back propagation method on each layer
- `PreUpdateWeights()` - pre-updates the weight parameters for each layer
- `UpdateWeights()` - updates the weight parameters for each layer
- `Train()` - training
- `SaveWeights()` - saves the weight parameters for each layer
- `LoadWeights()` - loads the weight parameters for each layer

#### Layer

是一个抽象类，以及更加需要增加的实际类，包括：

- Fully Connected Layer
- Classification Layer
- Activator Layer
- Dropout Layer
- Batch Norm Layer

将来还会包括：

- Convolution Layer
- Max Pool Layer

每个Layer都包括以下基本方法：
 - `ForwardCalculation()` - 调用本层的前向计算方法
 - `BackPropagation()` - 调用本层的反向传播方法
 - `PreUpdateWeights()` - 预更新本层的权重参数
 - `UpdateWeights()` - 更新本层的权重参数
 - `SaveWeights()` - 保存本层的权重参数
 - `LoadWeights()` - 加载本层的权重参数

#### Activator Layer

激活函数和分类函数：

- `Identity` - 直传函数，即没有激活处理
- `Sigmoid`
- `Tanh`
- `Relu`

#### Classification Layer

分类函数，包括：

- `Sigmoid`二分类
- `Softmax`多分类


 #### Parameters

 基本神经网络运行参数：

 - 学习率
 - 最大`epoch`
 - `batch size`
 - 损失函数定义
 - 初始化方法
 - 优化器类型
 - 停止条件
 - 正则类型和条件

#### LossFunction

损失函数及帮助方法：

- 均方差函数
- 交叉熵函数二分类
- 交叉熵函数多分类
- 记录损失函数
- 显示损失函数历史记录
- 获得最小函数值时的权重参数

#### Optimizer

优化器：

- `SGD`
- `Momentum`
- `Nag`
- `AdaGrad`
- `AdaDelta`
- `RMSProp`
- `Adam`

#### WeightsBias

权重矩阵，仅供全连接层使用：

- 初始化 
  - `Zero`, `Normal`, `MSRA` (`HE`), `Xavier`
  - 保存初始化值
  - 加载初始化值
- `Pre_Update` - 预更新
- `Update` - 更新
- `Save` - 保存训练结果值
- `Load` - 加载训练结果值

#### DataReader

样本数据读取器：

- `ReadData` - 从文件中读取数据
- `NormalizeX` - 归一化样本值
- `NormalizeY` - 归一化标签值
- `GetBatchSamples` - 获得批数据
- `ToOneHot` - 标签值变成OneHot编码用于多分类
- `ToZeroOne` - 标签值变成0/1编码用于二分类
- `Shuffle` - 打乱样本顺序

从中派生出两个数据读取器：

- `MnistImageDataReader` - 读取MNIST数据
- `CifarImageReader` - 读取Cifar10数据
