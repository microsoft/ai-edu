<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 14.5 Multi-class classification function test

In Chapter 11, we shared how to use a neural network to do multi-class classification. In this section, we will use the Mini framework to reproduce the case study, and then use a real world case to verify the usage of multi-class classification. 

### 14.5.1 Build a model

#### Model

A two-layer network with Sigmoid as the activation function, shown in Figure 14-12. 

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_net_sigmoid.png" />

Figure 14-12 Completed abstract model of non-linear multi-class classification teaching case 

#### Code

```Python
def model_sigmoid(num_input, num_hidden, num_output, hp):
    net = NeuralNet_4_0(hp, "chinabank_sigmoid")

    fc1 = FcLayer_1_0(num_input, num_hidden, hp)
    net.add_layer(fc1, "fc1")
    s1 = ActivationLayer(Sigmoid())
    net.add_layer(s1, "Sigmoid1")

    fc2 = FcLayer_1_0(num_hidden, num_output, hp)
    net.add_layer(fc2, "fc2")
    softmax1 = ClassificationLayer(Softmax())
    net.add_layer(softmax1, "softmax1")

    net.train(dataReader, checkpoint=50, need_test=True)
    net.ShowLossHistory()
    
    ShowResult(net, hp.toString())
    ShowData(dataReader)
```

#### Hyperparameter description 

1. Hidden layer 8 neurons 
2. Maximum `epoch=5000` 
3. Batch size = 10 
4. Learning rate = 0.1 
5. Absolute error stop condition = 0.08 
6. Multi-class network type 
7. The initialization method is Xavier 

The `net.train()` function is a blocking functionas it only returns when the training is complete.

#### Results of execution

The training process is shown in Figure 14-13, and the classification effect is shown in Figure 14-14.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_loss_sigmoid.png" />

Figure 14-13 Changes in loss function value and accuracy rate while training 

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_result_sigmoid.png" ch="500" />

Figure 14-14 Classification rendering 

### 14.5.2 Build another model

#### Model

A three-layer network using ReLU as the activation function, as shown in Figure 14-15. 

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_net_relu.png" />

Figure 14-15 Using the ReLU function abstract model 

It can also be implemented with a two-layer network. However, when the ReLE function is used, the training effect is not very stable, and so it is safer to use three layers.

#### Code

```Python
def model_relu(num_input, num_hidden, num_output, hp):
    net = NeuralNet_4_0(hp, "chinabank_relu")

    fc1 = FcLayer_1_0(num_input, num_hidden, hp)
    net.add_layer(fc1, "fc1")
    r1 = ActivationLayer(Relu())
    net.add_layer(r1, "Relu1")

    fc2 = FcLayer_1_0(num_hidden, num_hidden, hp)
    net.add_layer(fc2, "fc2")
    r2 = ActivationLayer(Relu())
    net.add_layer(r2, "Relu2")

    fc3 = FcLayer_1_0(num_hidden, num_output, hp)
    net.add_layer(fc3, "fc3")
    softmax = ClassificationLayer(Softmax())
    net.add_layer(softmax, "softmax")

    net.train(dataReader, checkpoint=50, need_test=True)
    net.ShowLossHistory()
    
    ShowResult(net, hp.toString())
    ShowData(dataReader)    
```

#### Hyperparameter description 

1. Implicit Layer 8 neurons 
2. Maximum `epoch=5000`
3. Batch size=10 
4. Learning rate = 0.1 
5. Absolute error stop condition = 0.08 
6. Multi-classification network type 
7. The initialization method is MSRA 

#### Results of execution

The training process is shown in Figure 14-16, and the classification effect is shown in Figure 14-17.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_loss_relu.png" />

Figure 14-16 Changes in the loss function value and accuracy rate while training 

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_result_relu.png" ch="500" />

Figure 14-17 Classification rendering 

### 14.5.3 Comparison

Table 14-1 compares the classification effect diagrams using different activation functions. 

Table 14-1 Comparison of classification results using different activation functions 

|Sigmoid|ReLU|
|---|---|
|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_result_sigmoid.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/ch11_result_relu.png'/>|

One can see that the boundary is much smoother in the picture on the left. This is the difference between ReLU and Sigmoid. ReLU uses a piecewise linear fitting to curve, and Sigmoid has real curve fitting capabilities. However, Sigmoid also has shortcomings. Consider the classification boundary. The classification boundary using the ReLU function is relatively clear, while the classification boundary using the Sigmoid function more gentle and the transition zone is wider.

TLDR, the difference between ReLU and Sigmoid is: Relu can be straight and more straight, which is suitable for square borders; Sigmoid can bend and bend more, which is suitable for circular borders. 

### Code location 

ch14, Level5
