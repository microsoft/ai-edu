<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->


## 1.2 反向传播四大公式推导

著名的反向传播四大公式是：

  $$\delta^{L} = \nabla_{a}C \odot \sigma_{'}(Z^L) \tag{80}$$
  $$\delta^{l} = ((W^{l + 1})^T\delta^{l+1})\odot\sigma_{'}(Z^l) \tag{81}$$
  $$\frac{\partial{C}}{\partial{b_j^l}} = \delta_j^l \tag{82}$$
  $$\frac{\partial{C}}{\partial{w_{jk}^{l}}} = a_k^{l-1}\delta_j^l \tag{83}$$

### 1.2.1 直观理解反向传播四大公式

下面我们用一个简单的两个神经元的全连接神经网络来直观解释一下这四个公式，

<img src="../Images/14/bp.png" />

图14-

每个结点的输入输出标记如图上所示，使用MSE作为计算loss的函数，那么可以得到这张计算图中的计算过公式如下所示：

$$e_{01} = \frac{1}{2}(y-a_1^3)^2$$
$$a_1^3 = sigmoid(z_1^3)$$
$$z_1^3 = (w_{11}^2 \cdot a_1^2 + w_{12}^2 \cdot a_2^2 + b_1^3)$$
$$a_1^2 = sigmoid(z_1^2)$$
$$z_1^2 = (w_{11}^1 \cdot a_1^1 + w_{12}^1 \cdot a_2^1 + b_1^2)$$

我们按照反向传播中梯度下降的原理来对损失求梯度，计算过程如下：

$$\frac{\partial{e_{o1}}}{\partial{w_{11}^2}} = \frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}\frac{\partial{z_{1}^3}}{\partial{w_{11}^2}}=\frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}a_{1}^2$$

$$\frac{\partial{e_{o1}}}{\partial{w_{12}^2}} = \frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}\frac{\partial{z_{1}^3}}{\partial{w_{12}^2}}=\frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}a_{2}^2$$

$$\frac{\partial{e_{o1}}}{\partial{w_{11}^1}} = \frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}\frac{\partial{z_{1}^3}}{\partial{a_{1}^2}}\frac{\partial{a_{1}^2}}{\partial{z_{1}^2}}\frac{\partial{z_{1}^2}}{\partial{w_{11}^1}} = \frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}\frac{\partial{z_{1}^3}}{\partial{a_{1}^2}}\frac{\partial{a_{1}^2}}{\partial{z_{1}^2}}a_1^1$$

$$=\frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}w_{11}^2\frac{\partial{a_{1}^2}}{\partial{z_{1}^2}}a_1^1$$

$$\frac{\partial{e_{o1}}}{\partial{w_{12}^1}} = \frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}\frac{\partial{z_{1}^3}}{\partial{a_{2}^2}}\frac{\partial{a_{2}^2}}{\partial{z_{1}^2}}\frac{\partial{z_{1}^2}}{\partial{w_{12}^1}} = \frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}\frac{\partial{z_{1}^3}}{\partial{a_{2}^2}}\frac{\partial{a_{2}^2}}{\partial{z_{1}^2}}a_2^2$$

$$=\frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}w_{12}^2\frac{\partial{a_{2}^2}}{\partial{z_{1}^2}}a_2^2$$

上述式中，$\frac{\partial{a}}{\partial{z}}$是激活函数的导数，即$\sigma^{'}(z)$项。观察到在求偏导数过程中有共同项$\frac{\partial{e_{o1}}}{\partial{a_{1}^3}}\frac{\partial{a_{1}^3}}{\partial{z_{1}^3}}$,采用$\delta$符号记录,用矩阵形式表示，
即：

$$\delta^L = [\frac{\partial{e_{o1}}}{\partial{a_{i}^L}}\frac{\partial{a_{i}^L}}{\partial{z_{i}^L}}] = \nabla_{a}C\odot\sigma^{'}(Z^L)$$

上述式中，$[a_i]$表示一个元素是a的矩阵，$\nabla_{a}C$表示将损失$C$对$a$求梯度，$\odot$表示矩阵element wise的乘积（也就是矩阵对应位置的元素相乘）。

从上面的推导过程中，我们可以得出$\delta$矩阵的递推公式：

$$\delta^{L-1} = (W^L)^T[\frac{\partial{e_{o1}}}{\partial{a_{i}^L}}\frac{\partial{a_{i}^L}}{\partial{z_{i}^L}}]\odot\sigma^{'}(Z^{L - 1})$$

所以在反向传播过程中只需要逐层利用上一层的$\delta^l$进行递推即可。

相对而言，这是一个非常直观的结果，这份推导过程也是不严谨的。下面，我们会从比较严格的数学定义角度进行推导，首先要补充一些定义。


### 1.2.2 神经网络有关公式证明

+ 首先，来看一个通用情况，已知$f = A^TXB$，$A,B$是常矢量，希望得到$\frac{\partial{f}}{\partial{X}}$，推导过程如下

  根据式(94)，

  $$
  df = d(A^TXB) = d(A^TX)B + A^TX(dB) = d(A^TX)B + 0 = d(A^T)XB+A^TdXB = A^TdXB
  $$

  由于$df$是一个标量，标量的迹等于本身，同时利用公式(99):

  $$
  df = tr(df) = tr(A^TdXB) = tr(BA^TdX)
  $$

  由于公式(92):

  $$
  tr(df) = tr({(\frac{\partial{f}}{\partial{X}})}^TdX)
  $$

  可以得到:

  $$
  (\frac{\partial{f}}{\partial{X}})^T = BA^T
  $$
  $$
  \frac{\partial{f}}{\partial{X}} = AB^T \tag{101}
  $$

+ 我们来看全连接层的情况：

  $$ Y = WX + B$$

  取全连接层其中一个元素

  $$ y = wX + b$$

  这里的$w$是权重矩阵的一行，尺寸是$1 \times M$，X是一个大小为$M \times 1$的矢量，y是一个标量，若添加一个大小是1的单位阵，上式整体保持不变：

  $$ y = (w^T)^TXI + b$$

  利用式(92)，可以得到

  $$ \frac{\partial{y}}{\partial{X}} = I^Tw^T = w^T$$

  因此在误差传递的四大公式中，在根据上层传递回来的误差$\delta$继续传递的过程中，利用链式法则，有

  $$\delta^{L-1} = (W^L)^T \delta^L \odot \sigma^{'}(Z^{L - 1})$$

  同理，若将$y=wX+b$视作：

  $$ y = IwX + b $$

  那么利用式(92),可以得到：

  $$ \frac{\partial{y}}{\partial{w}} = X^T$$

+ 使用softmax和交叉熵来计算损失的情况下：

  $$ l = - Y^Tlog(softmax(Z))$$

  式中，$y$是数据的标签，$Z$是网络预测的输出，$y$和$Z$的维度是$N \times 1$。经过softmax处理作为概率。希望能够得到$\frac{\partial{l}}{\partial{Z}}$，下面是推导的过程：

  $$
  softmax(Z) = \frac{exp(Z)}{\boldsymbol{1}^Texp(Z)}
  $$

  其中， $\boldsymbol{1}$是一个维度是$N \times 1$的全1向量。将softmax表达式代入损失函数中，有

  $$
  dl = -Y^T d(log(softmax(Z)))\\
  = -Y^T d (log\frac{exp(Z)}{\boldsymbol{1}^Texp(Z)}) \\
  = -Y^T dZ + Y^T \boldsymbol{1}d(log(\boldsymbol{1}^Texp(Z))) \tag{102}
  $$

  下面来化简式(102)的后半部分,利用式(98)：

  $$
  d(log(\boldsymbol{1}^Texp(Z))) = log^{'}(\boldsymbol{1}^Texp(Z)) \odot dZ
  = \frac{\boldsymbol{1}^T(exp(Z)\odot dZ)}{\boldsymbol{1}^Texp(Z)}
  $$

  利用式(100)，可以得到

  $$
  tr(Y^T \boldsymbol{1}\frac{\boldsymbol{1}^T(exp(Z)\odot dZ)}{\boldsymbol{1}^Texp(Z)}) =
  tr(Y^T \boldsymbol{1}\frac{(\boldsymbol{1} \odot (exp(Z))^T dZ)}{\boldsymbol{1}^Texp(Z)})
  $$
  $$ =
  tr(Y^T \boldsymbol{1}\frac{exp(Z)^T dZ}{\boldsymbol{1}^Texp(Z)}) = tr(Y^T \boldsymbol{1} softmax(Z)^TdZ) \tag{103}
  $$

  将式(103)代入式(102)并两边取迹，可以得到：

  $$
  dl = tr(dl) = tr(-y^T dZ + y^T\boldsymbol{1}softmax(Z)^TdZ) = tr((\frac{\partial{l}}{\partial{Z}})^TdZ)
  $$

  在分类问题中，一个标签中只有一项会是1，所以$Y^T\boldsymbol{1} = 1$，因此有

  $$
  \frac{\partial{l}}{\partial{Z}} = softmax(Z) - Y
  $$

  这也就是在损失函数中计算反向传播的误差的公式。


### 参考资料

  [矩阵求导术](https://zhuanlan.zhihu.com/p/24709748)

