Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 知识点

# 提出问题

手写识别是人工智能的重要课题之一。MNIST数字手写体识别图片集，大家一定不陌生，下面就是一些样本。

<img src='./images/9/Mnist.png'/>

由于这是从欧美收集的数据，从图中可以看出有几点和中国人的手写习惯不一样：

- 数字2，下面多一个圈
- 数字4，很多横线不出头
- 数字6，上面是直的
- 数字7，中间有个横杠

不过这些细节不影响咱们学习课程，正好还可以验证一下中国人的手写习惯。

MNIST数字手写体识别图片集，大家一定不陌生，[这是该网站的地址](http://yann.lecun.com/exdb/mnist/)。但是从上述地址下载的文件是压缩的，在解压缩过程中很可能会出现意想不到的问题，比如长度不对，后面补零等等，所以我们也准备了解压缩好的数据。

由于不是让我们识别26个英文字母或者3500多个常用汉字，所以问题还算是比较简单，不需要图像处理只是。咱们可以试试用一个两层的神经网络能不能解决，把每个图片的像素都当作一个向量来看，而不是作为点阵。

# 定义神经网络结构

<img src='./images/9/setupNN.jpg'/>

矩阵运算过程：
1. $W1(32,768) * X(768,1) + B1(32,1) => Z1(32,1)$
2. $Sigmoid(Z1) => A1(32,1)$
3. $W2(10,32) * A1(32,1) + B2(10,1) => Z2(10,1)$
4. $Softmax(Z2) => A2(10,1)$

## 输入层

$$
X = \begin{pmatrix}
X_1 & X_2 \dots X_{50000} 
\end{pmatrix}
= \begin{pmatrix}
x_{1,1} & x_{2,1} \dots x_{5000,1} \\
x_{1,2} & x_{2,2} \dots x_{5000,2} \\
\dots \\
x_{1,784} & x_{2,784} \dots x_{5000,784}
\end{pmatrix}
$$

我们的约定是行是样本，列一个样本的所有特征，这里是784个特征，因为图片高和宽是28x28，总共784个点。

## 中间层（隐藏层）

定义32个神经元，当然在代码里要把这个数字写成参数可调的。

$$
Z1 = \begin{pmatrix}
Z1_1 \\ 
Z1_2 \\ 
\dots \\
Z1_{32} \end{pmatrix},
A1 = \begin{pmatrix}
A1_1 \\ 
A1_2 \\ 
\dots \\
A1_{32} \end{pmatrix},
$$

其中，$Z1=W1*X+B1，A1=Sigmoid(Z1)$，大1表示第一层神经网络。

## 权重矩阵W1/B1
$$
W1=\begin{pmatrix}
w_{1,1} & w_{1,2} \dots w_{1,768}\\
w_{2,1} & w_{2,2} \dots w_{2,768} \\
\dots \\
w_{32,1} & w_{32,2} \dots w_{32,768}
\end{pmatrix}
$$

B1的尺寸是32x1，行数永远和W一样，列数永远是1。

$$
B1=\begin{pmatrix}
b_1 \\
b_2 \\
\dots \\
b_{32}
\end{pmatrix}
$$

## 输出层

输出层10个神经元，再加上一个Softmax计算，最后有a1,a2,...a10十个输出，写作：

$$
Z2 = \begin{pmatrix}
z_1 \\ 
z_2 \\ 
\dots \\
z_{10} \end{pmatrix},
A2 = \begin{pmatrix}
a_1 \\ 
a_2 \\ 
\dots \\
a_{10} \end{pmatrix}
$$

其中，$Z2=W2*A1+B2，A2=Softmax(Z2)$，大2表示第二层神经网络。

## 权重矩阵W2/B2

$$
W2=\begin{pmatrix}
w_{1,1} & w_{1,2} \dots w_{1,32}\\
w_{2,1} & w_{2,2} \dots w_{2,32} \\
\dots \\
w_{10,1} & w_{10,2} \dots w_{10,32}
\end{pmatrix}
$$

B2的尺寸是10x1，行数永远和W一样，列数永远是1。

$$
B2=\begin{pmatrix}
b_1 \\
b_2 \\
\dots \\
b_{10}
\end{pmatrix}
$$

# 下载训练数据

[点击下载训练及标签数据](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/Mnist.zip)

把下载的数据解压到一个目录中，最好是你的.py文件所在目录的.\Mnist子目录下，一共8个文件：
- Mnist-10-train-images.dat，0~9训练样本集
- Mnist-10-train-labels.dat，0~9训练标签集
- Mnist-10-test-images.dat，0~9测试样本集
- Mnist-10-test-labels.dat，0~9测试标签集
- Mnist-2-train-images.dat，0~1训练标签集
- Mnist-2-train-labels.dat，0~1训练标签集
- Mnist-2-test-images.dat，0~1测试样本集
- Mnist-2-test-labels.dat，0~1测试标签集
数据格式和原网站上的一致。

# 读取文件数据
```Python
def ReadImageFile(image_file_name):
    f = open(image_file_name, "rb")
    a = f.read(4)
    b = f.read(4)
    num_images = int.from_bytes(b, byteorder='big')
    c = f.read(4)
    num_rows = int.from_bytes(c, byteorder='big')
    d = f.read(4)
    num_cols = int.from_bytes(d, byteorder='big')

    image_size = num_rows * num_cols    # 784
    fmt = '>' + str(image_size) + 'B'
    image_data = np.empty((image_size, num_images)) # 784 x M
    for i in range(num_images):
        bin_data = f.read(image_size)
        unpacked_data = struct.unpack(fmt, bin_data)
        array_data = np.array(unpacked_data)
        array_data2 = array_data.reshape((image_size, 1))
        image_data[:,i] = array_data
    f.close()
    return image_data

def ReadLabelFile(lable_file_name, num_output):
    f = open(lable_file_name, "rb")
    f.read(4)
    a = f.read(4)
    num_labels = int.from_bytes(a, byteorder='big')

    fmt = '>B'
    label_data = np.zeros((num_output, num_labels))   # 10 x M
    for i in range(num_labels):
        bin_data = f.read(1)
        unpacked_data = struct.unpack(fmt, bin_data)[0]
        label_data[unpacked_data,i] = 1
    f.close()
    return label_data
```

# 前向计算
```Python
def ForwardCalculation(X, dict_Param):
    W1 = dict_Param["W1"]
    B1 = dict_Param["B1"]
    W2 = dict_Param["W2"]
    B2 = dict_Param["B2"]
    
    Z1 = np.dot(W1,X)+B1
    A1 = Sigmoid(Z1)
    #A1 = np.tanh(Z1)

    Z2=np.dot(W2,A1)+B2
    A2=Softmax(Z2)
    
    dict_Cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    return A2, dict_Cache
```

# 定义代价函数
```Python
# cross entropy: -Y*lnA
def CalculateLoss(dict_Param, X, Y, count):
    A2, dict_Cache = ForwardCalculation(X, dict_Param)
    p = Y * np.log(A2)
    Loss = -np.sum(p) / count
    return Loss
```

# 反向传播
```Python
def BackPropagation(dict_Param,cache,X,Y):
    W1=dict_Param["W1"]
    W2=dict_Param["W2"]
    A1 = cache["A1"]
    A2 = cache["A2"]
    Z1=cache["Z1"]

    dZ2= A2 - Y
    dW2 = np.dot(dZ2, A1.T)
    dB2 = np.sum(dZ2, axis=1, keepdims=True)

    dLoss_A1 = np.dot(W2.T, dZ2)
    dA1_Z1 = A1 * (1 - A1)     # sigmoid
    #dA1_Z1 = 1-np.power(A1,2)   # tanh
    dZ1 = dLoss_A1 * dA1_Z1
    
    dW1 = np.dot(dZ1, X.T)
    dB1 = np.sum(dZ1, axis=1, keepdims=True)

    dict_Grads = {"dW1": dW1, "dB1": dB1, "dW2": dW2, "dB2": dB2}
    return dict_Grads

def UpdateParam(dict_Param, dict_Grads, learning_rate):
    W1 = dict_Param["W1"]
    B1 = dict_Param["B1"]
    W2 = dict_Param["W2"]
    B2 = dict_Param["B2"]

    dW1 = dict_Grads["dW1"]
    dB1 = dict_Grads["dB1"]
    dW2 = dict_Grads["dW2"]
    dB2 = dict_Grads["dB2"]

    W1 = W1 - learning_rate * dW1
    B1 = B1 - learning_rate * dB1
    W2 = W2 - learning_rate * dW2
    B2 = B2 - learning_rate * dB2

    dict_Param = {"W1": W1, "B1": B1, "W2": W2, "B2": B2}
    return dict_Param
```

# 数据归一化
```Python
def NormalizeData(X):
    X_NEW = np.zeros(X.shape)
    # get number of features
    n = X.shape[0]
    for i in range(n):
        x_row = X[i,:]
        x_max = np.max(x_row)
        x_min = np.min(x_row)
        if x_max != x_min:
            x_new = (x_row - x_min)/(x_max-x_min)
            X_NEW[i,:] = x_new
    return X_NEW
```

# 帮助函数
```Python
def Sigmoid(x):
    s=1/(1+np.exp(-x))
    return s

def Softmax(Z):
    shift_z = Z - np.max(Z)
    exp_z = np.exp(shift_z)
    #s = np.sum(exp_z)
    s = np.sum(exp_z, axis=0)
    A = exp_z / s
    return A
```

# 初始化
```Python
def InitialParameters(num_input, num_hidden, num_output, flag):
    if flag == 0:
        # zero
        W1 = np.zeros((num_hidden, num_input))
        W2 = np.zeros((num_output, num_hidden))
    elif flag == 1:
        # normalize
        W1 = np.random.normal(size=(num_hidden, num_input))
        W2 = np.random.normal(size=(num_output, num_hidden))
    elif flag == 2:
        #
        W1=np.random.uniform(-np.sqrt(6)/np.sqrt(num_input+num_hidden),np.sqrt(6)/np.sqrt(num_hidden+num_input),size=(num_hidden,num_input))
        W2=np.random.uniform(-np.sqrt(6)/np.sqrt(num_output+num_hidden),np.sqrt(6)/np.sqrt(num_output+num_hidden),size=(num_output,num_hidden))

    B1 = np.zeros((num_hidden, 1))
    B2 = np.zeros((num_output, 1))
    dict_Param = {"W1": W1, "B1": B1, "W2": W2, "B2": B2}
    return dict_Param
```
# 测试函数
```Python
def Test(num_output, dict_Param, num_input):
    raw_data = ReadImageFile(test_image_file)
    X = NormalizeData(raw_data)
    Y = ReadLabelFile(test_label_file, num_output)
    num_images = X.shape[1]
    correct = 0
    for image_idx in range(num_images):
        x = X[:,image_idx].reshape(num_input, 1)
        y = Y[:,image_idx].reshape(num_output, 1)
        A2, dict_Cache = ForwardCalculation(x, dict_Param)
        if np.argmax(A2) == np.argmax(y):
            correct += 1
    return correct, num_images
```

# 程序主循环
```Python
print("Loading...")
learning_rate = 0.1
num_hidden = 32
num_output = 10

raw_data = ReadImageFile(train_image_file)
X = NormalizeData(raw_data)
Y = ReadLabelFile(train_label_file, num_output)

num_images = X.shape[1]
num_input = X.shape[0]
max_iteration = 1

dict_Param = InitialParameters(num_input, num_hidden, num_output, 1)

print("Training...")
for iteration in range(max_iteration):
    for item in range(num_images):
        x = X[:,item].reshape(num_input,1)
        y = Y[:,item].reshape(num_output,1)
        A2, dict_Cache = ForwardCalculation(x, dict_Param)
        dict_Grads = BackPropagation(dict_Param, dict_Cache, x, y)
        dict_Param = UpdateParam(dict_Param, dict_Grads, learning_rate)
        if item % 1000 == 0:
            Loss = CalculateLoss(dict_Param, X, Y, num_images)
            print(item, Loss)
    print(iteration)

print("Testing...")
correct, count = Test(num_output, dict_Param, num_input)
print(str.format("rate={0} / {1} = {2}", correct, count, correct/count))
```



