Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 知识点

- 两层神经网络结构设计与反向传播
- 复杂函数拟合的基本方法
- 参数对训练的影响

# 提出问题

早在1990年左右，数学理论证明：具有足够数量神经元的两层神经网络能够拟合任意精度的连续函数。所以，今天咱们就用实际数据来验证一下这个理论。

### 问题：我们给出如下一批训练数据，如何使用神经网络方法来拟合这条曲线？

|样本|1|2|3|...|1000|
|---|---|---|---|---|---|
|x|0.606|0.129|0.643|...|0.199|
|y|-0.113|-0.269|-0.217|...|-0.281|

我们可视化一下训练数据：

<img src=".\Images\8\Sample.png">

首先观察一下样本数据的范围，x是在[0,1]，y是[-1,1]，这样我们就不用做数据归一化了。这条线看起来像一条蛇！它的数学原型公式是：

$$y=0.4x^2 + 0.3xsin(15x) + 0.01cos(50x)-0.3$$

要是觉得这个公式还不够复杂，大家可以用更复杂的公式去自己试验。

# 定义神经网络结构

我们定义一个两层的神经网络，输入层不算，一个隐藏层，含128个神经元，一个输出层。

<img src=".\Images\8\setup.jpg">

## 输入层

输入层就是一个标量x值。

## 权重矩阵W1/B1

它是连接两层之间的纽带，有的人理解它应该属于输入层，有的人理解应该属于隐藏层，各有各的道理，我个人倾向于把它归到隐藏层，理由是$Z1=W1*X+B1$，在X固定的前提下，W1决定了Z1的值。另外一个理由是B1的存在位置，在本例中B1是一个128x1的矩阵，它是隐藏层128个神经元的偏移，所以它应该属于隐藏层。

$$
W1=
\begin{pmatrix}
w_{1,1} \\
w_{2,1} \\
w_{3,1} \\
\dots \\
w_{128,1} \\
\end{pmatrix}
$$

其实这里的B1所在的圆圈里应该是个常数1，而B1连接到Z1-1...Z1-128的权重线B1-1...B1-128应该是个浮点数。我们为了说明问题方便，就写了个B1，而实际的B1是指B1-1...B1-128的矩阵/向量。
$$
B1=
\begin{pmatrix}
b_{1} \\
b_{2} \\
b_{3} \\
\dots \\
b_{128} \\
\end{pmatrix}
$$


## 隐藏层

我们用一个128个神经元的网络来模拟函数，这个大家可以自己试验一下，把代码中的神经元数量修改一下，然后在保持迭代次数和其它（超）参数不变的情况，看看最终的精确度有何区别，训练时间的差异，以及内存占用有何差异。

每个神经元的输入$Z1 = W1 * X + B1$，我们在这里使用双曲sigmoid正切函数，所以输出是$A1 = Sigmoid(Z1)$。当然也可以使用其它激活函数如果tanh, Relu等等。

## 权重矩阵W2/B2

与W1/B1类似，我个人认为它属于输出层。W2的尺寸是1x128，B2的尺寸是1x1。
$$
W2=
\begin{pmatrix}
w_{1,1} \\
w_{1,2} \\
w_{1,3} \\
\dots \\
w_{1,128} \\
\end{pmatrix}
$$

$$
B2=
\begin{pmatrix}
b_{1} \\
b_{2} \\
b_{3} \\
\dots \\
b_{128} \\
\end{pmatrix}
$$

## 输出层

由于我们只想完成一个拟合任务，所以输出层只有一个神经元，$Z2=W2*A1+B2$。为什么在最后一步没有用激活函数？这个后面再说。

# 训练数据

下载后拷贝到您要运行的Python文件所在的文件夹。

[点击下载训练样本数据X](https://github.com/Microsoft/ai-edu/blob/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/CurveFittingTrainData.npy)

# 读取数据
```Python
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt


train_data_name = "CurveFittingTrainData.npy"
test_data_name = "CurveFittingTestData.npy"

def ReadData():
    Trainfile = Path(train_data_name)
    Testfile = Path(test_data_name)
    if Trainfile.exists() & Testfile.exists():
        TrainData = np.load(Trainfile)
        TestData = np.load(Testfile)
        return TrainData, TestData
    
    return None,None
```

# 前向计算

<img src=".\Images\8\FWC.jpg" width="600">

至此，我们得到了以下一串公式：

$$Z1=W1*X+B1$$

$$A1=sigmoid(Z1)$$

$$Z2=W2*A1+B2$$

$$A2=Z2 \tag{这一步可以省略}$$

```Python
def ForwardCalculation(x, dictWeights):
    W1 = dictWeights["W1"]
    B1 = dictWeights["B1"]
    W2 = dictWeights["W2"]
    B2 = dictWeights["B2"]

    Z1 = np.dot(W1,x) + B1
    A1 = sigmoid(Z1)
    Z2 = np.dot(W2,A1) + B2
    A2 = Z2  # 这一步可以省略，写在这里的目的是为了保持和后续的代码风格一致

    dictCache ={"A1": A1, "A2": A2}
    return A2, dictCache
```
由于参数较多，所以我们用一个dictionary(dictWeights)来保存W,B这些参数，如果是更多层的神经网络，就会有更多的参数，我们这里使用的还是一些最基本的参数。返回一个dictCache是因为反向传播时要用到。

# 损失函数

我们仍然使用传统的均方差函数。其中，Z是每一次迭代的预测输出，Y是样本标签数据

$$Loss = \frac{1}{2}\sum_i(z_i-y_i)^2 =\frac{1}{2}(Z - Y) ^ 2$$

分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

```Python
def LossCalculation(X, Y, dictWeights, prev_loss, count):
    A2, dict_Cache = ForwardCalculation(X, dictWeights)
    LOSS = (A2 - Y)**2
    loss = LOSS.sum()/count/2
    diff_loss = abs(loss - prev_loss)
    return loss, diff_loss
```

# 反向传播

关于反向传播函数的推导过程，请参考《8-扩展阅读》部分。

```Python
def BackPropagation(x, y, dictCache, dictWeights):
    A1 = dictCache["A1"]
    A2 = dictCache["A2"]
    W2 = dictWeights["W2"]

    dLoss_Z2 = (A2 - Y)
    dW2 = np.dot(dLoss_Z2, A1.T)
    dB2 = np.sum(dLoss_Z2, axis=1, keepdims=True)

    dLoss_A1 = np.dot(W2.T, dLoss_Z2)
    dA1_Z1 = A1 * (1 - A1)
     
    dLoss_Z1 = dLoss_A1 * dA1_Z1
    dW1 = np.dot(dLoss_Z1, X.T)
    dB1 = np.sum(dLoss_Z1, axis=1, keepdims=True)

    dictGrads = {"dW1":dW1, "dB1":dB1, "dW2":dW2, "dB2":dB2}
    return dictGrads

def UpdateWeights(dictWeights, dictGrads, learningRate):
    W1 = dictWeights["W1"]
    B1 = dictWeights["B1"]
    W2 = dictWeights["W2"]
    B2 = dictWeights["B2"]

    dW1 = dictGrads["dW1"]
    dB1 = dictGrads["dB1"]
    dW2 = dictGrads["dW2"]
    dB2 = dictGrads["dB2"]

    W1 = W1 - learningRate * dW1
    W2 = W2 - learningRate * dW2
    B1 = B1 - learningRate * dB1
    B2 = B2 - learningRate * dB2

    dictWeights = {"W1": W1,"B1": B1,"W2": W2,"B2": B2}

    return dictWeights
```
# 

# 帮助函数

- 第一个实现Sigmoid函数。
- 第二个函数是初始化权重矩阵。传入的参数是1或者2都可以，如果用0初始化，训练出来的是一条直线。
- 第三个是用于显示图形化结果的函数。

```Python
def sigmoid(x):
    s=1/(1+np.exp(-x))
    return s

def InitialParameters(num_input, num_hidden, num_output, flag):
    if flag == 0:
        # zero
        W1 = np.zeros((num_hidden, num_input))
        W2 = np.zeros((num_output, num_hidden))
    elif flag == 1:
        # normalize
        W1 = np.random.normal(size=(num_hidden, num_input))
        W2 = np.random.normal(size=(num_output, num_hidden))
    elif flag == 2:
        # Xavier method
        W1=np.random.uniform(-np.sqrt(6)/np.sqrt(num_input+num_hidden),np.sqrt(6)/np.sqrt(num_hidden+num_input),size=(num_hidden,num_input))
        W2=np.random.uniform(-np.sqrt(6)/np.sqrt(num_output+num_hidden),np.sqrt(6)/np.sqrt(num_output+num_hidden),size=(num_output,num_hidden))
 
    B1 = np.zeros((num_hidden, 1))
    B2 = np.zeros((num_output, 1))
    dict_Param = {"W1": W1, "B1": B1, "W2": W2, "B2": B2}
    return dict_Param

def ShowResult(iteration, neuron, loss, sample_count):
    # draw train data
    plt.scatter(TrainData[0,:],TrainData[1,:], s=1)
    # create and draw visualized validation data
    TX = np.linspace(0,1,100).reshape(1,100)
    TY, cache = ForwardCalculation(TX, dictWeights)
    plt.scatter(TX, TY, c='r', s=2)
    plt.title(str.format("neuron={0},example={1},loss={2},iteraion={3}", neuron, sample_count, loss, iteration))
    plt.show()
```

# 主程序
```Python
TrainData = ReadData()
num_samples = TrainData.shape[1]
X = TrainData[0,:].reshape(1, num_samples)
Y = TrainData[1,:].reshape(1, num_samples)

n_input, n_hidden, n_output = 1, 128, 1
learning_rate = 0.1
eps = 1e-10
dictWeights = InitialParameters(n_input, n_hidden, n_output, 2)
max_iteration = 100000
min_loss = 0.002
loss, prev_loss, diff_loss = 0, 0, 10
loop = num_samples
for iteration in range(max_iteration):
    for i in range(loop):
        x = X[0,i]
        y = Y[0,i]
        A2, dictCache = ForwardCalculation(x, dictWeights)
        dictGrads = BackPropagation(x, y, dictCache, dictWeights)
        dictWeights = UpdateWeights(dictWeights, dictGrads, learning_rate)
   
    loss, diff_loss = LossCalculation(X, Y, dictWeights, prev_loss, num_samples)
    print(iteration,loss,diff_loss)
    if diff_loss < eps:
        break
    if loss < min_loss:
        break
    prev_loss = loss

print(loss, diff_loss)
ShowResult(iteration+1, n_hidden, min_loss, loop)
```

# 运行结果

按照上述代码的“标准”设置，我们可以得到以下结果：

<img src=".\Images\8\r128-1000.png"> 

