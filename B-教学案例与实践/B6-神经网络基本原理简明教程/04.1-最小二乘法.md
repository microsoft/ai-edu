Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

“铁柱”是一名老师，在神经网络中穿梭多年，挂了满身的蜘蛛网。“木头”是一名刚入门者，木头木脑的，有问题经常向铁柱请教。

### 最小二乘法(最小平方法 Ordinary Least Square)

木头：老师，我还学过最小二乘法，也能解决这次的问题。那为什么我们还要用神经网络呢？

铁柱：问得好！其实最小二乘法的来源，也是对损失值求导而得出的最终方程。我们来看看过程：

线性回归试图学得：

$$z(x_i)=w \cdot x_i+b$$

使得：

$$z(x_i) \simeq y_i$$

如何学得w和b呢？均方差(MSE - mean squared error)是回归任务中常用的手段：
$$
Loss = \sum_{i=1}^m(z(x_i)-y_i)^2 = \sum_{i=1}^m(y_i-wx_i-b)^2
$$
其中，$x_i和y_i$是样本值，$z_i$是预测值。实际上就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。

<img src=".\Images\4\mse.png" width="500">  

假设我们计算出初步的结果是红色虚线所示，这条直线是否合适呢？我们来计算一下图中每个点到这条直线的距离（黄色线），把这些距离的值都加起来（都是正数，不存在互相抵消的问题）成为Error。

因为上图中的几个点不在一条直线上，所以不能有一条直线能同时穿过它们。所以，我们只能想办法不断改变红色直线的角度和位置，让Error最小（不可能是0），就意味着整体偏差最小，那么最终的那条红色直线就是我们要的结果。

如果想让Error的值最小，通过对w和b求导，再令导数为0（到达最小极值），就是w和b的最优解。

推导过程如下：

$$
{\partial{Loss} \over \partial{w}} ={\partial{(\sum(y_i-wx_i-b)^2)} \over \partial{w}} = 2\sum(y_i-wx_i-b)(-x_i)=0$$
$$
简化上式：\sum(y_i-wx_i-b)x_i=0 \tag{1}
$$
$$
{\partial{Loss} \over \partial{b}} ={\partial{(\sum(y_i-wx_i-b)^2)} \over \partial{b}} = 2\sum(y_i-wx_i-b)(-1)=0$$
$$
简化上式：\sum(y_i-wx_i-b)=0 \tag{2}
$$
由式2得到：
$$
\sum b = m \cdot b = \sum{y_i} - w\sum{x_i} \tag{假设有m个样本}$$
$$ 
$$
两边除以m：
$$
b = {1 \over m}(\sum{y_i} - w\sum{x_i})=\bar y-w \bar x \tag{3}
$$
其中：
$$
\bar y = {1 \over m}\sum y_i, \bar x={1 \over m}\sum x_i \tag{4}
$$
将3代入1：
$$
\sum(y_i-wx_i-\bar y + w \bar x)x_i=0 $$
$$
\sum(x_i y_i-wx^2_i-x_i \bar y + w \bar x x_i)=0 $$
$$
\sum(x_iy_i-x_i \bar y)-w\sum(x^2_i - \bar x x_i) = 0$$
$$
w = {\sum(x_iy_i-x_i \bar y) \over \sum(x^2_i - \bar x x_i)} \tag{5}
$$
将4代回5：
$$
w={\sum (x_i \cdot y_i) - \sum x_i \cdot {1 \over m} \sum y_i \over \sum x^2_i - \sum x_i \cdot {1 \over m}\sum x_i}
$$
分子分母都乘以m：
$$
w={m\sum x_i y_i - \sum x_i \sum y_i \over m\sum x^2_i - (\sum x_i)^2} \tag{6}
$$

$$
b=\frac{1}{m}\sum_{i=1}^m(y_i-wx_i) \tag{7}
$$

而事实上，式6有很多个变种，大家会在不同的文章里看到不同版本，往往感到困惑，比如下面两个公式也是正确的解：

$$
w = {\sum y_i(x_i-\bar x) \over \sum x^2_i - (\sum x_i)^2/m} \tag{8}
$$
$$
w = {\sum x_i(y_i-\bar y) \over \sum x^2_i - \bar x \sum x_i} \tag{9}
$$

以上两个公式，如果把公式4代入，也应该可以得到和式6相同的答案，只是在代入时，需要一些运算技巧，比如，很多人不知道这个神奇的公式：

$$
\sum (x_i \bar y)= \bar y \sum x_i =\frac{1}{m}(\sum y_i) (\sum x_i) =\frac{1}{m}(\sum x_i) (\sum y_i)= \bar x \sum y_i =\sum (y_i \bar x)
$$

木头：好！我们先试一下上面这两个公式是否好用（劈里啪啦写code，铁柱看了一眼木头用的机械键盘，暗暗叹气：够吵的！）
结果为：
```
w=1.9962
b=3.0054
```

铁柱：好，现在回答你的问题：既然我们已经可以用纯数学方法的最小二乘法得到w,b的值，为什么还要学机器学习的方法呢？因为最小二乘法以及一个叫做正规方程的东西，能做的事情有两种：
$$y=a_0+a_1x+a_2x^2+ \dots + a_mx^m \tag{一元多次方程}$$
$$y=a_0+a_1x_1+a_2x_2+ \dots + a_mx_m \tag{多元一次线性方程}$$

而且有一个最重要的前提条件是：我们根据经验预估到方程的形式是上面中的两种，才能求解。

当面对更复杂的形式时，就比较吃力甚至无能为力了，比如下面两个公式：

$$y=0.4x^2 + 0.3xsin(15x) + 0.01cos(50x)-0.3 \tag{一元二次复合三角函数}$$
$$y=3x_1^2 + 4x_2 \tag{二元二次线性}$$

而在客观世界中或实际的生产环境中，我们其实根本不知道要拟合的曲线是什么形式，就根本无从下手，这时只能用神经网络来拟合了，而拟合的结果也不是一个公式，而是一个神经网络模型，一个黑盒子。

**课堂练习：用Python代码来实现任意一个版本的最小二乘法，以便得到w,b的数学解析解**

先用4.0中的内容读取数据，然后可以根据公式6,7,8,9来写代码，比如：

```Python
# 根据公式6
def method3(X,Y,m):
    m = X.shape[0]
    p = m*sum(X*Y) - sum(X)*sum(Y)
    q = m*sum(X*X) - sum(X)*sum(X)
    w = p/q
    return w

# 根据公式7
def calculate_b(X,Y,w,m):
    b = sum(Y-w*X)/m
    return b
```

代码位置：ch04\level1
