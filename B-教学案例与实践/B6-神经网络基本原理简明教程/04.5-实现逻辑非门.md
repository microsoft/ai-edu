Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 实现逻辑非门

单层神经网络，又叫做感知机，它可以轻松实现逻辑与、或、非门。由于逻辑与、或门，需要有两个变量输入，目前我们只学习了单变量输入，所以，我们可以先实现非门。

## 解决方案

很多阅读材料上会这样介绍：有公式$z=wx+b$，令$w=-1,b=1$，则：

- 当x=0时，$z = -1 \times 0 + 1 = 1$
- 当x=1时，$z = -1 \times 1 + 1 = 0$
  
如下图所示：

<img src="./Images/4/LogicNotGate.png"/>

这是人工找到的精确解，如何使用单层神经网络训练出这个结果呢？我们先分析一下输入数据：

||样本1|样本2|
|---|---|---|
|x|0|1|
|y|1|0|

我们可以看作是用一条直线来拟合这两个样本，这样可以利用我们在第4章学到的知识和代码。

## 训练数据

```Python
# x=0,y=1; x=1,y=0
def ReadData():
    X = np.array([0,1]).reshape(1,2)
    Y = np.array([1,0]).reshape(1,2)
    return X,Y
```

## 正向

```Python
def ForwardCalculation(w,b,x):
    z = w * x + b
    return z
```

## 反向

```Python
def BackPropagation(x,y,z):
    dZ = z - y
    dB = dZ
    dW = dZ * x
    return dW, dB

def UpdateWeights(w, b, dW, dB, eta):
    w = w - eta*dW
    b = b - eta*dB
    return w,b
```

## 损失函数

由于是拟合直线，所以用均方差函数。

```Python
def CheckLoss(w, b, X, Y, count):
    Z = w * X + b
    LOSS = (Z - Y)**2
    loss = LOSS.sum()/count/2
    return loss
```

## 训练
```Python
if __name__ == '__main__':
    # initialize_data
    eta = 0.1
    # set w,b=0, you can set to others values to have a try
    #w, b = np.random.random(),np.random.random()
    w, b = 0, 0
    eps = 1e-10
    max_epoch = 1000
    # calculate loss to decide the stop condition
    loss = 1
    # create mock up data
    X, Y = ReadData()
    # count of samples
    num_features = X.shape[0]
    num_example = X.shape[1]

    for epoch in range(max_epoch):
        for i in range(num_example):
            # get x and y value for one sample
            x = X[:,i]
            y = Y[:,i]
            # get z from x,y
            z = ForwardCalculation(w, b, x)
            # calculate gradient of w and b
            dW, dB = BackPropagation(x, y, z)
            # update w,b
            w, b = UpdateWeights(w, b, dW, dB, eta)
            # calculate loss for this batch
            loss = CheckLoss(w,b,X,Y,num_example)
            print(epoch,i,loss,w,b)

        if loss < eps:
            break

    print("w=%f,b=%f" %(w,b))
```

## 运行结果

```
......
260 1 1.0097992508764906e-10 [-0.99997304] [0.99998202]
261 0 9.451720988184793e-11 [-0.99997304] [0.99998382]
261 1 9.306309896066242e-11 [-0.99997411] [0.99998274]
w=-0.999974,b=0.999983
[0.99998274] [8.62846911e-06]
```
由于设置了eps=1e-10，即损失函数值小于eps时，才算到达精度，因此迭代了261次。最后的w=-0.999，b=0.999，基本上与w=-1，b=1的数学解析解近似。

## 关于精度的说明

神经网络没有数学解析解，只有近似解，所以我们用以下测试函数来验证：

```Python
def Test(w,b):
    z1 = ForwardCalculation(w, b, 0)  # x=0, check if z==1
    z2 = ForwardCalculation(w, b, 1)  # x=1, check if z==0
    print (z1,z2)
    if np.abs(z1-1) < 0.001 and np.abs(z2-0)<0.001:
        return True
    return False
```

只要误差小于0.001，就算做达到精度了。下图中两个点(0,1)，(1,0)都被我们训练出来的直线拟合住了：

<img src="./Images/4/LogicNotGateResult.png"/>

代码位置：ch04, Level5