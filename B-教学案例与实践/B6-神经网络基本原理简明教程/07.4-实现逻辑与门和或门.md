Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 实现逻辑与门和或门

我们第4章用拟合的方式实现了逻辑非门。在学习了第6章的分类后，我们可以用分类的思想来实现下列4个逻辑门：

- 与门
- 与非门
- 或门
- 或非门

## 样本

逻辑与门的样本：

|样本|1|2|3|4|
|---|---|---|---|---|
|x1|0|0|1|1|
|x2|0|1|0|1|
|y|0|0|0|1|

逻辑与非门的样本：

|样本|1|2|3|4|
|---|---|---|---|---|
|x1|0|0|1|1|
|x2|0|1|0|1|
|y|1|1|1|0|

逻辑或门的样本：

|样本|1|2|3|4|
|---|---|---|---|---|
|x1|0|0|1|1|
|x2|0|1|0|1|
|y|0|1|1|1|

逻辑或非门的样本：

|样本|1|2|3|4|
|---|---|---|---|---|
|x1|0|0|1|1|
|x2|0|1|0|1|
|y|1|0|0|0|


## 解决方案

显示在下图中，同样位置的4个点，用红色表示y=1的情况，用蓝色表示y=0的情况：

|||
|---|---|
|<img src=".\Images\7\LogicAndGateResult.png">|<img src=".\Images\7\LogicNandGateResult.png">|
|上图：与门。下图：或门|上图：与非门。下图：或非门|
|<img src=".\Images\7\LogicOrGateResult.png">|<img src=".\Images\7\LogicNorGateResult.png">|
|||

把它门看成一个分类问题的话，实际上就是要在图中画出那条蓝色的线，把红蓝两个颜色的点分开。精度越高，则该直线的起点和终点越接近四边的中点。

## 代码

```Python
import numpy as np
import matplotlib.pyplot as plt

# 分类问题，用Sigmoid做分类函数
def Sigmoid(x):
    s=1/(1+np.exp(-x))
    return s

# 前向计算
def ForwardCalculationBatch(W, B, batch_X):
    Z = np.dot(W, batch_X) + B
    A = Sigmoid(Z)
    return A

# 反向计算
def BackPropagationBatch(batch_X, batch_Y, A):
    m = batch_X.shape[1]
    dZ = A - batch_Y
    # dZ列相加，即一行内的所有元素相加
    dB = dZ.sum(axis=1, keepdims=True)/m
    dW = np.dot(dZ, batch_X.T)/m
    return dW, dB

# 更新权重参数
def UpdateWeights(W, B, dW, dB, eta):
    W = W - eta * dW
    B = B - eta * dB
    return W, B

# 计算损失函数值，二分类交叉熵损失函数
def CheckLoss(W, B, X, Y):
    m = X.shape[1]
    A = ForwardCalculationBatch(W,B,X)
    
    p4 = np.multiply(1-Y ,np.log(1-A))
    p5 = np.multiply(Y, np.log(A))

    LOSS = np.sum(-(p4 + p5))  #binary classification
    loss = LOSS / m
    return loss

# 初始化权重值
def InitialWeights(num_input, num_output, method):
    if method == "zero":
        # zero
        W = np.zeros((num_output, num_input))
    elif method == "norm":
        # normalize
        W = np.random.normal(size=(num_output, num_input))
    elif method == "xavier":
        # xavier
        W=np.random.uniform(
            -np.sqrt(6/(num_input+num_output)),
            np.sqrt(6/(num_input+num_output)),
            size=(num_output,num_input))

    B = np.zeros((num_output, 1))
    return W,B

# 训练
def train(X, Y, ForwardCalculationBatch, CheckLoss):
    num_example = X.shape[1]
    num_feature = X.shape[0]
    num_category = Y.shape[0]
    # 学习率设置可以稍微大一些
    eta = 0.5
    # 最多循环次数要多一些，以保证精度
    max_epoch = 10000
    # 初始化权重值为0
    W, B = InitialWeights(num_feature, num_category, "zero")
    # calculate loss to decide the stop condition
    loss = 5        # initialize loss (larger than 0)
    error = 2e-3    # stop condition

    # 外循环
    for epoch in range(max_epoch):
        # 内循环
        for i in range(num_example):
            # get x and y value for one sample
            x = X[:,i].reshape(num_feature,1)
            y = Y[:,i].reshape(1,1)
            # get a from x,y
            batch_a = ForwardCalculationBatch(W, B, x)
            # calculate gradient of w and b
            dW, dB = BackPropagationBatch(x, y, batch_a)
            # update w,b
            W, B = UpdateWeights(W, B, dW, dB, eta)
        # end for
        # calculate loss for this batch
        loss = CheckLoss(W,B,X,Y)
        print(epoch,i,loss,W,B)
        # end if
        if loss < error:
            break
    # end for
    return W,B

# 显示结果
def ShowResult(W,B,X,Y,title):
    # 根据w,b值画出分割线
    w = -W[0,0]/W[0,1]
    b = -B[0,0]/W[0,1]
    x = np.array([0,1])
    y = w * x + b
    plt.plot(x,y)

    # 画出原始样本值
    for i in range(X.shape[1]):
        if Y[0,i] == 0:
            plt.scatter(X[0,i],X[1,i],marker="o",c='b',s=64)
        else:
            plt.scatter(X[0,i],X[1,i],marker="^",c='r',s=64)
    plt.axis([-0.1,1.1,-0.1,1.1])
    plt.title(title)
    plt.show()
```

## 逻辑或的运行结果
```
2265 0 0.0020013436850553168 [[11.74306772 11.74482656]] [[-5.41357292]]
epoch=2266
2266 0 0.002000455990632811 [[11.74395682 11.74571489]] [[-5.41401739]]
epoch=2267
2267 0 0.0019995690824527465 [[11.74484552 11.74660282]] [[-5.41446167]]
w= [[11.74573383 11.74749036]]
b= [[-5.41268583]]
input number one:1
input number two:1
[[0.99999999]]
True
input number one:1
input number two:0
[[0.99822654]]
True
input number one:0
input number two:0
[[0.00443985]]
True
input number one:0
input number two:1
[[0.99822965]]
True
```
迭代了2267次，达到精度loss < 0.002。当输入(1,1)(1,0)(0,0)(0,1)四种组合时，输出全都满足精度要求。

## 逻辑与的运行结果

```
4249 0 0.0020009736030668296 [[11.76552867 11.76405673]] [[-17.81318551]]
epoch=4250
4250 0 0.0020005031484058656 [[11.76599923 11.76452763]] [[-17.81389213]]
epoch=4251
4251 0 0.0020000329148008636 [[11.76646968 11.76499843]] [[-17.8145986]]
w= [[11.76694002 11.76546912]]
b= [[-17.81530488]]
input number one:1
input number two:1
[[0.99672156]]
True
input number one:1
input number two:0
[[0.00235616]]
True
input number one:0
input number two:1
[[0.0023527]]
True
input number one:0
input number two:0
[[1.8319406e-08]]
True
```
迭代了4251次，达到精度loss < 0.002。当输入(1,1)(1,0)(0,1)(0,0)四种组合时，输出全都满足精度要求。



||W1|W2|B|
|---|---|---|---|
|AND|11.76694002| 11.76546912|-17.81530488|
|NAND|-11.76694002|-11.76546912|17.81530488|
|OR|11.74573383|11.74749036|-5.41268583|
|NOR|-11.74573383|-11.74749036|5.41268583|



代码位置：ch07, Level4
(其中，Level4_LogicGateBase.py是共享的基础代码)
