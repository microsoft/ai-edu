Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 双层神经网络的结构

## 前向计算

在本章中，为了完成复杂函数拟合任务，我们将使用如下正向过程：

$$Z1=W1 \cdot X+B1$$

$$A1=Sigmoid(Z1)$$

$$Z2=W2 \cdot A1+B2$$

$$A2=Z2 \tag{没有激活函数}$$

以及均方差损失函数：

$$J(w,b) = \frac{1}{2m}\sum^m_{i=1}(z_i-y_i)^2$$

前向计算图：

<img src=".\Images\8\forward.png">

木头：这里有三个疑问：

1. 为什么要用两层神经网络？
2. 为什么在神经网络的第二层，没有用到激活函数？
3. 为什么我们用均方差损失函数，而不是交叉熵损失函数？

铁柱：这先要理解神经网络的原理，我们一个个地回答你的问题。

## 为什么要用两层神经网络？

首先，一层神经网络肯定不能完成这个复杂函数的拟合过程。因为一层神经网络，只能完成线性任务。这里的“线性任务”的定义，从简单到复杂，列表如下：

|名称|形式|能力
|---|---|---|
|单变量线性回归|$y=w_0+w_1·x$|拟合二维平面直线|
|多变量线性回归|$y=w_0+w_1·x_1+w_2·x_2...$|拟多高维空间直线或平面|
|高阶线性回归|$y=w_0+w_1·x_1+w_2·x^2_1...$|拟合二维平面高阶曲线|
|多变量高阶线性回归|$y=w_0+w_1·x_1+w_2·x_2+w_3·x^2_1+w_4·x^2_2...$|拟合多维空间高阶曲线或曲面|

所谓的“高阶”，指的是特征变量其实只有一个x1，但是把x1的平方也算作第二个特征向量。比如一栋房子的长度x1，宽度x2，占地面积x3=x1*x2。这里的x3并不是独立存在的，真正的自变量只有x1和x2。

这些高次线性回归问题，可以用单层的神经网络来解决，但是是有前提条件的，即假设函数必须和实际问题吻合。满足这个条件的实际工程问题并不多见，并且这种情况完全可以用两层的神经网络来解决，所以我们没有在单层的神经网络中涉及这个问题。

## 为什么在输出层没有用到激活函数？

神经网络不管有多少层，最后的输出层决定了这个神经网络能干什么。在单层神经网络中，我们学习到了以下示例：

|网络|输入|输出|激活函数|功能|
|---|---|---|---|---|
|单层|单变量|单输出|无|二维线性回归/拟合|
|单层|多变量|单输出|无|多维线性回归/拟合|
|单层|多变量|单输出|二分类函数|二分类|
|单层|多变量|多输出|多分类函数|多分类|

对于多层神经网络也是如此，我们要完成拟合任务，而不是分类，所以用不到激活/分类函数。通常把激活函数和分类函数混淆在一起说，如果明确地区分二者，则可以这样说：神经网络的最后一层不用激活函数，只可能用到分类函数。Sigmoid既是激活函数，又是分类函数，是个特例。

神经网络的拟合原理是这样的：在第一层神经网络，通过W1*X的计算做线性变化，把非线性问题转换成线性问题；在第二层神经网络做线性回归。所以在第二层是不需要激活函数的，否则就没法画出一条直线来。这个可以想象两个独立的神经网络，第一个网络已经把数据处理成线性的了，以便让我们使用第4章的方法，做一次线性回归就好了。

## 为什么用均方差而不是交叉熵损失函数？

我们把上面的表格拿来再扩充一下：

|网络|输入|输出|激活函数|损失函数|功能|
|---|---|---|---|---|---|
|单层|单变量|单输出|无|均方差|二维线性回归/拟合|
|单层|多变量|单输出|无|均方差|多维线性回归/拟合|
|单层|多变量|单输出|二分类函数|交叉熵|二分类|
|单层|多变量|多输出|多分类函数|交叉熵|多分类|

交叉熵函数是用于分类的，均方差函数是用于拟合的，可以理解为计算拟合的点和样本标签点的距离之平方和。

## 反向传播

木头：由于是第一次接触两层的神经网络，我看到反向传播的函数体中，比起一层的网络复杂了很多。您能详细解释一下吗？

铁柱：看一下计算图，然后用链式求导法则反推。

<img src=".\Images\8\backward.png">

蓝色的箭头线表示正向计算过程，黄色的箭头线表示反向的传播过程。

### 梯度生成

对损失函数求导，可以得到损失函数对输出层的梯度值，即上图中的Z2部分。

因为：

$$\because J(w,b) = \frac{1}{2m} \sum (z_i-y_i)^2$$

$$\therefore {\partial{J} \over \partial{z_i}}=\frac{1}{2m} \sum {\partial{(z_i-y_i)^2} \over \partial{z_i}}=\frac{1}{m} \sum (z_i-y_i)$$

用于矩阵运算，可以简写为：

$$dZ2 = \frac{\partial{J}}{\partial{Z2}} = Z2-Y \tag{1}$$

### 求W2的梯度

$$\because Z2 = W2 \cdot A1+B2$$

$$\therefore {\partial{Z2} \over \partial{W2}}=\frac{\partial{(W2 \cdot A1+B2)}}{\partial{W2}}$$
$$=\frac{\partial{(W2 \cdot A1)}}{\partial{W2}}+\frac{\partial{(B2)}}{\partial{W2}}$$
$$=A1^T+0=A1^T$$

结合损失函数对Z2的偏导结果，使用链式法则：

$$
dW2 = \frac{\partial{J}}{\partial{W2}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{W2}}
$$
$$=(Z2-Y)A1^T \tag{2}$$

### 求B2的梯度

$$dB2 = \frac{\partial{J}}{\partial{B2}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{B2}}$$

$$=(Z2-Y) \cdot 1=Z2-Y \tag{3}$$

### 求损失函数对隐层的梯度

对于深度神经网络，需要把梯度从最后一层逐层向前传递，经过激活函数的导数，直接达到线性计算部分，即下图中的Z1部分：

<img src=".\Images\8\backward.png">

链式求导公式如下：

$$
\frac{\partial{J}}{\partial{Z1}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{A1}} \cdot \frac{\partial{A1}}{\partial{Z1}}
$$

我们已经有了第一项的结果，现在来解决后面两项：

$$\frac{\partial{Z2}}{\partial{A1}} = \frac{\partial{(W2 \cdot A1 + B2)}}{\partial{A1}}=W2^T$$

$$\frac{\partial{A1}}{\partial{Z1}}=\frac{\partial{(Sigmoid(Z1))}}{\partial{Z1}}=A1 \cdot (1-A1)$$

所以：

$$dZ1=\frac{\partial{J}}{\partial{Z1}} = \frac{\partial{J}}{\partial{Z2}} \cdot \frac{\partial{Z2}}{\partial{A1}} \cdot \frac{\partial{A1}}{\partial{Z1}}$$
$$=W2^T \cdot (Z2-Y) \cdot A1 \cdot (1-A1)$$
$$=W2^T \cdot dZ2 \cdot A1 \cdot (1-A1) \tag{4}$$

### 求W1的梯度

$$\frac{\partial{Z1}}{\partial{W1}} = \frac{\partial{(W1 \cdot X+B1)}}{\partial{W1}} = X^T$$

$$
dW1=\frac{\partial{J}}{\partial{W1}} = \frac{\partial{J}}{\partial{Z1}} \frac{\partial{Z1}}{\partial{W1}}= dZ1 \cdot X^T \tag{5}
$$

### 求B1的梯度

$$\frac{\partial{Z1}}{\partial{B1}} = \frac{\partial{(W1 \cdot X+B1)}}{\partial{B1}} = 1$$

$$
dB1=\frac{\partial{J}}{\partial{B1}} = \frac{\partial{J}}{\partial{Z1}} \frac{\partial{Z1}}{\partial{B1}}= dZ1 \tag{6}
$$

变成代码：
```Python
def BackPropagationBatch(batch_x, batch_y, dict_cache, dict_weights):
    # 批量下降，需要除以样本数量，否则会造成梯度爆炸
    m = batch_x.shape[1]
    # 取出缓存值
    A1 = dict_cache["A1"]
    A2 = dict_cache["A2"]
    W2 = dict_weights["W2"]
    # 第二层的梯度
    dZ2 = A2 - batch_y  # 公式1
    # 第二层的权重和偏移
    dW2 = np.dot(dZ2, A1.T)/m # 公式2
    dB2 = np.sum(dZ2, axis=1, keepdims=True)/m  # 公式3
    # 第一层的梯度
    dA1 = np.dot(W2.T, dZ2)
    dA1_Z1 = np.multiply(A1, 1 - A1)
    dZ1 = np.multiply(dA1, dA1_Z1)  # 公式4
    # 第一层的权重和偏移
    dW1 = np.dot(dZ1, batch_x.T)/m  # 公式5
    dB1 = np.sum(dZ1, axis=1, keepdims=True)/m  # 公式6
    # 保存到词典中返回
    dict_grads = {"dW1":dW1, "dB1":dB1, "dW2":dW2, "dB2":dB2}
    return dict_grads
```
在计算dB2和dB1的时候，我们注意到参数中含有"axis=1, keepdims=True"，这是为了当用批量数据训练时，需要把一批的梯度相加后，再求一个平均值反向传播。

代码位置：ch08, Level1
