Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 参数调整

我们使用如下参数做第一次的训练：

|参数|缺省值|是否可调|
|---|---|---|
|输入层神经元数|1|No|
|隐层神经元数|4|Yes|
|输出层神经元数|1|No|
|学习率|0.1|Yes|
|批样本量|10|Yes|
|最大epoch|50000|Yes|
|损失门限值|0.001|No|
|损失函数|MSE|No|
|参数初始化方法|xavier|Yes|

木头：买嘎哒！怎么这么多参数！

铁柱：如果使用者不了解神经网络中的基本原理，那么所谓“调参”就是摸着石头过河了。今天咱们可以试着改变几个参数，来看看训练结果。

## 权重初始化的方法的调整

我们提供有三种方法初始化权重矩阵：

1. 零值初始化
2. [0,1]正态分布随机值初始化（均值为0，方差为1）
3. [0,1]Xavier均匀分布随机值初始化

|初始化方法|损失函数值|训练结果|
|---|---|---|
|零值初始化|<img src=".\Images\8\zero_loss.png">|<img src=".\Images\8\zero_result.png">|
|正态分布初始化|<img src=".\Images\8\norm_loss.png">|<img src=".\Images\8\norm_result.png">|
|Xavier初始化|<img src=".\Images\8\xavier_loss.png">|<img src=".\Images\8\xavier_result.png">|

木头：为什么零值初始化不能得到正确的结果呢？

铁柱：我们只训练了50000个epoch，如果训练更多的轮数，也不会得到正确的结果。看下面的零值初始化的权重矩阵值打印输出：
```
[[-10.29778156]
 [-10.29778156]
 [-10.29778156]
 [-10.29778156]]
[[9.92847462]
 [9.92847462]
 [9.92847462]
 [9.92847462]]
[[-0.29563053 -0.29563053 -0.29563053 -0.29563053]]
[[0.93438753]]
```
可以看到W1和W2的值内部4个单元都一样，这是因为初始值都是0，所以梯度均匀回传，导致所有w的值都同步更新，没有差别。这样的话，无论多少论，最终的结果也不会准确。

正态分布初始化的结果：

```
[[ -1.0711346 ]
 [-19.75850005]
 [ -8.84552296]
 [  5.08410339]]
[[ 1.81361968]
 [16.69671287]
 [ 4.25394174]
 [-3.49831165]]
[[ 2.47519669 -2.01738001 -2.73550422 -4.55522827]]
[[2.50646588]]
```

Xavier初始化的结果：

```
[[ -5.86643409]
 [-12.80232309]
 [ -0.94429957]
 [ 10.36952416]]
[[ 4.13811338]
 [10.71442528]
 [-0.27816434]
 [-4.92664038]]
[[ 5.65247181 -3.34531482  0.9183063   2.53985718]]
[[-2.84968849]]
```

后两者最后都得到了准确的结果，但是Xavier的epoch次数是13166，比第二种方法的29096少了一倍多。所以我们推荐使用Xavier初始化方法。

在后面的几组比较中，都是用Xavier方法初始化的。另外，两次使用Xavier初始化，也会得到不同的结果，为了避免这个随机性，我们在代码Level1_TwoLayer.py中，使用了一个小技巧，调用下面这个函数：

```Python
    def GenerateWeightsArrayFileName(self):
        self.w1_name = str.format("w1_{0}_{1}_{2}.npy", self.num_hidden, self.num_input, self.init_method.name)
        self.w2_name = str.format("w2_{0}_{1}_{2}.npy", self.num_output, self.num_hidden, self.init_method.name)

    # load exist initial weights which has same parameters
    # if not found, create new, then save
    def LoadSameInitialParameters(self):
        self.GenerateWeightsArrayFileName()
        w1_file = Path(self.w1_name)
        w2_file = Path(self.w2_name)
        if w1_file.exists() and w2_file.exists():
            W1 = np.load(w1_file)
            W2 = np.load(w2_file)
            B1 = np.zeros((self.num_hidden, 1))
            B2 = np.zeros((self.num_output, 1))
        else:
            W1, B1 = CParameters.InitialParameters(self.num_input, self.num_hidden, self.init_method)
            W2, B2 = CParameters.InitialParameters(self.num_hidden, self.num_output, self.init_method)
            np.save(w1_file, W1)
            np.save(w2_file, W2)
        # end if
        return W1, B1, W2, B2
```

第一次调用时，会得到一个随机初始化矩阵，以后再次调用时，只要隐层神经元数量不变并且初始化方法不变，就会用第一次的初始化结果，否则后面的各种参数调整的结果就没有可比性了。

## 学习率的调整

|学习率的影响||
|---|---|
|<img src=".\Images\8\eta01_loss.png">|<img src=".\Images\8\eta05_loss.png">|
|eta=0.1, epoch=12166|eta=0.5, epoch=5940|
|<img src=".\Images\8\eta07_loss.png">|<img src=".\Images\8\eta09_loss.png">|
|eta=0.7, epoch=2204|eta=0.9, epoch=2599|

对于这个特定问题，较大的学习率可以带来很快的收敛速度，但并不是对所有问题都这样。


## 批大小的调整

|批大小的影响||
|------|---|
|<img src=".\Images\8\bz1_loss.png">|<img src=".\Images\8\bz5_loss.png">|
|batch_size=1, epoch=1973|batch_size=5, epoch=3278|
|<img src=".\Images\8\bz10_loss.png">|<img src=".\Images\8\bz20_loss.png">|
|batch_size=10, epoch=5940|batch_size=20, epoch=11894|

批样本量越小，epoch次数越少，收敛越快。但是收敛速度快，不代表运行速度快，在笔者的机器上，图一比图二需要更长时间运行完毕，因为图二是5个样本一起计算的，底层库做了优化。

## 隐层神经元数量的调整

|隐层神经元数量的影响||
|---|---|
|<img src=".\Images\8\ne4_loss.png">|<img src=".\Images\8\ne6_loss.png">|
|neuron number = 4, epoch = 5940|neuron number = 6, epoch = 2007|
|<img src=".\Images\8\ne8_loss.png">|<img src=".\Images\8\ne10_loss.png">|
|neuron number = 8, epoch = 3271|neuron number = 10, epoch = 3575|

对于这个特定问题，隐层神经元个数为6时，收敛速度最快。

代码位置：ch08, Level3
