Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 知识点

- 万能近似定理及其验证方法 
  - 复杂函数拟合的基本方法
- [双层神经网络的原理](08.1-双层神经网络的原理.md) 
  - 神经网络结构设计与反向传播
- [双层神经网络的实现](08.2-双层神经网络的实现.md)
  - 用Python代码实现

# 提出问题

验证前馈神经网络的万能近似定理。

## 问题：给出如下一批训练数据，如何使用神经网络方法来拟合这条曲线？

|样本|1|2|3|...|1000|
|---|---|---|---|---|---|
|特征X|0.606|0.129|0.643|...|0.199|
|标签Y|-0.113|-0.269|-0.217|...|-0.281|

我们可视化一下训练数据：

<img src=".\Images\9\Sample.png">


$$y=0.4x^2 + 0.3xsin(15x) + 0.01cos(50x)-0.3$$

我们特意把数据限制在[0,1]之间，避免做归一化的麻烦。要是觉得这个公式还不够复杂，大家可以用更复杂的公式去自己试验。

## 训练数据

下载后拷贝到您要运行的Python文件所在的文件夹。

[点击下载训练样本数据X](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/CurveX.dat)

[点击下载训练样本数据Y](https://github.com/Microsoft/ai-edu/tree/master/B-%E6%95%99%E5%AD%A6%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5/B6-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/Data/CurveY.dat)


## 理论解释

这里有一篇论文，Kurt Hornik在1991年发表的，说明了含有一个隐层的神经网络能拟合任意复杂函数：

https://www.sciencedirect.com/science/article/pii/089360809190009T

Abstract - We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to L(u) performance criteria, for arbitrary finite input envrionment measres u, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.

简言之：两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它能以任意精度来近似拟合任意连续函数。原文提到Borel可测函数，超出了本课程的范围。

## 直观解释

还有一种直观的解释是这样的（个人认为这不是神经网络的工作原理），用无数个矩形的拼接来近似一条曲线：

|粗粒度|细粒度|
|---|---|
|<img src=".\Images\9\histogram1.jpg">|<img src=".\Images\9\histogram2.jpg">|

它的理论基础是：

<img src=".\Images\9\histogram3.jpg">

假设在隐层有两个神经元，都配置有Sigmoid激活函数。第一个神经元在-0.01处产生一个阶跃，第二个神经元在+0.01处产生一个阶跃，都是用Sigmoid函数完成的，然后用第一个神经元的输出减去第二个神经元的输出（设置权重值为[1,-1]）,就会形成一个“门”。无数个这样的“门”就能模拟出一条曲线。

对于三维空间，它是这个样子的：

|阶跃面|封闭塔|
|---|---|
|<img src=".\Images\9\histogram4.jpg">|<img src=".\Images\9\histogram5.jpg">|

在三维空间中，两个有不同偏置值的sigmoids激活函数相减，我们将得到左侧的等效曲线。如果我们采用另一个水平垂直的塔架到现在组合的曲线上。在叠加这两个水平垂直的开放式塔时，我们就可以得到封闭的塔。然后就可以用封闭塔模拟任何三维曲面。

[原文在这里](https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586)。
