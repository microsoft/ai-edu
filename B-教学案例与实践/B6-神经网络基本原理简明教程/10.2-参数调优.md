Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 参数调整

## 可调与不可调参数

我们使用如下参数做第一次的训练：

|参数|缺省值|是否可调|注释|
|---|---|---|---|
|输入层神经元数|1|No|
|隐层神经元数|4|Yes|影响迭代次数|
|输出层神经元数|1|No|
|学习率|0.1|Yes|影响迭代次数|
|批样本量|10|Yes|影响迭代次数|
|最大epoch|30000|Yes|影响终止条件,建议不改动|
|损失门限值|0.001|Yes|影响终止条件,建议不改动|
|损失函数|MSE|No|
|参数初始化方法|Xavier|Yes|参看10.1节

木头：买嘎哒！怎么这么多参数！

铁柱：如果使用者不了解神经网络中的基本原理，那么所谓“调参”就是摸着石头过河了。今天咱们可以试着改变几个参数，来看看训练结果。

## 前提

在后面的几组比较中，都是用Xavier方法初始化的。另外，两次使用Xavier初始化，也会得到不同的结果，为了避免这个随机性，我们在代码Level0_TwoLayerNet.py中，使用了一个小技巧，调用下面这个函数：

```Python
    def InitializeWeights(self, create_new = False):
        self.__GenerateWeightsArrayFileName()
        if create_new:
            self.__CreateNew()
        else:
            self.__LoadExistingParameters()
        # end if
        self.dW = np.zeros(self.W.shape)
        self.dB = np.zeros(self.B.shape)

```

第一次调用时，会得到一个随机初始化矩阵。以后再次调用时，如果参数值为False，只要隐层神经元数量不变并且初始化方法不变，就会用第一次的初始化结果，否则后面的各种参数调整的结果就没有可比性了。

## 学习率的调整

|学习率的影响||
|---|---|
|<img src=".\Images\10\eta_01_loss.png">|<img src=".\Images\10\eta_03_loss.png">|
|eta=0.1, epoch=11089|eta=0.3, epoch=3606|
|<img src=".\Images\10\eta_05_loss.png">|<img src=".\Images\10\eta_07_loss.png">|
|eta=0.5, epoch=2835|eta=0.7, epoch=3`74|

对于这个特定问题，较大的学习率可以带来很快的收敛速度，但并不是对所有问题都这样。


## 批大小的调整

|批大小的影响||
|------|---|
|<img src=".\Images\10\bz_1_loss.png">|<img src=".\Images\10\bz_5_loss.png">|
|batch_size=1, epoch=2100|batch_size=5, epoch=2705|
|<img src=".\Images\10\bz_10_loss.png">|<img src=".\Images\10\bz_20_loss.png">|
|batch_size=10, epoch=2835|batch_size=20, epoch=4472|

批样本量越小，epoch次数越少，收敛越快。但是收敛速度快，不代表运行速度快，在笔者的机器上，图一比图二需要更长时间运行完毕，因为图二是5个样本一起计算的，底层数学库做了优化。

但是！但是！这个结论的前提是我们用了0.5的学习率，如果用0.1的话，将会得到不同结论。

## 隐层神经元数量的调整

|隐层神经元数量的影响||
|---|---|
|<img src=".\Images\10\ne_2_loss.png">|<img src=".\Images\10\ne_2_result.png">|
|2个神经元|2个神经元拟合效果很差|
|<img src=".\Images\10\ne_4_loss.png">|<img src=".\Images\10\ne_8_loss.png">|
|4个神经元 epoch=2835|8个神经元 epoch=4601|
|<img src=".\Images\10\ne_16_loss.png">|<img src=".\Images\10\ne_16_result.png">|
|16个神经元|16个神经元 拟合效果比较奇怪|

对于这个特定问题，隐层神经元个数为6时，收敛速度最快。

代码位置：ch08, Level3


**课后作业**

使用下列参数设置，找到批大小和学习率的关系：

- 隐层神经元：4
- 初始化：Xavier
- 批大小选择：1，5，10，15，20，25，30
- 学习率选择：0.1，0.3，0.5，0.7

得到下表：

|批大小|1|5|10|15|20|
|---|---|---|---|---|---|
|0.1|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.3|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.5|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|
|0.7|epoch=?|epoch=?|epoch=?|epoch=?|epoch=?|

从而得到批大小与学习率的最佳组合。
