Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 搜索最优学习率

## 挑战

前面章节学习过，普通梯度下降法，包含三种形式：

1. 单样本
2. 全批量样本
3. 小批量样本

我们通常把1和3统称为SGD(Stochastic Gradient Descent)。当批量不是很大时，全批量也可以纳入此范围。大的含义是：十万级以上的数据量。

使用梯度下降的这些形式时，我们通常面临以下挑战：

1. 很难选择出合适的学习率。太小的学习率会导致网络收敛过于缓慢，而学习率太大可能会影响收敛，并导致损失函数在最小值上波动，甚至出现梯度发散。
2. 此外，相同的学习率并不适用于所有的参数更新。如果训练集数据很稀疏，且特征频率非常不同，则不应该将其全部更新到相同的程度，但是对于很少出现的特征，应使用更大的更新率。
3. 在神经网络中，最小化非凸误差函数的另一个关键挑战是避免陷于多个其他局部最小值中。实际上，问题并非源于局部极小值，而是来自鞍点，即一个维度向上倾斜且另一维度向下倾斜的点。这些鞍点通常被相同误差值的平面所包围，这使得SGD算法很难脱离出来，因为梯度在所有维度上接近于零。

<img src=".\Images\10\saddle_point.png" width="400">

比如我们在本章前面的实例结果，Loss值随迭代次数而下降的历史记录图是这样的：

<img src=".\Images\10\eta01_loss.png" width="600">

为什么在2000至9000个epoch之间，有很大一段平坦地段，Loss值并没有显著下降？这其实也体现了这个问题的实际损失函数的形状，在这一区域上梯度比较平缓，以至于梯度下降算法并不能找到合适的突破方向寻找最优解，而是在原地转腰子（徘徊）。这一平缓地区就是损失函数的驻点或者鞍点。

## 学习率的选择

我们前面一直使用固定的学习率，比如0.1或者0.05。这是因为在接近极小点时，损失函数的梯度也会变小，因此不会担心越过极小点。保证SGD收敛的充分条件是：

$$\sum_{k=1}^\infty \eta_k = \infty，且： \sum_{k=1}^\infty \eta^2_k < \infty$$ 

   
实践中，有必要随之迭代次数的增加而逐渐降低学习率。

下图是不同的学习率的选择对损失函数与迭代次数的比值：

<img src=".\Images\10\learning_rate.jpg">

- 黄色：学习率太大，loss值增高，网络发散
- 绿色：学习率可以使网络收敛，但值较大，loss值徘徊不降
- 蓝色：学习率值太小，loss值下降速度慢，训练次数长，收敛慢
- 红色：正确的学习率设置

## 如何找到最佳学习率

木头：在第三节里，我们已经知道了学习率对的影响，结论是学习率在0.7左右效果最好，只是基于遍历的方法搜索到的，很费时费力。有什么通用的办法可以帮助我们吗？

铁柱：好问题！学习率是“灰常灰常”重要的一个参数，所以很多人研究了它。Leslie N. Smith 在2015年的一篇论文[Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)中的3.3节描述了一个非常棒的方法来找初始学习率，同时推荐大家去看看这篇论文，有一些非常启发性的学习率设置想法。

这个方法在论文中是用来估计网络允许的最小学习率和最大学习率，我们也可以用来找我们的最优初始学习率，方法非常简单。首先我们设置一个非常小的初始学习率，比如1e-5，然后在每个batch之后都更新网络，同时增加学习率，统计每个batch计算出的loss。最后我们可以描绘出学习的变化曲线和loss的变化曲线，从中就能够发现最好的学习率。

下面就是随着迭代次数的增加，学习率不断增加的曲线，以及不同的学习率对应的loss的曲线。

|随着迭代次数增加学习率|观察Loss值与学习率的关系|
|---|---|
|<img src=".\Images\10\lr-select-1.jpg">|<img src=".\Images\10\lr-select-2.jpg">|

从有图可以看到，学习率在0.3左右表现最好，再大就有可能发散了。我们把这个方法用于到我们的代码中试一下是否有效。

首先，设计一个数据结构，做出如下这张表：

|学习率段|0.0001~0.0009|0.001~0.009|0.01~0.09|0.1~0.9|1.0~1.1|
|----|----|----|----|---|---|
|步长|0.0001|0.001|0.01|0.1|0.01|
|迭代|10|10|10|10|10|

对于每个学习率段，在每个点上迭代10次，然后：

$$当前学习率+步长=>下一个学习率$$

以第一段为例，会在0.0001迭代10次，在0.0002上迭代10次，......，在0.0009上迭代10次，然后进入第二段。步长和迭代次数可以分段设置。代码如下：

```Python
    lr_Searcher = CLearningRateSearcher()
    looper = CLooper(0.0001,0.0001,10)
    lr_Searcher.addLooper(looper)
    looper = CLooper(0.001,0.001,10)
    lr_Searcher.addLooper(looper)
    looper = CLooper(0.01,0.01,10)
    lr_Searcher.addLooper(looper)
    looper = CLooper(0.1,0.1,10)
    lr_Searcher.addLooper(looper)
```

调用时，建立class CBestLRSearcher(CTwoLayerNet)继承自CTwoLayerNet，只overwrite其中的train方法，得到的曲线如下：

<img src=".\Images\10\LR_try_1.png">

横坐标用了np.log10()函数来显示对数值。

好像并不理想，前面一大段都是在下降，说明0.0001(对应坐标值-4)，0.001(对应坐标值-3)，0.01(对应坐标值-2)，都太小了。那我们就继续探查0.1以后的段：

```Python
    looper = CLooper(0.1,0.1,100)
    lr_Searcher.addLooper(looper)
    looper = CLooper(1.0,0.01,100,1.1)
    lr_Searcher.addLooper(looper)
```

因为要探查细节，我们把第三个参数（迭代次数）设置为100，能看到更多变化：

<img src=".\Images\10\LR_try_2.png">

坐标值-0.2对应学习率0.6，$10^{-0.2}=0.63$，所以我们从0.63开始再进一步探查：

```Python
    looper = CLooper(0.63,0.01,100,1.1)
    lr_Searcher.addLooper(looper)
```

<img src=".\Images\10\LR_try_3.png">

到-0.02时（对应学习率0.95）开始，损失值上升，所以合理的学习率应该是0.7~0.9之间，于是我们可以用0.7，0.8，0.9去做试验了。

代码位置：ch10, Level4
