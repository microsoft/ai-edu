Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


## 批归一化 Batch Normalization

为什么要有Batch Normalization呢？在神经网络中，我们可以将每一层视为对输入的信号做了一次变换，

$$
Z = W \cdot X + B
$$

经过这样的变换之后，神经网络的每一层所接受到的输入的分布就可能会发生变化，不再是输入的原始数据所适应的分布了。这就会有一种什么问题呢？

   1. 随着网络的加深，在深层的层接收到的输入的分布和输入的分布可能差距很大，这会在一定程度上影响神经网络I.I.D.（独立同分布）的假设
   2. 随着网络的加深，深层的层接受到的数据分布会因为每次训练导致的参数变化而不断变化，那么这些层就既需要去提取特征，又需要去适应不断变化的分布，加大了训练和收敛的难度

基于这样的考虑，batch normalization致力于将每一层的输入数据正则化成$N(0,1)$的分布，为了不影响网络本身的表达能力，以及可能一些层对非正态分布的数据表现更为优异，给予网络一定的调整能力，在正则化后，额外使用两个参数$\gamma$，$\beta$对分布进行了一定的线性变化。

为什么采用batch的方式而不是对整个数据集进行正则化呢？这样的考虑更多的基于对计算力的考量，具体请参看[论文](https://arxiv.org/abs/1502.03167)

下面我们来阐述一下具体的batch normalization的处理方法。

### 前向过程

#### 符号表

|符号|数据类型|数据形状|
|:---------:|:-----------:|:---------:|
|$X$| 输入数据 | [batchSize, Channles] |
|$\hat{X}$| 经过归一化的数据 | [batchSize, Channles] |
|$\mu$| 数据均值 | [1, Channels] |
|$\sigma^2$| 数据方差 | [1, Channels] |
|$m$|样本数量| [1] |
|$x_{ij}$| 第i个输入的第j个channel的数据| [1] |
|$x_i$|第i个样本的channel数据| [1, Channels] |
|$\sigma^2_i$| 第i个channel的方差数据 | [1] |
|$\mu_i$| 第i个channel的均值数据 | [1] |
|$\gamma$|线性变换参数| [1, Channels] |
|$\beta$|线性变换参数| [1, Channels] |
|$C$| 反向传递的误差 | [batchSize, Channles] |
||||
P.S. 无特殊说明，以下乘法为元素乘，即element wise的乘法

在训练过程中，针对每一个batch数据，进行的操作是，将这组数据正则化，之后对其进行线性变换。

具体的算法步骤是：

<img src=".\Images\14\1.png">

在测试过程或推理时，我们只有一张或者几张图的数据，无法算出来正确的均值，因此，我们使用的均值和方差数据是在测试过程中样本值的平均。也就是：

<img src=".\Images\14\2.png">

如上所述，我们给出前向传播的代码

```python
class batchNorm():
    """Batch normalization layer implemented by numpy

    This is the basic class for batch normalization layer used in deep learning

    Attributes:
        input: the name of layer before this layer
        output: the name of layer after this layer
        mean_list: the list stores the mean values of variables trained using this layer
        var_list: the list stores the variance values of variables trained using this layer
        eps: a small value to avoid the variance is zero
        gamma: a learnable gamma to do linear transformation
        beta: a learnable beta to do linear transformation
        variable: store the input variable
        gradient_gamma: the gradient to update gamma
        gradient_beta: the gradient to update beta
    """

    def __init__(self, input, output, outputChannel):
        """Construct a BN layer"""
        self.input = input
        self.output = output
        self.mean_list = []
        self.var_list = []
        self.eps = 1e-5
        self.gamma = np.ones([1, outputChannel])
        self.beta = np.zeros([1, outputChannel])

    def forward(self, variable, train=True):
        """the function used in forward process

        This function describes the process of forward calculation

        Args:
            variable: the input variable to be processed, 
                      assume the shape of this variable is [batch, channels]
            train: a bool variable represents whether it is still under training 

        Return:
            the variable after processed, [batch, channels]

        """
        if train:
            mean = np.mean(variable, axis=0)
            var = np.var(variable, axis=0)
            self.variable = variable
            variable = (variable - mean) / np.sqrt(var + self.eps)
            self.mean_list.append(mean)
            self.var_list.append(var)
        else:
            mean = sum(self.mean_list) / len(self.mean_list)
            var = sum(self.var_list) / len(self.var_list)
            var = var * self.variable.shape[0] / (self.variable.shape[0] - 1)
            variable = (variable - mean) / np.sqrt(var + self.eps)
        # end if

        return self.gamma * variable + self.beta
```

### 反向过程

在BN层的反向传播过程中，我们需要计算哪些东西呢？
首先是两个可学习的变量$\gamma$,$\beta$的梯度，我们需要计算出对应的梯度信息来更新这两个变量，其次，我们需要计算出这一层的误差函数来继续进行传播。计算梯度信息的过程和全连接层计算的过程几乎完全一样，这里只给出计算公式：

$$\gamma_{gradient}=C\times \hat{X} = \sum_i c_ix_i$$
$$\beta_{gradient} = \sum_i c_i$$

下面我们来推导误差传递的代码，根据反向传播公式，我们需要计算的是
$\frac{\partial C}{\partial X}$，根据链式法则，我们先计算$\frac{\partial C}{\partial \hat{X}}$。根据和全连接层类似的推导，我们可以很容易的得到：

$$
\frac{\partial C}{\partial \hat{X}} = \gamma C
$$

下面，根据链式法则，

$$
\frac{\partial C}{\partial X} = \frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial X} + \frac{\partial C}{\partial \sigma ^2} \frac{\partial \sigma^2}{\partial X} + \frac{\partial C}{\partial \mu} \frac{\partial \mu}{\partial X}
$$

我们分别来看各个部分的导数，

首先是，此处$\gamma C$ 为元素之间相乘的结果，因为各Channel的数据对应各自的常数$\gamma_i$

$$\frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial X} = \gamma C \frac{1}{\sqrt{\sigma^2 + \varepsilon}}$$

接着是

$$\frac{\partial C}{\partial \sigma ^2} \frac{\partial \sigma^2}{\partial X} = \frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial \sigma ^2} \frac{\partial \sigma^2}{\partial X}$$
$$ = \gamma C \cdot (X - \mu) \cdot (\frac{-0.5}{{(\sigma^2 + \varepsilon)}^{\frac{3}{2}}}) \frac{1}{m}$$
$$=(\sum_i (\gamma_i C_i) (x_i - \mu)) (\frac{-0.5}{{(\sigma^2 + \varepsilon)}^{\frac{3}{2}}})\frac{1}{m}$$

最后来看：

$$
\frac{\partial C}{\partial \mu} \frac{\partial \mu}{\partial X} = (\frac{\partial C}{\partial \hat{X}} \frac{\partial \hat{X}}{\partial \mu} + \frac{\partial C}{\partial \sigma^2} \frac{\partial \sigma^2}{\partial \mu})\frac{\partial \mu}{\partial X}$$
$$ = (\sum_i^m \gamma_i C_i \frac{-1}{\sqrt{\sigma^2 + \varepsilon}} + \frac{\partial C}{\partial \sigma^2} \cdot (-2)(X - \mu) \frac{1}{m}) \cdot \frac{1}{m}
$$

因此，我们只需要根据链式法则将上述几个式子相加即可得到最终结果。用代码表述即为：

```python
def backward(self, error):
        """backward function used in training

        This function will compute the gradient of the gamma and beta in BN layer
        and return error to previous layer

        Args:
            error: the error returned by the later layer in the network

        Return:
            the computed error to transfer to the previous layer
        """

        batch_size = self.variable.shape[0]
        mean = self.mean_list[-1]
        var = self.var_list[-1]

        # calculate the gradient to update gamma and beta
        self.gradient_gamma = np.sum(error * ((self.variable - mean) / np.sqrt(var + self.eps)), axis=0)
        self.gradient_beta = np.sum(error, axis=0)

        # the error to normalized variable, [1, channels] .* [batch, channels] = [batch, channles]
        error_normalized_variable = self.gamma * error

        # the error related to the var of the input variable, [1, channels]
        error_var = np.sum(error_normalized_variable * (self.variable - mean) * (-0.5) * (var + self.eps) ** (-1.5), axis = 0)

        # the error related to the mean of the input variable, [1, channels]
        error_mean = (-1) * np.sum(error_normalized_variable / np.sqrt(var + self.eps), axis=0) + error_var * (-2) * np.mean(self.variable - mean, axis=0)

        # sum the above error items to get the final error
        error = error_normalized_variable / np.sqrt(var + self.eps) + (error_var * 2 * (self.variable - mean) + error_mean) / batch_size

        return error
```

如果已知BN层的输入误差L，想要求BN层的输出误差，以便回传到前层。即求y_i对x_i的偏导。

从正向公式中看：


$$
{dJ \over dx_i} = {dJ \over d \hat{x_i}}{d \hat{x_i} \over dx_i} + {dJ \over d \sigma}{d \sigma \over dx_i} + {dJ \over d \mu}{d \mu \over dx_i}
$$

$$
{dJ \over d \hat{x_i}}=a
$$

$$
{dJ \over d \sigma}={dJ \over d \hat{x_i}}{d \hat{x_i} \over d \sigma} ={dJ \over d \hat{x_i}} \cdot -{1 \over 2}(x_i-\mu)(\sigma + \epsilon)^{-1.5} 
$$

$$
{d \hat{x_i} \over dx_i}={1 \over \sqrt{\sigma + \epsilon}}
$$

$$
{d \sigma \over dx_i} = {2(x_i - \mu) \over m}
$$

$$
{dJ \over d \mu}={dJ \over d \hat{x_i}}{d \hat{x_i} \over d \mu}={dJ \over d \hat{x_i}} \cdot {-1 \over \sqrt{\sigma + \epsilon}}
$$

$$
{d \mu \over dx_i} = {1 \over m}
$$

$$
{dJ \over dx_i} = {dJ \over d \hat{x_i}} \cdot {1 \over \sqrt{\sigma + \epsilon}} + {dJ \over d\sigma} \cdot {2(x_i - \mu) \over m} + {dJ \over d\mu} \cdot {1 \over m}
$$
