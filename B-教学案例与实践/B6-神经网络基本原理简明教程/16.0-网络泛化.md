Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 过拟合

## 拟合程度比较

在深度神经网络中，我们遇到的另外一个挑战，就是网络的泛化问题。所谓泛化，就是模型在测试集上的表现要和训练集上一样好。经常有这样的例子：一个模型在训练集上千锤百炼，能到达99%的准确率，拿到测试集上一试，准确率还不到90%。这说明模型过度拟合了训练数据，而不能反映真实世界的情况。解决过度拟合的手段和过程，就叫做泛化。

神经网络的两大功能：回归和分类。这两类任务，都会出现欠拟合和过拟合现象，如下图所示：

<img src=".\Images\16\fitting.png">

上图是回归任务中的三种情况，依次为：欠拟合、正确的拟合、过拟合。

<img src=".\Images\16\classification.png">

上图是分类任务中的三种情况，依次为：分类欠妥、正确的分类、分类过度。由于分类可以看作是对分类边界的拟合，所以我们经常也统称其为拟合。

上图中对于“深入敌后”的那颗绿色点样本，正确的做法是把它当作噪音看待，而不要让它对网络产生影响。而对于上例中的欠拟合情况，如果简单的（线性）模型不能很好地完成任务，我们可以考虑使用复杂的（非线性或深度）模型，即加深网络的宽度和深度，提高神经网络的能力。

但是如果网络过于宽和深，就会出现第三张图展示的过拟合的情况。

出现过拟合的原因：

1. 训练集的数量和模型的复杂度不匹配，样本数量级小于模型的参数
2. 训练集和测试集的特征分布不一致
3. 样本噪音大，使得神经网络学习到了噪音，正常样本的行为被抑制
4. 迭代次数过多，过分拟合了训练数据，包括噪音部分和一些非重要特征

## 过拟合的例子

我们将要使用MNIST数据集做例子，模拟出一个过拟合的情况。从上面的过拟合出现的4点原因分析，第2点和第3点对于MNIST数据集来说并不成立，所以我们只能利用第1点和第4点来模拟，即：故意减少数据集的样本数量，并且故意搭建一个复杂网络。

首先，只使用1000个样本来做训练（原始数据集有60000个样本），如下面的代码所示，调用一个ReadLessData(1000)函数，并且用GenerateDevSet(k=10)函数把1000个样本分成800和200两部分，分别做为训练集和验证集：

```Python
def LoadData():
    mdr = MnistImageDataReader(train_image_file, train_label_file, test_image_file, test_label_file, "vector")
    mdr.ReadLessData(1000)
    mdr.Normalize()
    mdr.GenerateDevSet(k=10)
    return mdr
```

然后，我们搭建一个深度网络：

<img src=".\Images\16\overfit_net.png">

这个网络有5个全连接层，前4个全连接层后接ReLU激活函数层，最后一个全连接层接Softmax分类函数做10分类。由于我们在第14章就已经搭建好了深度神经网络的Mini框架，所以可以简单地搭建这个网络，如下代码所示：

```Python
def Net(dateReader, num_input, num_hidden1, num_hidden2, num_hidden3, num_hidden4, num_output, params):
    net = NeuralNet(params)

    fc1 = FcLayer(num_input, num_hidden1, params)
    net.add_layer(fc1, "fc1")
    relu1 = ActivatorLayer(Relu())
    net.add_layer(relu1, "relu1")

    fc2 = FcLayer(num_hidden1, num_hidden2, params)
    net.add_layer(fc2, "fc2")
    relu2 = ActivatorLayer(Relu())
    net.add_layer(relu2, "relu2")

    fc3 = FcLayer(num_hidden2, num_hidden3, params)
    net.add_layer(fc3, "fc3")
    relu3 = ActivatorLayer(Relu())
    net.add_layer(relu3, "relu3")

    fc4 = FcLayer(num_hidden3, num_hidden4, params)
    net.add_layer(fc4, "fc4")
    relu4 = ActivatorLayer(Relu())
    net.add_layer(relu4, "relu4")

    fc5 = FcLayer(num_hidden4, num_output, params)
    net.add_layer(fc5, "fc5")
    softmax = ActivatorLayer(Softmax())
    net.add_layer(softmax, "softmax")

    net.train(dataReader, checkpoint=5)
    
    net.ShowLossHistory()
```

net.train(dataReader, checkpoint=5)函数的参数checkpoint的含义是，每隔5个epoch记录一次训练过程中的损失值和准确率。

在main过程中，设置一些超参数，然后调用刚才建立的Net进行训练：

```Python
if __name__ == '__main__':

    dataReader = LoadData()
    num_feature = dataReader.num_feature
    num_example = dataReader.num_example
    num_input = num_feature
    num_hidden = 30
    num_output = 10
    max_epoch = 200
    batch_size = 100
    learning_rate = 0.01
    eps = 1e-5

    params = CParameters(
      learning_rate, max_epoch, batch_size, eps,
      LossFunctionName.CrossEntropy3, 
      InitialMethod.Xavier, 
      OptimizerName.SGD)

    Net(dataReader, num_input, num_hidden, num_hidden, num_hidden, num_hidden, num_output, params)
```

在超参数中，我们指定了：
1. 每个隐层64个神经元（4个隐层在Net函数里指定）
2. 最多训练200个epoch
3. 批大小为10个样本
4. 学习率为0.1
5. 多分类交叉熵损失函数(CrossEntropy3)
6. Xavier权重初始化方法
7. 随机梯度下降算法

最终我们可以得到下图：

<img src=".\Images\16\overfit_result.png">

在训练集上（蓝色曲线），很快就达到了损失函数值趋近于0，准确度100%的程度。而在验证集上（红色曲线），损失函数值缺越来越大，准确度也在下降。这就造成了一个典型的过拟合网络，即所谓U型曲线，无论是损失函数值和准确度，都呈现出了这种分化的特征。

我们再看打印输出部分：
```
epoch=199, total_iteration=15999
loss_train=0.0004, accuracy_train=1.000000
loss_valid=0.9409, accuracy_valid=0.820000
time used: 6.960122585296631
total weights abs sum= 3721.4152347575673
total weights = 63104
little weights = 6378
zero weights = 60
```

结果说明：
1. 第199个epoch上（从0开始计数，所以实际是第200个epoch），训练集的损失为0，准确率为100%。测试集损失值0.9474，准确率84%。过拟合线性很严重。
2. 为什么total_iteration=15999呢？因为batch size=5，500个样本中的100个做为验证集，其余400个训练样本，400/5*200=16000，从0计数的话就是15999。
3. total weights abs sum = 3721.4，实际上是把所有全连接层的权重值先取绝对值，再求和。这个值和下面三个值在后面会有比较说明。
4. total weights = 63104，一共63104个权重值。
5. little weights = 6378，一共6378个权重值小于0.01。
6. zero weights = 60，是权重值中接近于0的数量（小于0.0001）。

在着手解决过拟合的问题之前，我们先来学习一下关于偏差与方差的知识，以便得到一些理论上的指导，虽然神经网络是一门实验学科。

# 偏差与方差

## 直观的解释

先用一个直观的例子来理解偏差和方差。比如打靶，如下图所示：

<img src=".\Images\16\variance_bias.png" width="600">

||低偏差|高偏差|
|---|---|---|
|低方差|射手很稳，枪的准星也很准。|射手很稳，但是枪的准星有问题，所有子弹都固定地偏向一侧。|
|高方差|射手不太稳，但枪的准星没问题，虽然弹着点分布很散，但没有整体偏移。|射手不稳，而且枪的准星也有问题，弹着点分布很散且有规律地偏向一侧。|

## 神经网络训练的例子

我们在前面讲过数据集的使用，包括训练集、验证集、测试集。在训练过程中，我们要不断监测训练集和验证集在当前模型上的误差，和上面的打靶的例子一样，有可能产生四种情况：

|误差|情况1|情况2|情况3|情况4|
|---|---|---|---|---|
|训练集|1.5%|12.3%|1.2%|12.3%|
|验证集|1.7%|11.4%|13.1%|21.5%|
|偏差与方差|低偏差，低方差|高偏差，低方差|低偏差，高方差|高偏差，高方差|
|解释|好|欠拟合|过拟合|情况不明|

上述四种情况中：

1. 效果很好，可以考虑进一步降低误差值，提高准确度
2. 训练集和验证集同时出现较大的误差，有可能是：迭代次数不够、数据不好、网络设计不好，需要继续训练，观察误差变化情况
3. 训练集的误差已经很低了，但验证集误差很高，说明过拟合了，即训练集中的某些特殊样本影响了网络参数，但类似的样本在测试集中并没有出现
4. 两者误差都很大，目前还看不出来是什么问题，需要继续训练

## 公式

|符号|含义|
|---|---|
|$x$|测试样本|
|$D$|数据集|
|$y$|x的真实标记|
|$y_D$|x在数据集中标记(可能有误差)|
|$f$|从数据集D学习的模型|
|$f_{x;D}$|从数据集D学习的模型对x的预测输出|
|$f_x$|模型f对x的期望预测输出|

学习算法期望的预测：
$$f_x=E[f_{x;D}] \tag{1}$$
不同的训练集/验证集产生的预测方差：
$$var(x)=E[(f_{x;D}-f_x)^2] \tag{2}$$
噪声：
$$\epsilon^2=E[(y_D-y)^2] \tag{3}$$
期望输出与真实标记的偏差：
$$bias^2(x)=(f_x-y)^2 \tag{4}$$
算法的期望泛化误差：
$$
\begin{aligned}
E(f;D)&=E[(f_{x;D}-y_D)^2] \\
&=E[(f_{x;D}-f_x+f_x-y_D)^2] \\
&=E[(f_{x;D}-f_x)^2]+E[(f_x-y_D)^2] \\
&+E[2(f_{x;D}-f_x)(f_x-y_D)](从公式1，此项为0) \\
&=E[(f_{x;D}-f_x)^2]+E[(f_x-y_D)^2] \\
&=E[(f_{x;D}-f_x)^2]+E[(f_x-y+y-y_D)^2] \\
&=E[(f_{x;D}-f_x)^2]+E[(f_x-y)^2]+E(y-y_D)^2] \\
&+E[2(f_x-y)(y-y_D)](噪声期望为0，所以此项为0)\\
&=E[(f_{x;D}-f_x)^2]+(f_x-y)^2+E[(y-y_D)^2] \\
&=var(x) + bias^2(x) + \epsilon^2
\end{aligned}
$$

所以，各个项的含义是：

- 偏差：度量了学习算法的期望与真实结果的偏离程度，即学习算法的拟合能力。
- 方差：训练集与验证集的差异造成的模型表现的差异。
- 噪声：当前数据集上任何算法所能到达的泛化误差的下线，即学习问题本身的难度。

想当然地，我们希望偏差与方差越小越好，但实际并非如此。一般来说，偏差与方差是有冲突的，称为偏差-方差窘境 (bias-variance dilemma)。

- 给定一个学习任务，在训练初期，由于训练不足，网络的拟合能力不够强，偏差比较大，也是由于拟合能力不强，数据集的特征也无法使网络产生显著变化，也就是欠拟合的情况。
- 随着训练程度的加深，网络的拟合能力逐渐增强，训练数据的特征也能够渐渐被网络学到。
- 充分训练后，网络的拟合能力已非常强，训练数据的微小特征都会导致网络发生显著变化，当训练数据自身的、非全局的特征被网络学到了，则将发生过拟合。

<img src=".\Images\16\error.png" width="600">

在上图中，随着训练程度的增加，偏差（蓝色实线）一路下降，但是方差（蓝色虚线）一路上升，整体误差（红色实线，偏差+方差+噪音误差）呈U形，最佳平衡点就是U形的最低点。

# 解决过拟合问题

有了直观感受和理论知识，下面我们看看如何解决过拟合问题。

数据

1. 数据扩展 Data Augmentation
2. 特征工程

模型


1. 简化模型
2. 集成学习法 ensemble
   bagging通过平均多个模型的结果，来降低模型的方差
   boosting不仅能减小偏差，也能减小方差2. 


   
筛选组合得到更高质量的特征

4. 正则 Regularization



算法

4. 丢弃法
5. 早停法 early stopping

# 参考资料

- 周志华老师的西瓜书
- http://scott.fortmann-roe.com/docs/BiasVariance.html

# 代码位置

ch16, Level0
