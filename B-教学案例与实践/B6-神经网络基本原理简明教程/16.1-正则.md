Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


https://blog.csdn.net/liangyihuai/article/details/78811664


## L1&L2 Norm

正则化项一般使用在损失函数的计算中，用于约束某些权重不至于过大，导致网络爆炸或者过拟合现象的产生，一般使用的形式为：
$$ loss = J(predict, groundtruth) + \alpha Regulazation$$
这里的正则化即代表了对参数的约束条件，$\alpha$参数代表了在当前正则化项中对参数的约束程度。目前常常使用的正则化方法是L1正则化和L2正则化，下面我们将分别介绍这两种方法

### L1 Norm

L1正则化项，代表了参数的曼哈顿距离，其实就是将各个参数的绝对值进行相加，约束这样一个参数和不至于过大，

$$ L_1 = \sum_i|w_i| = ||w||_1$$

使用L1正则化去训练模型有助于让我们得到一个相对稀疏的权重矩阵，以二维空间为例，我们做出$|w_1| + |w_2| = k$这样的矩阵如下图：

<img src=".\Images\16\3.png">


我们在这样一个矩形框上进行均匀采样的话，会发现所采集的样本会比较集中在四个角上，也就是说，会有更多的权重会趋近于零，也就造成了权重矩阵的稀疏性。

这样一个L1 Norm的前向和反向传播是很显然的，也就是根据参数的符号，传递1和-1即可。也就是说，假设原始$w_i$对应的梯度是$d$的话，那么我们得到的新的梯度就是

$$d^/' = d + \alpha \times sign(w_i)$$

### L2 Norm

比起L1 Norm， L2 Norm的使用更为频繁，L2 Norm就是欧氏距离，采用的是约束的参数的平方和的形式，

$$ L_2 = \sum_i (w_i)^2 = ||w||_2$$

$$ loss = J(predict, groundtruth) + \lambda \frac{1}{2m} \sum_i^m (w_i)^2$$

使用L2 Norm能够约束各个参数不至于过大，同时也不会造成很大的稀疏性，因为L2 Norm形成的边框是一个圆，在这个边框上进行采样并不会有大量的数据集中在坐标轴附近，而是较为均匀的分布的。

<img src=".\Images\16\4.png">


类似得，假设原始$w_i$的梯度是$d$的话，我们可以得到加上L2 Norm之后的梯度$d^/'$为：

$$d^/' = d + \frac{\lambda}{m}w_i$$

# 参考资料

https://blog.csdn.net/red_stone1/article/details/80755144