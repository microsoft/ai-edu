Copyright © Microsoft Corporation. All rights reserved.
适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可
  

# 定义神经网络结构

我们是首次尝试建立神经网络，先搞一个最简单的单层单点神经元：

<img src=".\Images\4\Setup.jpg"> 

## 输入层

它在输入层只接受一个输入，经过参数w,b的计算后，直接输出结果。这样一个简单的“网络”，只能解决简单的一元线性回归问题，而且由于是线性的，我们不需要定义激活函数，这就大大简化了程序，而且便于大家循序渐进地理解各种知识点。下面，我们在这个最简的线性回归的例子中，来说明神经网络中最重要的反向传播和梯度下降的概念和过程以及编码实现。

$$
X=\begin{pmatrix}
x_1 & x_2 & \dots & x_{200}
\end{pmatrix}
\\
Y=\begin{pmatrix}
y_1 & y_2 & \dots & y_{200}
\end{pmatrix}
$$

其中，x就是上图中红色点的横坐标值（服务器数），y是纵坐标值（空调功率）。

## 权重W/B
因为是一元线性问题，所以W/B都是一个标量，记为w和b，我们认为这组数据有这个关系：$y = w*x+b$

## 输出层

输出层1个神经元，是上述预测公式的直接输出，但定义上有所变化，应该是$z = w*x+b$，z是我们的预测输出，y是实际的样本标签值。


# 读取文件数据

**注意：从本章开始，所有的样本数据的格式都是：每一列是一个样本的所有特征，每一行是某个特征的所有样本。**

比如，如果有一个4个样本的数据集，每个样本有两个特征，其数据结构是这样的：
最终，样本数据的样子是：
|样本Id|1|2|3|4|(更多样本向右扩展)|
|---|---|---|---|---|---|
|特征1|$x_{1,1}$|$x_{2,1}$|$x_{3,1}$|$x_{4,1}$|...|
|特征2|$x_{1,2}$|$x_{2,2}$|$x_{3,2}$|$x_{4,2}$|...|
|(更多特征向下扩展)|...|...|...|...|...|

对于标签数据Y，一般只有一个标签值，所以是个(1,m)的二维矩阵，与X对应的。

```Python
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

x_data_name = "TemperatureControlXData.dat"
y_data_name = "TemperatureControlYData.dat"

def ReadData():
    Xfile = Path(x_data_name)
    Yfile = Path(y_data_name)
    if Xfile.exists() & Yfile.exists():
        X = np.load(Xfile)
        Y = np.load(Yfile)
        # 注意这里和前面的例子不同
        return X.reshape(1,-1),Y.reshape(1,-1)
    else:
        return None,None
```
在上面的code里的reshape，实际上是把一个一维数组(m,)变成1行m列的二维数组(1,m)。这个例子中，X只有一个特征，所以只有一行。

# 前向计算
```Python
def ForwardCalculation(w,b,x):
    z = w * x + b
    return z
```

**关于Python的函数命名规范，一般是用aa_bb这种形式，而不是AaBb的形式，这是个人习惯而已，大家自己随意。**
**而关于变量命名规范，我个人习惯用大写X表示一个矩阵，用小写x表示一个变量或一个样本。**

# 损失函数

我们用传统的均方差函数，其中，z是每一次迭代的预测输出，y是样本标签数据。我们使用所有样本参与计算，因此损失函数实际为：

$$Loss = \frac{1}{2m}\sum_{i=1}^{m}(z_i - y_i) ^ 2$$

其中的分母中有个2，实际上是想在求导数时把这个2约掉，没有什么原则上的区别。

**为什么使用所有样本参与计算呢？因为单个样本或少量样本的损失并不能代表广大人民群众的利益。一条直线可能正好穿过一个点，那么对于这个点来说，它的误差为0，但对于其它样本来说，误差就可能很大。**

我们暂时不需要实现这个损失函数，只是用来定义梯度下降时的求导过程。

# 反向传播

下面的代码是通过《4.2-梯度下降》中的公式推导而得的。

```Python
def BackPropagation(x,y,z):
    dZ = z - y
    dB = dZ
    dW = dZ * x
    return dW, dB
```
dZ是中间变量，避免重复计算。dZ又可以写成delta_Z，是某一层神经网络的反向误差输入。

# 梯度更新

```Python
def UpdateWeights(w, b, dW, dB, eta):
    w = w - eta*dW
    b = b - eta*dB
    return w,b
```

# 推理
```Python
def Inference(w,b,x):
    z = ForwardCalculation(w,b,x)
    return z
```
推理过程，实际上就是一个前向计算过程，我们把它单独拿出来，方便对外接口的设计。


# 结果显示函数

```Python
def ShowResult(X, Y, w, b, iteration):
    # draw sample data
    plt.plot(X, Y, "b.")
    # draw predication data
    PX = np.linspace(0,1,10)
    PZ = w*PX + b
    plt.plot(PX, PZ, "r")
    plt.title("Air Conditioner Power")
    plt.xlabel("Number of Servers(K)")
    plt.ylabel("Power of Air Conditioner(KW)")
    plt.show()
    print(iteration)
    print(w,b)
```

**对于初学神经网络的人来说，可视化的训练过程及结果，可以极大地帮助理解神经网络的原理。**

# 主程序
```Python
if __name__ == '__main__':
    # learning rate
    eta = 0.1
    # set w,b=0, you can set to others values to have a try
    #w, b = np.random.random(),np.random.random()
    w, b = 0, 0
    # create mock up data
    X, Y = ReadData()
    # count of samples
    num_example = X.shape[1]

    for i in range(num_example):
        # get x and y value for one sample
        x = X[0,i]
        y = Y[0,i]
        # get z from x,y
        z = ForwardCalculation(w, b, x)
        # calculate gradient of w and b
        dW, dB = BackPropagation(x, y, z)
        # update w,b
        w, b = UpdateWeights(w, b, dW, dB, eta)

    ShowResult(X, Y, w, b, 1)

    result = Inference(w,b,0.346)
    print("result=", result)
```

# 运行结果

<img src="./Images/4/SGD.png"/>

```
iteration= 1
w=1.918221,b=3.077801
result= 3.741505043252122
```

最终我们得到了W=1.918，B=3.077，然后根据这两个值画出了上图中的红线$y=1.918x+3.077$。

预测时，已知有346台服务器，先要除以1000，因为横坐标是以K(千台)服务器为单位的，代入前向计算函数，得到的结果是3.74千瓦。

