Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


## 多分类函数 - Softmax

### 形式与特点

softmax函数，是大名鼎鼎的在计算多分类问题时常使用的一个函数，他长成这个样子:

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}} \tag{对第j项的分类值计算结果}
$$

上式中:
- $z_j$是对第 j 项的分类原始值（即矩阵运算的结果，假设j=1，上图中$z_1=w_{11}x_1+w_{12}x_2+b1$）
- $z_i$是参与分类计算的每个类别的原始值（即矩阵运算的结果，上图中$z_i=w_{i1}x_1+w_{i2}x_2+bi$）
- m 是总的分类数（上图中m=3）
- $a_j$是对第 j 项的计算结果（假设j=1，上图中$a_1=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_1)}{exp(z_1)+exp(z_2)+exp(z_3)}$）

也就是说把接收到的输入归一化成一个每个分量都在$(0,1)$之间并且总和为一的一个概率函数。在上图中，$A1+A2+A3=1$。

分类的方式是：看A1/A2/A3三者谁的比重最大，就算是那一类。比如A1=0.6, A2=0.1, A3=0.3，则这个样本就算是第一类了。

用一张图来形象说明这个过程：

<img src=".\Images\7\softmax.png">

当输入的数据是3，1，-3时（$z_1,z_2,z_3$），按照图示过程进行计算，可以得出输出的概率分布是0.88，0.12，0。

试想如果我们并没有这样一个softmax的过程而是直接根据[3，1，-3]这样的输出，而我们期望得结果是[1，0，0]这样的概率分布结果，那传递给网络的信息是什么呢？

其实[3,1,-3]这一组数字和[1,0,0]是不可比的，就好像用1公里的“1”和1毫米的“1”去比。如果非要比的话，第一类3-1=2，要抑制，但抑制多少呢？第三类-3-0=-3，要鼓励？真的要鼓励吗？想不清楚。所以Softmax函数就有用了，它先归一化所有输入数据到[0,1]之间，然后再和标签值[1,0,0]去比，一是可比了，二是知道要抑制或鼓励的幅度。

从继承关系的角度来说，softmax函数可以视作sigmoid的一个扩展，比如我们来看一个二分类问题，

$$
A_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2 - z_1}} = \frac{1}{1 + e^{z_2} e^{- z_1}}
$$

是不是和sigmoid的函数形式非常像？比起原始的sigmoid函数，softmax的一个优势是可以用在多分类的问题中。另一个好处是在计算概率的时候更符合一般意义上我们认知的概率分布，体现出物体属于各个类别相对的概率大小。

### Softmax求导

由于Softmax涉及到求和，所以有两种情况：
- 当预测的分类项 j 和标签项 i 一致时，$i=j$。比如分类结果是[0.8, 0.1, 0.1]，第一项最大。标签项是[1, 0, 0]，表示当前样本确实是第一类。
- 当预测的分类项 j 和标签项 i 不一致时，$i \neq j$。比如分类结果是[0.1, 0.8, 0.1]，第二项最大。标签项是[1, 0, 0]，表示当前样本应该是第一类。

Softmax函数的分子：因为是计算$a_j$，所以分子是$e^{z_j}$
Softmax函数的分母：
$$
\sum\limits_{i=1}^m e^{z_i} = e^{z_1} + \dots + e^{z_j} + \dots +e^{z_m} = \sum \tag{为了方便简写成$\sum$}
$$

- $i=j$时（比如输出分类值a1对z1的求导），求$a_j$对$z_i$的导数，此时分子上的$e^{z_j}$要参与求导。参考基本数学导数公式33：

$$
\frac{\partial{a_j}}{\partial{z_i}} = \frac{\partial{(e^{z_j}/(e^{z_1} + \dots + e^{z_j} + \dots +e^{z_m}))}}{\partial{z_i}}
$$
$$
= \frac{\partial{}}{\partial{z_i}}(e^{z_j}/\sum)  = \frac{\partial{}}{\partial{z_j}}(e^{z_j}/\sum) \tag{因为$z_i=z_j$}
$$
$$
=\frac{e^{z_j}\sum-e^{z_j}e^{z_j}}{(\sum)^2} =\frac{e^{z_j}}{\sum} - \frac{(e^{z_j})^2}{(\sum)^2} \\
$$
$$
= a_j-a^2_j=a_j(1-a_j)
$$

也就是说，此时softmax的梯度就是$a_j(1-a_j)$。

- $i \neq j$时（比如输出分类值a1对z2的求导，j=1, i=2），$a_j$对$z_i$的导数，分子上的$z_j与i$没有关系，求导为0，分母的求和项中$e^{z_i}$要参与求导。同样是公式33，因为分子$e^{z_j}$对$e^{z_i}$求导的结果是0：

$$
\frac{\partial{a_j}}{\partial{z_i}}=\frac{-(\sum)'e^{z_j}}{(\sum)^2}
$$
求和公式对$e^{z_i}$的导数$(\sum)'$，除了$e^{z_i}$项外，其它都是0：
$$
(\sum)' = (e^{z_1} + \dots + e^{z_i} + \dots +e^{z_m})'=e^{z_i}
$$
所以：
$$
\frac{\partial{a_j}}{\partial{z_i}}=\frac{-(\sum)'e^{z_j}}{(\sum)^2}=-\frac{e^{z_j}e^{z_i}}{{(\sum)^2}}=-\frac{e^{z_j}}{{\sum}}\frac{e^{z_j}}{{\sum}}=-a_{i}a_{j}
$$

### 结合损失函数的整体反向传播公式

<img src=".\Images\7\Loss-A-Z.jpg">

看上图，假设我们要求Loss值对Z1的偏导数，应该怎么做呢？

先从Loss的公式看，$Loss=-(y_1lna_1+y_2lna_2+y_3lna_3)$，a1肯定与z1有关，那么a2,a3是否与z1有关呢？

再从Softmax函数的形式来看：
$$
a_1=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_1)}{exp(z_1)+exp(z_2)+exp(z_3)}
$$
$$
a_2=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_2)}{exp(z_1)+exp(z_2)+exp(z_3)}
$$
$$
a_3=\frac{exp(z_1)}{\sum_i exp(z_i)}=\frac{exp(z_3)}{exp(z_1)+exp(z_2)+exp(z_3)}
$$
无论是a1，a2，a3，都是与z1相关的，而不是一对一的关系，所以，想求Loss对Z1的偏导，必须把Loss->A1->Z1， Loss->A2->Z1，Loss->A3->Z1，这三条路的结果加起来。于是有了如下公式：

$$
\frac{\partial{Loss}}{\partial{z_i}}= \frac{\partial{Loss}}{\partial{a_1}}\frac{\partial{a_1}}{\partial{z_i}} + \frac{\partial{Loss}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_i}} + \frac{\partial{Loss}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_i}}\\
=\sum_j \frac{\partial{Loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}
$$

你可以假设上式中$i=1，j=3$，就完全符合我们的假设了，而且不失普遍性。

前面说过了，因为Softmax涉及到各项求和，A的分类结果和Y的标签值分类是否一致，所以需要分情况讨论：

$$
\frac{\partial{a_j}}{\partial{z_i}} = \begin{cases} a_j(1-a_j), & i = j \\ -a_ia_j, & i \neq j \end{cases}\\
$$

因此，$\frac{\partial{Loss}}{\partial{z_i}}$应该是$i=j和i \neq j$两种情况的和：
- $i = j时，Loss通过A1对Z1求导（或者是通过A2对Z2求导）$：

$$
\frac{\partial{Loss}}{\partial{z_i}} = \frac{\partial{Loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=-\frac{y_j}{a_j}*a_j(1-a_j)=y_j(a_j-1)=y_i(a_i-1) \tag{公式一}
$$

- $i \neq j，Loss通过A2+A3对Z1求导$：

$$
\frac{\partial{Loss}}{\partial{z_i}} = \frac{\partial{Loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=\sum_j^m(-\frac{y_j}{a_j})(-a_ja_i)=\sum_j^m(y_ja_i)=a_i\sum{y_j} \tag{公式二}
$$

如果不好理解的话，就拆开上面的公式，假设求Loss通过a2/a3对z1的导数：

$$
\frac{\partial{Loss}}{\partial{z_1}}=\frac{\partial{Loss}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_1}}+\frac{\partial{Loss}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_1}} \\
$$
因为：
$$
\frac{\partial{Loss}}{\partial{a_2}}=-\frac{y_2}{a_2} \\
\frac{\partial{a_2}}{\partial{z_1}}=-a_2a_1 \\
\frac{\partial{Loss}}{\partial{a_3}}=-\frac{y_3}{a_3} \\
\frac{\partial{a_3}}{\partial{z_1}}=-a_3a_1
$$
所以：
$$
\frac{\partial{Loss}}{\partial{z_1}} = (-\frac{y_2}{a_2})(-a_2a_1)+(-\frac{y_3}{a_3})(-a_3a_1)=y_2a_1+y_3a_1=a1(y_2+y_3)=a_i\sum_{j\neq i}y_j
$$

把两种情况加起来：
$$
\frac{\partial{Loss}}{\partial{z_i}} = y_i(a_i-1)+a_i\sum_{j \neq i}y_j
$$
$$
=-y_i+a_iy_i+a_i\sum_{j \neq i}y_j =-y_i+a_i(y_i+\sum_{j \neq i}y_j)
$$
$$
=-y_j + a_i\sum_jy_j =-y_j + a_i*1=a_i-y_j
$$
因为$y_j$是取值[1,0,0]或者[0,1,0]或者[0,0,1]的，这三者用$\sum$加起来，就是[1,1,1]，在矩阵乘法运算里乘以[1,1,1]相当于什么都不做，就等于原值。

