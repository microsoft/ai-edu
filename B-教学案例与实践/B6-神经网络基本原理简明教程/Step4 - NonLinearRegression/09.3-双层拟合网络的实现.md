Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 用于拟合的双层神经网络的实现

先来观察一下样本：

|样本|1|2|3|...|1000|
|---|---|---|---|---|---|
|x|0.606|0.129|0.643|...|0.199|
|y|-0.113|-0.269|-0.217|...|-0.281|

首先观察一下样本数据的范围，x是在[0,1]，y是[-0.5,0.5]，这样我们就不用做数据归一化了。这条线看起来像一条处于攻击状态的眼镜蛇！

由于是拟合任务，所以标签值y是一系列的实际数值，并不是0,1这样的特殊标记。

<img src="..\Images\9\Sample.png">

## 定义神经网络结构

我们定义一个两层的神经网络，输入层不算，一个隐藏层，含4个神经元，一个输出层。

<img src="..\Images\9\nn.png">

## 输入层

输入层就是一个标量x值。

## 权重矩阵W1/B1

$$
W1=
\begin{pmatrix}
w_{1,1} \\
w_{2,1} \\
w_{3,1} \\
w_{4,1} \\
\end{pmatrix}
$$

其实这里的B1所在的圆圈里应该是个常数1，而B1连接到Z1-1...Z1-4的权重线B1-1...B1-4应该是个浮点数。我们为了说明问题方便，就写了个B1，而实际的B1是指B1-1...B1-4的矩阵/向量。

$$
B1=
\begin{pmatrix}
b_{1,1} \\
b_{2,1} \\
b_{3,1} \\
b_{4,1} \\
\end{pmatrix}
$$


## 隐层

我们用一个4个神经元的网络来模拟函数，每个神经元的输入$Z1 = W1 \cdot X + B1$，我们在这里使用sigmoid函数，所以输出是$A1 = Sigmoid(Z1)$。

$$
Z1 = \begin{pmatrix}
z_{1,1} \\ 
z_{2,1} \\ 
z_{3,1} \\
z_{4,1} \end{pmatrix},
A1 = \begin{pmatrix}
a_{1,1} \\ 
a_{2,1} \\ 
a_{3,1} \\
a_{4,1} \end{pmatrix}
$$


## 权重矩阵W2/B2

W2的尺寸是1x4，B2的尺寸是1x1。
$$
W2=
\begin{pmatrix}w_{1,1} & w_{1,2} & w_{1,3} & w_{1,4} \end{pmatrix}
$$

$$
B2=
\begin{pmatrix}
b_{1,1}
\end{pmatrix}
$$

## 输出层

由于我们只想完成一个拟合任务，所以输出层只有一个神经元，$Z2=W2 \cdot A1+B2$。


## 前向计算图

刚开始学习神经网络时，总没有矩阵尺寸的概念，所以建议大家用下图的方式来加强一下认识，其中矩形的宽和高象征性地表示了这个矩阵的形状。

<img src="..\Images\9\fc.png">

至此，我们得到了以下一串公式：

$$Z1=W1 \cdot X+B1$$

$$A1=Sigmoid(Z1)$$

$$Z2=W2 \cdot A1+B2$$

$$A2=Identity(Z2) \tag{这一步可以省略}$$

## 代码结构

<img src="..\Images\9\modules.png">

### 四个底层模块
- Data Reader
  - 读取数据，处理数据
- Activations
  - 激活函数库，包括正向和反向
- Loss Function
  - 损失函数库，损失值历史记录
- WeightsBias
  - 权重参数，初始化，更新

### 双层网络主模块

流程说明：

```Python
    def train(self, dataReader, params, loss_history):
        # 初始化权重参数
        wb1 = WeightsBias(params.num_input, params.num_hidden, params.eta)
        wb1.InitializeWeights()
        wb2 = WeightsBias(params.num_hidden, params.num_output, params.eta)
        wb2.InitializeWeights()

        # 初始化损失值记录器
        loss = 0 
        lossFunc = CLossFunction(params.loss_func_name)

        # 计算批大小和内循环次数
        max_iteration = (int)(dataReader.num_example / params.batch_size)
        # 外循环，控制epoch次数
        for epoch in range(params.max_epoch):
            # 每个epoch都要打乱数据顺序
            #dataReader.Shuffle()
            # 控制内循环次数
            for iteration in range(max_iteration):
                # 获得当前批次的样本数据和标签
                batch_x, batch_y = dataReader.GetBatchSamples(params.batch_size,iteration)
                # 前向计算
                dict_cache = self.ForwardCalculationBatch(batch_x, wb1, wb2)
                # 反向计算梯度
                self.BackPropagationBatch(batch_x, batch_y, dict_cache, wb1, wb2)
                # 更新权重数组
                self.UpdateWeights(wb1, wb2)
            # end for            
            # 计算全批量损失值并记录
            output = self.ForwardCalculationBatch(dataReader.X, wb1, wb2)
            loss = lossFunc.CheckLoss(dataReader.Y, output["Output"])
            print("epoch=%d, loss=%f" %(epoch,loss))
            loss_history.AddLossHistory(loss, epoch, iteration, wb1, wb2)            
            # 检查停止条件
            if loss < params.eps:
                break
            # end if
        # end for
        # 返回训练好的权重值
        return wb1, wb2
    # end def
```

## 运行结果

按照上述代码的“标准”设置，我们可以得到以下结果：

|损失函数值|拟合结果|
|---|---|
|<img src="..\Images\9\sgd_loss.png">|<img src="..\Images\9\sgd_result.png">|


代码位置：ch09, Level1
