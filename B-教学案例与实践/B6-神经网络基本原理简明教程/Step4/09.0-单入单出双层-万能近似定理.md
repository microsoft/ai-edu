Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 提出问题

验证前馈神经网络的万能近似定理。

## 问题：给出如下一批训练数据，如何使用神经网络方法来拟合这条曲线？

|样本|1|2|3|...|1000|
|---|---|---|---|---|---|
|特征X|0.606|0.129|0.643|...|0.199|
|标签Y|-0.113|-0.269|-0.217|...|-0.281|

我们可视化一下训练数据：

<img src=".\Images\9\Sample.png">


$$y=0.4x^2 + 0.3xsin(15x) + 0.01cos(50x)-0.3$$

我们特意把数据限制在[0,1]之间，避免做归一化的麻烦。要是觉得这个公式还不够复杂，大家可以用更复杂的公式去自己试验。

以上问题可以叫做非线性回归，即自变量X和因变量Y之间不是线性关系。常用的传统的处理方法有线性迭代法、分段回归法、迭代最小二乘法等。在神经网络中，解决这类问题的思路非常简单，就是使用带有一个隐层的两层神经网络。


# 理论解释

这里有一篇论文，Kurt Hornik在1991年发表的，说明了含有一个隐层的神经网络能拟合任意复杂函数：

https://www.sciencedirect.com/science/article/pii/089360809190009T

Abstract - We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to L(u) performance criteria, for arbitrary finite input envrionment measres u, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives.

简言之：两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它能以任意精度来近似拟合任意连续函数。原文提到Borel可测函数，超出了本课程的范围。

# 直观解释

还有一种直观的解释是这样的（个人认为这不是神经网络的工作原理），用无数个矩形的拼接来近似一条曲线：

|粗粒度|细粒度|
|---|---|
|<img src=".\Images\9\histogram1.jpg">|<img src=".\Images\9\histogram2.jpg">|

它的理论基础是：

<img src=".\Images\9\histogram3.jpg">

假设在隐层有两个神经元，都配置有Sigmoid激活函数。第一个神经元在-0.01处产生一个阶跃，第二个神经元在+0.01处产生一个阶跃，都是用Sigmoid函数完成的，然后用第一个神经元的输出减去第二个神经元的输出（设置权重值为[1,-1]）,就会形成一个“门”。无数个这样的“门”就能模拟出一条曲线。

对于三维空间，它是这个样子的：

|阶跃面|封闭塔|
|---|---|
|<img src=".\Images\9\histogram4.jpg">|<img src=".\Images\9\histogram5.jpg">|

在三维空间中，两个有不同偏置值的sigmoids激活函数相减，我们将得到左侧的等效曲线。如果我们采用另一个水平垂直的塔架到现在组合的曲线上。在叠加这两个水平垂直的开放式塔时，我们就可以得到封闭的塔。然后就可以用封闭塔模拟任何三维曲面。

# 参考资料

https://towardsdatascience.com/representation-power-of-neural-networks-8e99a383586
