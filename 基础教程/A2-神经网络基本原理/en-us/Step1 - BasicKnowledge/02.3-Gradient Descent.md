<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 2.3 Gradient Descent

### 2.3.1 Understanding gradient descent from natural phenomena

Most articles use the example of "a person is trapped in a mountain and needs to descend to the bottom" to illustrate gradient descent. This person will "look for the steepest place in his current position and walk down." However, this example ignores the consideration of safety. This person cannot just walk in the steepest direction and must consider the angle of the slope.

The best example of gradient descent in nature is the process of spring water flowing down a mountain:

1. Water is affected by gravity and will flow down following the steepest path from its current location, sometimes forming a waterfall (gradient descent);
2. The path of the water flowing down the mountain is not unique. In the same location, there may be multiple locations with the same steepness, resulting in a fork(multiple solutions can be obtained);
3. When encountering lakebeds, water will accumulate into lakes, and the downhill process is terminated (can not get the optimal global solution, but can get the optimal local solution).

### 2.3.2 Mathematical understanding of gradient descent

The mathematical formula of gradient descent:

$$\theta_{n+1} = \theta_{n} - \eta \cdot \nabla J(\theta) \tag{1}$$

In this formula:

- $\theta_{n+1}$：the next value;
- $\theta_n$：the current value；
- $-$：minus sign, the reverse of gradient；
- $\eta$：Learning rate or step size, controls the distance of each step. Ideally the rate is not too fast so as to miss the optimal solution, and not too slow so that the process takes too long;
- $\nabla$：Gradient, the fastest rising point of the current position of the function;
- $J(\theta)$：The function.

#### The three elements of gradient descent

1. Current point;
2. Direction;
3. Step length.

#### Why is it called "gradient descent"?

"Gradient descent" has two layers of meanings:

1. Gradient: the fastest rising point of the current position of the function;
2. Descent: The direction opposite to the derivative, described in mathematical language, which is the minus sign.

That is, moving in the opposite direction to rising is falling.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd_concept.png" ch="500" />

Figure 2-9 Steps of gradient descent

Figure 2-9 explains the calculation process of gradient descent on both sides of the function extreme point. The purpose of gradient descent is to make the x value approach the extreme point.

### 2.3.3 Gradient descent of a univariate function

Suppose we have a single variable function:

$$J(x) = x ^2$$

Our goal is to find the minimum value of this function, so calculate its derivative:

$$J'(x) = 2x$$

Suppose the initial position is:

$$x_0=1.2$$

Assume the learning rate is:

$$\eta = 0.3$$

According to formula (1), the iterative formula is:

$$x_{n+1} = x_{n} - \eta \cdot \nabla J(x)= x_{n} - \eta \cdot 2x$$

Assuming the termination condition is $J(x)<0.01$, the iteration process is as follows:
```
x=0.480000, y=0.230400
x=0.192000, y=0.036864
x=0.076800, y=0.005898
x=0.030720, y=0.000944
```

The above process is shown in Figure 2-10.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd_single_variable.png" ch="500" />

Figure 2-10 The iterative process of using the gradient descent method

### 2.3.4 Bivariate Gradient Descent

Suppose we have a two-variable function:

$$J(x,y) = x^2 + \sin^2(y)$$

Our goal is to find the minimum value of this function, so calculate its derivative:

$${\partial{J(x,y)} \over \partial{x}} = 2x$$
$${\partial{J(x,y)} \over \partial{y}} = 2 \sin y \cos y$$

Suppose the initial position is:

$$(x_0,y_0)=(3,1)$$

Assume the learning rate is:

$$\eta = 0.1$$

According to formula (1), the iterative formula is:
$$(x_{n+1},y_{n+1}) = (x_n,y_n) - \eta \cdot \nabla J(x,y)$$
$$ = (x_n,y_n) - \eta \cdot (2x,2 \cdot \sin y \cdot \cos y) \tag{1}$$

According to formula (1), assuming the termination condition is $J(x,y)<0.01$, the iteration process is shown in Table 2-3.

Table 2-3 Iterative process of bivariate gradient descent

|Number of iterations|x|y|J(x,y)|
|---|---|---|---|
|1|3|1|9.708073|
|2|2.4|0.909070|6.382415|
|...|...|...|...|
|15|0.105553|0.063481|0.015166|
|16|0.084442|0.050819|0.009711|

After 16 iterations, the value of $J(x,y)$ is $0.009711$, which satisfies the condition of being less than $0.01$, and the iteration stops.

The above process is shown in Table 2-4. Because it is bivariate, it needs to be explained by a three-dimensional graph. Please pay attention to the faint black line in the middle of the two pictures, which represents the process of gradient descent,  from the red highlands to the low blue lands along the slope.

Table 2-4 Gradient descent process in three-dimensional space

|Observation angle 1|Observation angle 2|
|--|--|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\2\gd_double_variable.png">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\2\gd_double_variable2.png">|

### 2.3.5 Choice of learning rate η

In a formula, the learning rate is expressed as $\eta$. In code, we define the learning rate as `learning_rate`, or `eta`. For the above example, test the impact of different learning rates on the iteration situation, as shown in Table 2-5.

Table 2-5 The impact of different learning rates on iteration

|Learning rate|Iteration roadmap|Description|
|---|---|---|
|1.0|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd100.png" width="500" height="150"/>|The learning rate is too high, the iteration situation is terrible, jumping around on a horizontal line, and never going down.|
|0.8|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd080.png" width="400"/>|The learning rate is large, and this kind of left-to-right jumping occurs, which is not conducive to the training of the neural network.|
|0.4|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd040.png" width="400"/>|If the learning rate is appropriate, the loss value will drop down on one side to the ideal value within 4 steps.|
|0.1|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/2/gd010.png" width="400"/>|The learning rate is relatively small, and the loss value will drop down on one side, but the decline speed is very slow, and it has not reached the ideal state after ten steps.|


### Code location

ch02, Level3, Level4, Level5
