<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# Chapter 3.0 Loss Function

## 3.0 Introduction to Loss Function

### 3.0.1 Basic Concept
In various machine learning materials, commonly seen expressions such as Error, Bias, Cost and Loss all have very similar meanings. In this book, the expression "loss function" is used. "Loss Function" is represented by the symbol $J$, and the error value calculated by the Loss function is referred to as $loss$.\\

"Loss" is the sum of "errors" of all samples ($m$ is the number of samples).

$$J = \sum_{i=1}^m loss_i$$  

In the black box example, it is inaccurate to say "loss of a sample". We can only say "error of a sample" since samples are calculated independently. Suppose we adjust the neural network's parameters for fully satisfying the independent sample's error to $0$, the error of other examples will usually increase, leading to a larger sum of loss value. Therefore, we typically calculate the loss function as a whole by adjusting the weight according to the error of every single training example to determine whether the network has been fully trained.
#### Purpose of the Loss function

The Loss function quantifies the error between predicted values and real values by running the forward pass to update the weight in the correct direction. 

To compute the loss function:  

1.Initialize parameters of the forward pass with random values;  
2.Feed a training example through the network to compute predicted output.  
3.Utilize the loss function to compute the error between the predicted value and the label value (the real value);  
4.Compute the derivative of the loss function, propagate this error backwards through the network along the minimum gradient direction and update the forward pass's weights.  
5.Repeat steps 2-4 until the loss function value becomes satisfactory.  

### 3.0.2 Common Loss functions in machine learning

Notations: $a$ is predicted output, $y$ is label value of the example, $loss$ is loss function value.

- Gold Standard Loss，also called 0-1 Loss
$$
loss=\begin{cases}
0 & a=y \\\\
1 & a \ne y 
\end{cases}
$$

- Absolute Error function

$$
loss = |y-a|
$$

- Hinge Loss，primarily used in SVM classifiers where the target values are in the set ${-1, 1}$.

$$
loss=\max(0,1-y \cdot a) \qquad y=\pm 1
$$

- Log Loss，also called the Cross-entropy error

$$
loss = -[y \cdot \ln (a) + (1-y) \cdot \ln (1-a)]  \qquad y \in \\{ 0,1 \\} 
$$

- Squared Loss
$$
loss=(a-y)^2
$$

- Exponential Loss
$$
loss = e^{-(y \cdot a)}
$$


### 3.0.3 Understanding the Loss function image

#### Use a two-dimensional function image to understand the effect of a single variable on the loss function

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/gd2d.png" />

Fig3-1 Loss Function with a single variable

In Fig3-1, the y-axis is the loss value, and the x-axis is the variable. Changing the value of the variable will cause the loss value to rise or fall. The gradient descent algorithm leads us to move in the direction of loss function value decreasing.  
1. Suppose we are initially at point $A$, $x=x_0$, and the loss value (y coordinate) is large. It is propagated back to the network for training;
2. After one iteration, we move to point $B$ , $x=x_1$, and the loss value is reduced accordingly. We propagate it back to retrain;
3. Edging ever closer to the minimum, the loss function has experienced $x_2,x_3,x_4,x_5$;
4. When the loss value reaches an acceptable level, such as  $x_5$, stop training.

#### Understanding the effect of two variables on loss function with the contour map

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/gd3d.png" />

Fig3-2 Bivariate Loss Function Diagram

In Fig3-2，the x and y-axis each represent a variable $w$ and  $b$. The loss value formed by combining two variables corresponds to a unique coordinate point on the contour line in the diagram. Different values of $w,b$ will form a matrix of loss values. Connect the matrix points with the same (similar) loss values to form an irregular ellipse. The loss is $0$ at the center of the ellipse, which is where we want to approach.

The ellipse represents a depression in the contour plot in which the center position is lower than the edge position. Computing a derivative of the loss function leads us to gradually descend along the contour line, edging ever closer to the minimum.

### 3.0.4 Common Loss functions in neural networks

- Mean square error function, primarily used for Regression problems

- Cross entropy function, mainly used for Classification problems.

Both Loss functions are non-negative functions with extreme at the bottom, which can be solved with the gradient descent method.

