<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 3.2 Cross-Entropy Loss function

Cross-entropy is an essential concept in Shannon information theory, mainly used to measure the distance between two probability distributions. In information theory, the cross-entropy is the difference between two probability distributions: $P, Q$, where $p$ is the real distribution and $q$ is the predicted distribution. $H(p,q)$ represents the cross-entropy:

$$H(p,q)=\sum_i p_i \cdot \ln {1 \over q_i} = - \sum_i p_i \ln q_i \tag{1}$$

The cross-entropy loss function can be used as a loss function in the neural network, where $p$ represents the real label distribution, and $q$ is the prediction label distribution of the trained model. Cross-entropy measures similarity between $p$ and $q$.

**The cross-entropy function is often used for logistic regression or classification.**

### 3.2.1 The origin of cross-entropy

#### Quantity of information

In information theory, the quantity of information is represented by:

$$I(x_j) = -\ln (p(x_j)) \tag{2}$$

$x_j$：represents an event Q

$p(x_j)$：indicates the probability of occurrence od $x_j$ 

$I(x_j)$：The quantity of information. The less likely $x_j$ is to happen, the more information it will have once it happens

Suppose we have three possible scenarios for learning the principles of neural networks, as shown in table 3-2.

Table 3-2 Overview and quantity of information for three events

|Event number|Event|Probability $p$|Quantity of information $I$|
|---|---|---|---|
|$x_1$|A|$p=0.7$|$I=-\ln(0.7)=0.36$|
|$x_2$|Pass|$p=0.2$|$I=-\ln(0.2)=1.61$|
|$x_3$|Fail|$p=0.1$|$I=-\ln(0.1)=2.30$|

Wow, someone failed! That’s a lot of information! In contrast, the event where someone got an "A" contains much less information.

#### Entropy

$$H(p) = - \sum_j^n p(x_j) \ln (p(x_j)) \tag{3}$$

The entropy of the above problem is：

$$
\begin{aligned}  
H(p)&=-[p(x_1) \ln p(x_1) + p(x_2) \ln p(x_2) + p(x_3) \ln p(x_3)] \\\\
&=0.7 \times 0.36 + 0.2 \times 1.61 + 0.1 \times 2.30 \\\\
&=0.804
\end{aligned}
$$

#### Relative entropy (KL divergence)

Relative entropy is also called KL divergence. Suppose we have two different probability distributions of $P(x)$ and $Q(x)$ for the same random variable $x$, we can use KL divergence (Kullback-Leibler (KL) divergence) to measure the difference between the two distributions. This is equivalent to the mean square error of the category of information theory.

The equation for KL divergence is:

$$D_{KL}(p||q)=\sum_{j=1}^n p(x_j) \ln{p(x_j) \over q(x_j)} \tag{4}$$

$n$ is the possibility for an event. 
The $d$ value decreases as the $q$ distribution approaches the $p$ distribution

#### Cross-entropy

Transform the above equation into:

$$
\begin{aligned}  
D_{KL}(p||q)&=\sum_{j=1}^n p(x_j) \ln{p(x_j)} - \sum_{j=1}^n p(x_j) \ln q(x_j) \\\\
&=- H(p(x)) + H(p,q) 
\end{aligned}
\tag{5}
$$

The first part of the equation is the entropy of $p$, and the second part of the equation is the cross-entropy:

$$H(p,q) =- \sum_{j=1}^n p(x_j) \ln q(x_j) \tag{6}$$

In machine learning, when we need to evaluate the difference between the label value $y$ and the predicted value $a$, it is appropriate to use KL divergence，or $D (y | A) $.  Since the first part of the KL divergence $h (Y)$ remains unchanged, we only need to focus on the cross-entropy in the optimization process. Therefore, the cross-entropy is directly used as the loss function to evaluate the model in machine learning.

$$loss =- \sum_{j=1}^n y_j \ln a_j \tag{7}$$

Equation 7 is the case of a single sample, where $n$ is not the number of examples but the number of classes. Thus, the cross-entropy formula for batch samples is:

$$J =- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \ln a_{ij} \tag{8}$$

where $m$ is the number of samples and $n$ is the number of classes.

There is a special type of problem where events have only two outcomes, such as “Learned” and “Unlearned," known as the $0/1$ classification or the binary classification. For such problems, if $n=2，y_1=1-y_2，a_1=1-a_2$, the cross-entropy can be reduced to:

$$loss =-[y \ln a + (1-y) \ln (1-a)] \tag{9}$$

The binary cross-entropy of batch samples is:

$$J= - \sum_{i=1}^m [y_i \ln a_i + (1-y_i) \ln (1-a_i)] \tag{10}$$

### 3.2.2 Cross-entropy of binary classification problems

Break Equation 10 into two cases. When $y = 1$, or when the label value is $1$, which is a positive example, the plus sign is followed by an entry of $0$:

$$loss = -\ln(a) \tag{11}$$

The x-axis is the predicted output, and the y-axis is the loss value. $y = 1$ means that the current sample has a label value of 1. The training result is more accurate when the predicted output approaches 1 and the loss gets lower. The loss function value is higher when the prediction output is closer to 0, and the training result is worse.

When $y=0$, or when the label value is 0, which is a counterexample, the entry before the plus sign is 0:
$$loss = -\ln (1-a) \tag{12}$$

At this point, the loss resembles that of Figure 3-10.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/crossentropy2.png" ch="500" />

Fig3-10 Binary classification cross entropy loss diagram

Suppose the label value of the well-learned course is 1, and the label value of the unlearned course is 0. We wanted to establish a system to predict the probability of a particular student who will learn the course well based on his attendance rate, class performance, homework, learning ability and other features.

For student A, the probability of learning the course well was predicted as 0.6, while the student passed the exam with an actual value of 1. Thus, the cross-entropy loss function for Student A is:

$$
loss_1 = -(1 \times \ln 0.6 + (1-1) \times \ln (1-0.6)) = 0.51
$$

For student B, the probability of learning the course well was predicted as 0.7, while the student also passed the exam. So, the cross-entropy loss function for student B is:

$$
loss_2 = -(1 \times \ln 0.7 + (1-1) \times \ln (1-0.7)) = 0.36
$$

Since 0.7 is closer to 1 and the prediction is relatively accurate, $loss2$ is less than $loss1 $, and its strength is lower in backpropagation.

### 3.2.3 Cross-entropy of multi-classification problems

When the label value is not 0 or 1, it is a multi-classification problem. Suppose there are three outcomes for the final exam:

1. A，OneHot encoding of the label is $[1,0,0]$；
2. Pass，OneHot encoding of the label is  $[0,1,0]$；
3. Fail，OneHot encoding of the label is  $[0,0,1]$。

Suppose we predict the probability that student C will get an "A", a "Pass", and a "Fail" is $[0.2,0.5,0.3]$, while the real situation is that the student fails, then the cross-entropy is:

$$
loss_1 = -(0 \times \ln 0.2 + 0 \times \ln 0.5 + 1 \times \ln 0.3) = 1.2
$$

Suppose we predict the probability that student D will get an A, pass, or fail: $[0.2,0.2,0.6] $, and the actual situation is that student D fails, then the cross-entropy is:

$$
loss_2 = -(0 \times \ln 0.2 + 0 \times \ln 0.2 + 1 \times \ln 0.6) = 0.51
$$

The loss2 of 0.51 is much lower than 1.2, which indicates that its predicted value is closer to the actual label value (0.6 vs 0.3). The smaller the difference between the two values, the smaller the value of cross-entropy and the easier the backpropagation process.

### 3.2.4 Why can’t we use mean square error as the loss function for classification problems?

1. The regression problem usually uses the mean square error function to ensures the loss function is convex, so that the optimal solution can be obtained. 
However, it is difficult to get the optimal solution if the loss function is not convex when the mean square error is used. The cross-entropy function can guarantee the monotonicity in the interval.

2. The last layer network of classification problems requires a classification function such as Sigmoid or Softmax. If the mean square error function is connected, the derivation result is complex and requires massive calculations.
A straightforward calculation can be obtained using the cross-entropy function, and a simple subtraction can be used to obtain the reverse error.