<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 6.1 Binary classification function

This function is suitable for both linear and non-linear binary classifications.

### 6.1.1 Binary classification function

The logistic function can be used as an activation function or as a binary classification function. In many informal texts, these two concepts are mixed up, such as the following statement: "We use the Sigmoid activation function at the end to do two classifications," which is inappropriate. This book will distinguish between the activation function and the classification function according to their different applications. In the binary classification application, this is called the Logistic function, and when used as an activation function, it is called the Sigmoid function.

- Logistic function equation

$$Logistic(z) = \frac{1}{1 + e^{-z}}$$

denoted as $a=Logistic(z)$。

- Derivative

$$Logistic'(z) = a(1 - a)$$

The specific derivation process can be found in section 8.1.

- Input range

$$(-\infty, \infty)$$

- Output Range 

$$(0,1)$$

- Function plot（Figure6-2）

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/8/logistic.png" ch="500" />

Figure6-2 Logistic function plot

- How to use

This function is simply a measure of probability. It compresses every number between $(-\infty, \infty)$ to $(0,1)$, and returns a probability value. When said probability value is close to $1$, it is regarded as a positive example; otherwise, it is considered a negative example.

During training, a sample $x$ is used as input $z$ after matrix operation through the last layer of the neural network. A prediction value between $(0,1)$ is output after Logistic calculation. We assume that the label value of this sample $0$ belongs to the negative category. If the predicted value is closer to $0$, it approaches the label value, representing a smaller error and a lower intensity of the backpropagation.

In the process of inference, we preset a threshold such as $0.5$. The result is considered positive if it is greater than $0.5$, negative if the output is less than $0.5$. When it is equal to $0.5$, classify as positive or negative according to the specific situation. The threshold is not necessarily $0.5$; it can also be $0.65$ or other values. The larger the threshold is, the higher the accuracy and the lower the recall rate is; In contrast, a smaller threshold will cause lower accuracy and increase the recall rate.

For example:

- when input=2，output=0.88，0.88>0.5，considered as a positive case
- when input=-1，output=0.27，0.27<0.5，considered as a negative case

### 6.1.2 Feed-forward propagation

#### Matrix operations

$$
z=x \cdot w + b \tag{1}
$$

#### Classification calculation

$$
a = Logistic(z)=\frac{1}{1 + e^{-z}} \tag{2}
$$

#### Loss function calculation

Binary classification Cross-entropy loss function:

$$
loss(w,b) = -[y \ln a+(1-y) \ln(1-a)] \tag{3}
$$

### 6.1.3 Back Propagation

#### Calculate the partial derivatives of the loss function with respect to $a$

$$
\frac{\partial loss}{\partial a}=-\left[\frac{y}{a}-\frac{1-y}{1-a}\right]=\frac{a-y}{a(1-a)} \tag{4}
$$

#### Calculate the partial derivatives of $a$ with respect to $z$

$$
\frac{\partial a}{\partial z}= a(1-a) \tag{5}
$$

#### Calculate the partial derivatives of $loss$ with respect to $z$

apply Chain Rule to Eq4 and Eq5:

$$
\frac{\partial loss}{\partial z}=\frac{\partial loss}{\partial a}\frac{\partial a}{\partial z}=\frac{a-y}{a(1-a)} \cdot a(1-a)=a-y \tag{6}
$$

We are surprised to discover that the denominator obtained by the derivatives of the cross-entropy function cancel out the derivatives of the Logistic classification function, leaving only the term $a-y $. Is this a coincidence? This actually relies on the ingenuity of mathematicians who found this matching relationship, which fulfills the following conditions:

1. The loss function meets the requirements of binary classification; both positive and negative examples are monotonous
2. The loss function is differentiable to facilitate the use of backpropagation algorithm;
3. The computational procedure is straightforward, can be done with a simple subtraction.

#### Multi-sample cases

We represent the deduction with three samples:

$$Z=
\begin{pmatrix}
  z_1 \\\\ z_2 \\\\ z_3
\end{pmatrix},
A=Logistic\begin{pmatrix}
  z_1 \\\\ z_2 \\\\ z_3
\end{pmatrix}=
\begin{pmatrix}
  a_1 \\\\ a_2 \\\\ a_3
\end{pmatrix}
$$

$$
\begin{aligned}
J(w,b)=&-[y_1 \ln a_1+(1-y_1)\ln(1-a_1)]  \\\\
&-[y_2 \ln a_2+(1-y_2)\ln(1-a_2)]  \\\\
&-[y_3 \ln a_3+(1-y_3)\ln(1-a_3)]  \\\\
\end{aligned}
$$
Apply the result of Formula 6: 
$$ 
\frac{\partial J(w,b)}{\partial Z}=
\begin{pmatrix}
  \frac{\partial J(w,b)}{\partial z_1} \\\\
  \frac{\partial J(w,b)}{\partial z_2} \\\\
  \frac{\partial J(w,b)}{\partial z_3}
\end{pmatrix}
=\begin{pmatrix}
  a_1-y_1 \\\\
  a_2-y_2 \\\\
  a_3-y_3 
\end{pmatrix}=A-Y
$$

Therefore, the matrix operation can be simplified to the form of matrix subtraction：$A-Y$.

### 6.1.4 The origin of the log odds

After mathematical deduction, it is clear that the neural network actually does the same thing: after adjusting the values of $w$ and $b$, all samples of positive cases are grouped into a range greater than $0.5$, and all negative cases are less than $0.5$. However, if we just say it is greater than or less than, we cannot do an accurate quantitative calculation, so we use a log-odds function to simulate it.

One more question about the logistic function is: why is it called the "log-odds" probability function?

Let's take an example: Suppose a coin is tossed and the probability of getting heads or tails is each $0.5$. If you divide the probability of heads by the probability of tails, $0.5/0.5=1$, this value is called $odds$, which is the probability.

To generalize, if the probability of getting heads is $a$, then the probability of getting tails is $1-a$, and the probability is equal to:

$$odds = \frac{a}{1-a} \tag{9}$$

In the above formula, if $a$ is the probability that predicts sample $x$ as a positive, then $1-a$ is the probability of its negative example. Accordingly,  $\frac{a}{1-a}$ is the ratio of positive to negative, called "odds," which reflects the relative probability of $x$ as a positive case. Computing the logarithm of the odds is called log-odds (logit).

Suppose the probabilities are as shown in table 6-3.

Table 6-3 A comparison table of probability and log-odds

|Probability$a$|0|0.1|0.2|0.3|0.4|0.5|0.6|0.7|0.8|0.9|1|
|--|--|--|--|--|--|--|--|--|--|--|--|
|Inverse probability$1-a$|1|0.9|0.8|0.7|0.6|0.5|0.4|0.3|0.2|0.1|0|
|Odds $odds$ |0|0.11|0.25|0.43|0.67|1|1.5|2.33|4|9|$\infty$|
|Log-odds $\ln(odds)$|N/A|-2.19|-1.38|-0.84|-0.4|0|0.4|0.84|1.38|2.19|N/A|

It can be seen that the value of probability is not linear, which is not conducive to analyzing the problem. Therefore, we take the logarithm of the odds in the fourth row of the table to obtain a value that constitutes a linear relationship, which can be represented by a linear equation, namely:
$$
\ln(odds) = \ln \frac{a}{1-a}= xw + b \tag{10}
$$

Take the natural exponent for both sides of Equation 10:

$$
\frac{a}{1-a}=e^{xw+b} \tag{11}
$$

$$
a=\frac{1}{1+e^{-(xw+b)}}
$$

Let $z=xw+b$：

$$
a=\frac{1}{1+e^{-z}} \tag{12}
$$

Formula 12 is Formula 2! The function of log-odds can be considered to be obtained in this way.

The above deduction process actually uses the prediction result of the linear regression model to approximate the log-odds of the sample classification. This is why it is called logistic regression, but it is actually a classification learning method. The advantages of this method are as follows:

- It introduces the successful experiences of linear regression into the classification problem. It is equivalent to modelling the "boundary line" prediction, which is a straight line in two-dimensional space, without considering the distribution of specific samples (e.g., coordinates in two-dimensional space). This avoids the problem of inaccurate assumptions about the distribution;
- Not only does it predicts the category (0/1) but also yields an approximate probability value (such as 0.31 or 0.86), which is helpful for many tasks that require probability to assist decision-making;
- The log-odds function is a derivable convex function of any order. It has good mathematical properties and can be used directly by many numerical optimization algorithms to obtain the optimal solution.