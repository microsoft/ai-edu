<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 6.3 Principle of Linear Binary Classification

### 6.3.1 Similarities and differences between linear classification and linear regression

This principle is applicable to both linear and non-linear binary classification.

Recall the linear regression that we learned earlier，adjusting the angles (weights) and bias of the fitting line through back-propagate MSE loss. The mean square error function can accurately reflect the current fitting degree. Can we take a similar approach in linear classification?

Linear classification attempts to draw a dividing line in a space containing two kinds of samples so that the two sides are completely separated, just like the Chu and Han boundary on the Chinese chess chessboard. The similarity with linear regression is that both need to draw a "straight line." However, the difference is also apparent, as shown in Table 6-4.

Table 6-4 Comparison of linear regression and linear classification

| |Linear Regression|Linear Classification|
|---|---|---|
|Similarity|Need to find a straight line in the sample group|Need to find a straight line in the sample group|
|Difference|Fit all samples with a straight line so that the distance from each example to this boundary is as short as possible|Use a straight line to divide all samples so that the positive and negative examples are distributed on both sides of the straight line as much as possible|

It can be seen that the goal in linear regression-"the shortest distance" is still easy to understand, but how can we mathematically describe the purpose of linear classification--"distributed on both sides"? We can describe it with algebra and geometry .

### 6.3.2 Algebraic principle of binary classification

Algebraic method: Calculate the probability value of all sample points after linear transformation through a classification function. The probability of positive samples is greater than 0.5, and the probability of negative examples is less than 0.5.

#### Review of basic formulas

The following example uses a single sample with two features to illustrate the binary classification process of neural networks. This is an algebraic approach to explain how it works.

1. Feed-forward calculation

$$
z = x_1 w_1+ x_2 w_2 + b  \tag{1}
$$

2. Classification calculation

$$
a={1 \over 1 + e^{-z}} \tag{2}
$$

3. Loss function calculation

$$
loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3}
$$

Figure 6-5 illustrates the calculation process.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/sample.png" ch="500" />

Figure 6-5 The incorrect decision boundary attempts to classify the red and green sample points

There are three points on the plane, divided into two categories: the green square for the positive class and the red triangle for the negative class. The coordinates of the points are: $A (2,4) , B (2,1) , C (3,3)$.

#### When the decision boundary is $L_1$ 

Suppose the neural network first uses $L_1$ as the decision boundary. At this time: $w_1=-1,w_2=2,b=-2$, let's calculate these three points.

Point $A$：

$$
z_A = (-1)\times 2 + 2 \times 4 -2 = 4 > 0 \tag{Correct}
$$

Point $B$：

$$
z_B = (-1)\times 2 + 2 \times 1 -2 = -2 < 0 \tag{Correct}
$$

Point $C$：

$$
z_C = (-1)\times 3 + 2 \times 3 -2 = 1 > 0 \tag{Incorrect}
$$

From Section 6.1, we know that when $z>0$, $Logistic(z)> 0.5$ is a positive example; otherwise, it is a negative example. Therefore we only need to see whether the $z$ value of the three points is greater than 0 or less than 0. There is no need to calculate the value of the $Logistic$ function.

The $A, B$ points are in the correct category, and the $c$ points are in the wrong category. The loss function value for the $c$ point is (note the $c$ label value of $y = 0 $):

$$
a_C = Logistic(z_C) = 0.731
$$

$$
loss_Z = -(0 \cdot \ln(0.731) + 1 \cdot \ln(1-0.731))=1.313
$$

Readers may have no idea whether $1.313$ is big or small. Let’s calculate the coordinates of the points of $A$ and $B$ that are correctly classified:

$$
loss_A = 0.018, \quad loss_B = 0.112
$$

For the correctly classified $A, B$ points, the loss function value is much smaller than the $C$ point, so the backpropagation of the $C$ point is more significant. The comparison is summarized in Table 6-5.

Table 6-5 Compare the computed values of three points in each process

|Point|Coordinate|$z$ value|$a$ value|$y$ value|$loss$ value|Classification Result|
|---|---|---|---|---|---|---|
|A|(2,4)|4|0.982|1|0.018|Correct|
|B|(2,1)|-2|0.119|0|0.112|Correct|
|C|(3,3)|1|0.731|0|1.313|Incorrect|

- In the positive case $y=1$, if $a$ is closer to $1$, it indicates that the classification approaches the correct result, and the loss value will be smaller at this time. This is the case at point $A$ : $a=0.982$, not far from $1$; $loss$ value $0.018$, which is very small;
- In the negative case $y=0$, $a$ is closer to $0$ indicates that the classification approaches the correct result, and the loss value will be smaller at this time. This is the case at point $B$: $a=0.119$, not far from $0$; $loss$ value $0.112$,which is fairly small;
- Point $C$ is a misclassification situation, $a=0.731$, which should be less than $0.5$, but in fact, it is far away from $0$, but close to $1$. The $loss=1.313$ is different from the other two points: The relative value is large. Accordingly, the error is high, and the backpropagation strength is also more significant.

#### When the decision boundary is $L_2$ 

We assume that after backpropagation, the neural network adjusts the position of the straight line to $L_2$, using $L_2$ as the decision boundary, that is, $w_1=-1, w_2=1, b=-1$, then the $z$ value of three points will be consistent with its classification:

$$
z_A = (-1)\times 2 + 1 \times 4 -1 = 1 > 0 \tag{Correct}
$$

$$
z_B = (-1)\times 2 + 1 \times 1 -1 = -2 < 0 \tag{Correct}
$$

$$
z_C = (-1)\times 3 + 1 \times 3 -1 = -1 < 0 \tag{Correct}
$$

A question may arise: Since we can resolve whether the classification is correct or not by determining if the value of $z$ is greater than 0, why should the $Logistic$ function be used for classification in the binary classification theory?

Since we can only know whether it is greater or less than 0 just by looking at $z$ values,  we cannot effectively perform backpropagation. That is to say, we cannot tell the neural network how strong the error of backpropagation is. For example, is $z=5$ 5 times stronger compared with $z=-1$?

With the $Logistic$ classification,  the value obtained is a probability between the range $(0,1)$. For example:  $Logistic(5) = 0.993$ when $z=5$, and $Logistic(-1)=0.269$ when $z= At -1$. The meaning of these two values is the probability that the two samples are within the classification area. The former probability is $99.3%$, which is relatively close to the positive case, and the latter probability is $26.9%$, which is closer to the negative case. Then the loss function is calculated to get the backpropagation error that the neural network can understand, such as the $loss_A,loss_B,loss_C$ calculated above.

### 6.3.3 The geometric principle of binary classification

Geometric method: Let all positive samples be on one side of the straight line and all negative ones on the other side. The straight line should be in the middle of the two types of examples as much as possible.

#### The geometric role of the binary classification function

The final result of the binary classification function is to map the positive examples to the upper half of the curve in Figure 6-6, and all the negative examples to the lower half of the curve.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/sigmoid_binary.png"/>


Figure 6-6 The $Logistic$ function maps the input points to the $(0,1)$ interval to achieve classification

We use a positive case as an example:

$$a = Logistic(z) = \frac{1}{1 + e^{-z}} > 0.5$$

By transforming the formula and taking natural logarithms on both sides, we can get:

$$z > 0$$

which is：
$$
z = x_1 \cdot w_1 + x_2 \cdot w_2 + b > 0
$$

Transform the above formula, put $x_2$ on the left and other items on the right (assuming $w_2>0$, the direction of the inequality sign remains the same):

$$
x_2 > - \frac{w_1}{w_2}x_1 - \frac{b}{w_2} \tag{5}
$$

Simplify the two coefficients, let $w'=-w1/w2,b'=-b/w2$：

$$
x_2 > w' \cdot x_1 + b' \tag{6}
$$

The geometric explanation for Formula 6 is: there is a straight line whose equation is $z = w'\cdot x_1+b'$. All positive samples are above this straight line, and all negative samples are below this straight line.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/linear_binary_analysis.png" ch="500" />

Figure 6-7 Two types of samples separated by a straight line

Let's take another look at the correctly classified picture as shown in Figure 6-7. Assume that the green square represents the positive category, in which the label value is $y=1$; the red triangle is the negative category, in which the label value is $y=0$. Geometrically, if we have a straight line with the formula $z = w'\cdot x_1+b'$, as shown by the dotted line $L_1$ in the figure, then the $x_2$ of all positive class samples are greater than $z$, and $x_2$ of all negative samples are less than $z$. This straight line is the dividing line we are looking for.

This shows that the neural network works on the same principle as our intuitive sense of the two-dimensional plane. In other words, the job of the neural network is to find such a suitable straight line which lets as many positive samples above the straight line and as many negative samples below the straight line as possible. In fact, this is similar to the process of finding a straight line through all sample points in linear regression.

We additionally benefit from this method, because:

$$w' = - w1 / w2 \tag{7}$$

$$b' = -b/w2 \tag{8}$$

We can use the neural network to calculate the three values of $w_1, w_2, b$, and convert them into $w',b'$, so as to draw the dividing line on the two-dimensional plane to  intuitively judge the correctness of the neural network training results.

### Thinking Exercise

1. When deducing formula 5, we assumed that $w_2> 0$. Are there problems that could arise because of this assumption? What if we assumed that $w_2<0$?