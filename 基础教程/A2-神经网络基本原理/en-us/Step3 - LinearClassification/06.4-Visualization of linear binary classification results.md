<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 6.4 Binary classification result visualization

### 6.4.1 The importance of visualization

We have the calculation results, but at first glance, they are all just mysterious numbers. How do we know if they are correct or wrong?

As we will discuss later, in actual engineering practice, we generally divide the samples into the training set, validation set, and test set, then use the test set to test the correctness of the training results. We did not do this in this example for two reasons:

1. The sample size is relatively small, with only 200 samples in total, if further divided into two parts, it will cause incomplete coverage of the dataset, and there will be huge differences in the sets, which is not helpful for training, validation or testing.
2.  Since the data features of this example are relatively small, the better method is visualization. In the early stages of studying neural networks, visualization of the training process and results will also be of great help to the reader.

The key to visualizing neural networks is understanding the framework system, the operating mechanism, and the working principle. Once these are mastered, visualization will be easy and enjoyable.

### 6.4.2 Meaning of the weight value

The training results in Section 6.2 are as follows. How are these numbers interpreted?

```
W= [[-7.66469954]
 [ 3.15772116]]
B= [[2.19442993]]
A= [[0.65791301]
 [0.30556477]
 [0.53019727]]
``````

In Section 6.1, we learned the principle of linear binary classification. Wouldn't it be intuitive to draw a straight line on the graph to split the two regions of positive and negative examples based on the training results?

$$
z = x_{1} \cdot w_1 + x_{2} \cdot w_2 + b \tag{1}
$$
$$
a=Logistic(z) \tag{2}
$$

For equation 2, when $a>0.5$, it is a positive case (belongs to Han), and when $a<0.5$, it is a negative case (belongs to Chu). Then when $a=0.5$, it is the Chu-Han boundary! In fact, we have
$$a = 0.5 \Leftrightarrow z=0$$
$$z = x_{1} \cdot w_1 + x_{2} \cdot w_2 + b = 0$$

By leaving $x_2$ on the left side of the equation and moving the rest to the right side, we can obtain the equation of a line:

$$x_{2} \cdot w_2 = -x_{1} \cdot w_1 - b$$
$$x_2 = -\frac{w_1}{w_2}x_1 - \frac{b}{w_2} \tag{3}$$

Well, this is the form of the standard linear equation $y=ax+b$. This equation is equivalent to Equations 7 and 8 in the principle of binary classification.

### 6.4.3 Code Implementation

Equation 3 is implemented in `Python` code as follows：

```Python
def draw_split_line(net):
    b12 = -net.B[0,0]/net.W[1,0]
    w12 = -net.W[0,0]/net.W[1,0]
    print(w12,b12)
    x = np.linspace(0,1,10)
    y = w12 * x + b12
    plt.plot(x,y)
    plt.axis([-0.1,1.1,-0.1,1.1])
    plt.show()
```
The `w12`, `b12` in the above code is based on Equation 3, except that our definition of $W$ is $(w_1, w_2)$ and `Python` is "zero-based", so：
$w_1 = W[0,0],w_2 = W[0,1],b = B[0,0]$。


The sample data also needs to be shown in order to determine how well the split line and the sample data match:

```Python
def draw_source_data(net, dataReader):
    fig = plt.figure(figsize=(6.5,6.5))
    X,Y = dataReader.GetWholeTrainSamples()
    for i in range(200):
        if Y[i,0] == 1:
            plt.scatter(X[i,0], X[i,1], marker='x', c='g')
        else:
            plt.scatter(X[i,0], X[i,1], marker='o', c='r')
        #end if
    #end for
```

Finally, we can also show the positions of the three predicted points to see if they are correct:

```Python
def draw_predicate_data(net):
    x = np.array([0.58,0.92,0.62,0.55,0.39,0.29]).reshape(3,2)
    a = net.inference(x)
    print("A=", a)
    for i in range(3):
        if a[i,0] > 0.5:
            plt.scatter(x[i,0], x[i,1], marker='^', c='g', s=100)
        else:
            plt.scatter(x[i,0], x[i,1], marker='^', c='r', s=100)
        #end if
    #end for
```
Main program：

```Python
# Main program
if __name__ == '__main__':
    ......
    # show result
    draw_source_data(net, reader)
    draw_predicate_data(net)
    draw_split_line(net)
    plt.show()
```

### 6.4.4 Compiling result

Figures 6-8 show the binary classification results.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/binary_result.png" ch="500" />

Figure 6-8 Slightly deficient binary classification results

Although the blue dividing line roughly separates Chu and Han, careful readers will see that at the top and bottom, it is still the case that the green dots are on the right side of the dividing line while the red dots are on the left side of the dividing line. This indicates that the training accuracy of our neural network is insufficient. Therefore, we should adjust the hyperparameters and train again:

```Python
params = HyperParameters(eta=0.1, max_epoch=10000, batch_size=10, eps=1e-3, net_type=NetType.BinaryClassifier)
```
Changed `max_epoch` from `100` to `10000` and re-ran it.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/binary_loss_10k.png" ch="500" />

Figure 6-9 Variation of loss function values during training

Looking at the curves in Figure 6-9, the loss function values have decreased, indicating that the network is continuously converging. Looking at the position of the straight line in Figure 6-10 again, it has separated the red and green regions more perfectly.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/binary_result_10k.png" ch="500" />

Figure 6-10 The result of a relatively perfect binary classification

The three triangular points are the three coordinates for solving the problem, where the third triangular point is near the dividing line, where it is harder to distinguish with the naked eye on which side it sits. Check the printout:

```
W= [[-42.62417571]
 [ 21.36558218]]
B= [[10.5773054]]
A= [[0.99597669]
 [0.01632475]
 [0.53740392]]
w12= 1.994992477013739
b12= -0.49506282174794675
```

The probabilities of the first two points are 0.995 and 0.016, which can be clearly distinguished as positive or negative cases. The third point is 0.537, which is greater than 0.5 and can be counted as a positive case.

In the `matplot`, we can also zoom in for a localized view, as shown in Figure 6-11.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/binary_result_10k_zoom.png" ch="500" />

Figure 6-11 Detail after zooming in, the green dot is indeed on the left side of the line and is correctly classified


The third point is located in the positive example area on the left side.

Well, we have confidently found the principles that explain how neural networks work, with numerical calculations verification, formula derivation, and graphical display, so it seems that we have tied a neat bow on this topic. But is this actually the case? Is there a more profound principle that has not been touched on yet? For the time being, we will leave this question for the next chapters.

### Code Location

ch06, Level2
