<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 6.6 Binary classification function with hyperbolic tangent function

Through a series of modifications to the source code, we can finally unlock the purpose of using the hyperbolic tangent function as the classification function. Although this process is not strictly required, we can deepen our understanding of the basic concepts of classification function, loss function, backpropagation, etc., by practicing it.

### 6.6.1 Raising Questions

In binary classification problems, the logistic function (often called Sigmoid Function) is generally used as the classification function, together with the binary cross-entropy loss function.

$$a_i=Logistic(z_i) = \frac{1}{1 + e^{-z_i}} \tag{1}$$

$$loss_i(w,b)=-[y_i \ln a_i + (1-y_i) \ln (1-a_i)] \tag{2}$$

There is also a function that looks very similar to the logistic function--the hyperbolic tangent function (Tanh Function)--with the following equation:

$$Tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} = \frac{2}{1 + e^{-2z}} - 1 \tag{3}$$

**The question is: Can the hyperbolic tangent function be used as a classification function?**

Compare the images of the two functions, as shown in Table 6-9.

Table 6-9 Comparison of the logistic function and Tanh function

|Logistic function|Tanh function|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/logistic_seperator.png">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/tanh_seperator.png">|
|Positive-negative boundary:$a=0.5$|Positive-negative boundary:$a=0$|

In the logistic function, $0.5$ is generally used as the boundary between positive and negative classes. It is natural to think that $0$ can be used to separate the positive and negative classes in the Tanh function.

The term dividing line is actually just a way for people to understand how neural networks practice binary classification. For neural networks, there is actually no such concept as a dividing line. All it does is push the positive examples upwards and the negative examples downwards as much as possible through a linear transformation.

### 6.6.2 Modify the feed-forward calculation and backpropagation functions

Let's get down to business now. The first step is to replace the logistic function with a Tanh function and modify the feed-forward calculation while remembering to revise the code for backpropagation.

#### Add the Tanh function

```Python
def Tanh(z):
    a = 2.0 / (1.0 + np.exp(-2*z)) - 1.0
    return a
```

#### Modify the feed-forward calculation function

An important principle in software development is the open and closed principle: open to extension but closed for modification. The primary purpose of this is to prevent introducing bugs. In order not to modify the existing code of the  `NeuralNet` class, we derive a subclass and add new functions to the subclass to override the functions of the parent class, and still use the logic of the parent class for the rest of the code; for this example, use the logic of the subclass.

```Python
class TanhNeuralNet(NeuralNet):
    def forwardBatch(self, batch_x):
        Z = np.dot(batch_x, self.W) + self.B
        if self.params.net_type == NetType.BinaryClassifier:
            A = Sigmoid().forward(Z)
            return A
        elif self.params.net_type == NetType.BinaryTanh:
            A = Tanh().forward(Z)
            return A
        else:
            return Z

```

The new function in the subclass overwrites the previous feed-forward calculation function and calls the Tanh function by determining the `NetType` parameter. Accordingly, an enumerated value is added to the network type: `BinaryTanh`, meaning that Tanh is used for binary classification.

```Python
class NetType(Enum):
    Fitting = 1,
    BinaryClassifier = 2,
    MultipleClassifier = 3,
    BinaryTanh = 4,
```

#### Modify the backpropagation function

The feed-forward calculation is easy to modify, but we need to deduce the backpropagation formula ourselves!

For the derivation of the cross-entropy function of Eq. 2, we write the derivation process by using a single sample approach for convenience:

$$
\frac{\partial{loss_i}}{\partial{a_i}}= \frac{a_i-y_i}{a_i(1-a_i)} \tag{4}
$$

Usually, the loss function is used to derive the logistic function, but now we need to use the Tanh function as the classification function, and so we derive the Tanh function for Equation 3 as follows:
$$
\frac{\partial{a_i}}{\partial{z_i}}=(1-a_i)(1+a_i) \tag{5}
$$

Use the chain rule combined with Eq.4, 5:

$$
\begin{aligned}    
\frac{\partial loss_i}{\partial z_i}&=\frac{\partial loss_i}{\partial a_i} \frac{\partial a_i}{\partial z_i} \\\\
&= \frac{a_i-y_i}{a_i(1-a_i)} (1+a_i)(1-a_i) \\\\
&= \frac{(a_i-y_i)(1+a_i)}{a_i}
\end{aligned} \tag{6}
$$

The implementation of the backpropagation in the subclass `TanhNeuralNet` is to write a new `backwardBatch` method to override the parent class function:

```Python
class TanhNeuralNet(NeuralNet):
    def backwardBatch(self, batch_x, batch_y, batch_a):
        m = batch_x.shape[0]
        dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
        dB = dZ.sum(axis=0, keepdims=True)/m
        dW = np.dot(batch_x.T, dZ)/m
        return dW, dB
```

This implementation uses the results of Equation 6. After carefully deducing the formula again and confirming that it is correct, we can try to run the program:

```
epoch=0
Level4_TanhAsBinaryClassifier.py:29: RuntimeWarning: divide by zero encountered in true_divide
  dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
Level4_TanhAsBinaryClassifier.py:29: RuntimeWarning: invalid value encountered in true_divide
  dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
0 1 nan
0 3 nan
0 5 nan
......
```

Unsurprisingly, there is an error! The first error should be that the divisor is 0, which means that the value of `batch_a` is 0. Why hasn't such an exception been thrown when using the pair rate function? There are two reasons:

1. with the logistic function, the output value range is $(0,1)$, so the value of a will always be greater than 0; it cannot be 0. Whereas the output value range of the Tanh function is $(-1,1)$, it is possible to be 0.
2. the previous error term, dZ = batch_a - batch_y, does not have a division term.

We cannot solve the first reason since that is a characteristic inherent to the function. The derivative of the Tanh function is a fixed form (1+A)(1-A) and cannot be modified. If it is modified, it is not a Tanh function anymore.

Then, let's consider the second reason, can we remove `batch_a` from dZ? That is, let the derivative of the cross-entropy function contain a (1-a)(1+a) term in the denominator, thereby allowing the derivatives of the Tanh function to cancel each other out? We modify the cross-entropy function according to this concept, with a simplified way to facilitate the derivation.

### 6.6.3 Modify the loss function

The original formula of cross-entropy loss function is:

$$loss_i=-[y_i \ln a_i + (1-y_i) \ln (1-a_i)]$$

Change it to：

$$loss_i=-[(1+y_i) \ln (1+a_i) + (1-y_i) \ln (1-a_i)] \tag{7}$$

Calculate the derivative of Equation 7:

$$
\frac{\partial loss}{\partial a_i} = \frac{2(a_i-y_i)}{(1+a_i)(1-a_i)} \tag{8}
$$


Combining the derivatives of Tanh from Equation 5:

$$
\begin{aligned}
\frac{\partial loss_i}{\partial z_i}&=\frac{\partial loss_i}{\partial a_i}\frac{\partial a_i}{\partial z_i} \\\\
&=\frac{2(a_i-y_i)}{(1+a_i)(1-a_i)} (1+a_i)(1-a_i) \\\\
&=2(a_i-y_i) 
\end{aligned}
\tag{9}
$$

Well, we've successfully eliminated the denominator! Now we need to modify both the loss function and the backpropagation function.

#### Add the new loss function

```Python
class LossFunction(object):
    def CE2_tanh(self, A, Y, count):
        p = (1-Y) * np.log(1-A) + (1+Y) * np.log(1+A)
        LOSS = np.sum(-p)
        loss = LOSS / count
        return loss
```
In the original `LossFunction` class, a new loss function called `CE2_tanh` has been added, implemented exactly as in Equation 7.

#### Modify the backpropagation function

```Python
class NeuralNet(object):
    def backwardBatch(self, batch_x, batch_y, batch_a):
        m = batch_x.shape[0]
        # setp 1 - use original cross-entropy function
#        dZ = (batch_a - batch_y) * (1 + batch_a) / batch_a
        # step 2 - modify cross-entropy function
        dZ = 2 * (batch_a - batch_y)
        ......
```
Note that we commented out the code for step1, and using the result from equation 9, we replaced the code for step2.

In the second run, the result only runs for one round and then it stops. Looking at the printout and the loss function value, we see that the loss function is actually a negative number!

```
epoch=0
0 1 -0.1882585728753378
W= [[0.04680528]
 [0.10793676]]
B= [[0.16576018]]
A= [[0.28416676]
 [0.24881074]
 [0.21204905]]
w12= -0.4336361115243373
b12= -1.5357156668786782
```

If we ignore that the loss function is negative and force the training to proceed, we can see the loss function in Figure 6-17.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/tanh_loss_2.png" ch="500" />

Figure 6-17 Variation of loss function values during training

Superficially the loss value keeps decreasing as if it is converging, but it is negative throughout. From the original definition of the cross-entropy loss function, its value should always be greater than 0. Could it be that the loss function form was changed so as to obtain a negative value? Let's compare Equation 2 and Equation 7 again:

$$loss_i=-[y_i \ln(a_i)+(1-y_i) \ln (1-a_i)] \tag{2}$$

Since the output a is calculated using the logistic function, the value of a is always between $(0,1)$, then 1-a is also between $(0,1)$, so $\ln(a_i)$ and $\ln(1-a_i)$ are both negative. The value of y is 0 or 1, and the sum of the two items is also negative. The final result of loss is a positive number since there is a negative sign at the front.

After changing to 1+a:

$$loss_i=-[(1+y_i) \ln (1+a_i) + (1-y_i) \ln (1-a_i)] \tag{7}$$

The Tanh function outputs the value $a$ as $(-1,1)$ such that $1+a \in (0,2)$ and $1-a \in (0,2)$. When in the (1,2) interval, the values of $ln(1+a)$ and $ln(1-a)$ are greater than 0, which eventually leads to a negative loss.  If we still use the cross-entropy function, it must conform to its original design concept of having both $1+a$ and $1-a$ in the $(0,1)$ domain.

### 6.6.4 Re-modify the code of loss function 

Since both $1+a$ and $1-a$ are in the $(0,2)$ interval, we divide both of them by 2 to make the $(0,1)$ interval.

$$loss_i=-[(1+y_i) \ln (\frac{1+a_i}{2})+(1-y_i) \ln (\frac{1-a_i}{2})] \tag{9}$$

Although there is a 2 in the denominator, it has no effect on the derivative formula, and the final result remains in the form of Equation 8:

$$\frac{\partial loss_i}{\partial z_i} =2(a_i-y_i) \tag{8}$$

```Python
class LossFunction(object):
    def CE2_tanh(self, A, Y, count):
        #p = (1-Y) * np.log(1-A) + (1+Y) * np.log(1+A)
        p = (1-Y) * np.log((1-A)/2) + (1+Y) * np.log((1+A)/2)
        ......
```

Note that we commented out the code from the previous run, added the code with a denominator of 2, and implemented it exactly as in Equation 9.

On the third run, it finally works, and we get the results shown in Figure 6-18 and Figure 6-19.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/tanh_loss_3.png">

Figure 6-18 Variation of loss function values during training

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/tanh_result_3.png">

Figure 6-19 The effect of classification with deviation

This time, the loss function value curve is perfect, and the value domain is correct and converged. But when we look at classification results in Figure 6-19, why is the dividing line shifted to the right as a whole?

This shift reminds us of comparing the logistic function and the Tanh function at the beginning of this section. The output of Tanh is between $(-1,1)$ and the output of logistic is between $(0,1)$, which is equivalent to stretching $(0,1)$ to $(-1,1)$. Does this have anything to do with the offset of the classification result?

### 6.6.5 Modify sample data label values

The original data's label value is either 0 or 1, indicating positive and negative classes, which is consistent with the output value domain of the logistic function. The Tanh requires the labels of positive and negative classes to be -1 and 1, so we need to alter the label values.

Derive a subclass `SimpleDataReader_tanh` on the `SimpleDataReader` class and add a `ToZeroOne()` method, the purpose is to change the original [0/1] label into a [-1/1] label.

```Python
class SimpleDataReader_tanh(SimpleDataReader):
    def ToZeroOne(self):
        Y = np.zeros((self.num_train, 1))
        for i in range(self.num_train):
            if self.YTrain[i,0] == 0:     # The label of the first category is set to 0
                Y[i,0] = -1
            elif self.YTrain[i,0] == 1:   # The label of the second category is set to 1
                Y[i,0] = 1
        ......
```

Also, don't forget to turn 0.5 in the prediction function into 0 since the positive and negative boundary of the Tanh function is 0, not 0.5.

```Python
def draw_predicate_data(net):
    ......
    for i in range(3):
        # if a[i,0] > 0.5:  # logistic function
        if a[i,0] > 0:      # tanh function
            ......
```

Finally, don't forget to call the method for modifying label values in the main program:

```Python
if __name__ == '__main__':
    ......
    reader.ToZeroOne()  # change lable value from 0/1 to -1/1
    # net
    params = HyperParameters(eta=0.1, max_epoch=100, batch_size=10, eps=1e-3, net_type=NetType.BinaryTanh)
    ......
    net = TanhNeuralNet(params, num_input, num_output)
    ......
```

The fourth run! ..... ...Perfect! Both the printout and the final visualization result in Figure 6-20 are perfect.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/tanh_result_4.png" ch="500" />

Figure 6-20 Perfect classification results diagram

Finally, we compare the images of the two classification functions and corresponding cross-entropy functions, as in Table 6-10.

Table 6-10 Comparing the differences of the cross-entropy functions using different classification functions

|Classification function|Cross-entropy function|
|---|---|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/logistic_seperator.png">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/3/crossentropy2.png"/>|
|output value domain a between (0,1), the cutoff is a=0.5, the label value is y=0/1|y=0 for negative cases and y=1 for positive cases, the input value domain a is between (0,1), which is consistent with the output value domain of the logistic function|
|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/tanh_seperator.png">|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/6/modified_crossentropy.png"/>|
|output value domain a between (-1,1), the dividing line is a=0, the label value is y=-1/1|y=-1 for negative cases and y=1 for positive cases, the input value domain a is between (-1,1), which is consistent with the output value domain of the Tanh function|

It can be graphically summarized that when the Tanh function is used, it is equivalent to stretching the range of the Logistic output domain by a factor of 2, and the lower boundary is changed from 0 to -1; whereas the corresponding cross-entropy function, which stretches the input value domain by a factor of 2, and changes the left boundary from 0 to -1, which matches the classification function exactly.

### Code Location

ch06, Level5
