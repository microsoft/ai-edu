<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->
  
# Chapter 7 Multi-input and multi-output single-layer neural nets - Linear multi-classification

## 7.0 Linear multi-classification problem

### 7.0.1 Raising Questions

We have solved the Chu-Han Contention problem from around 200 BCE and now let us look at the Three Kingdoms problem four hundred years later.

There are 140 sample data points in the dataset, as shown in Table 7-1.

Table 7-1 Sample Data Sampling

|Sample number|$x_1=$Relative Longitude|$x_2=$Relative Latitude|$y=$Classification|
|---|---|---|---|
|1|7.033|3.075|3|
|2|4.489|4.869|2|
|3|8.228|9.735|1|
|...|...|...|...|
|140|4.632|9.014|1|

Meaning of the classification label values:

1. Cities of Wei: labelled as 1, blue dots in Figure 7-1
2. Cities of Shu: labelled 2, red dots in Figure 7-1
3. Cities of Wu : labelled 3, green dots in Figure 7-1

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/source_data.png" ch="500" />

Figure 7-1 Sample data visualization

Questions：

1. Under which country's territory do the relative latitude and longitude values $(5,1)$ fall?
2. Under which country's territory do the relative latitude and longitude values $(7,6)$ fall?
3. Under which country's territory do the relative latitude and longitude values $(5,6)$ fall?
4. Under which country's territory do the relative latitude and longitude values $(2,7)$ fall?

### 7.0.2 Multi-classification learning strategy

#### The difference between linear and non-linear multi-classification

Figure 7-2 shows the difference between linear and non-linear multi-classification.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/linear_vs_nonlinear.png" />

Figure 7-2 Intuitive understanding of the difference between linear and non-linear multi-classification

The left side is an example of linear multi-classification, and the right side is an example of non-linear multi-classification. The difference between them is whether or not the sample points of different categories can be separated by a straight line. For neural networks, linear multiclassification can be solved using a single-layer structure, while non-linear multi-classification requires a two-layer structure.

#### The relationship between binary classification and multi-classification

We have learned how to use neural networks for binary classification, which does not work for multi-classification. In traditional machine learning, some binary classification algorithms can be directly generalized for multi-classification, but more often than not, we will use basic strategies of binary classification to solve multi-classification problems.

There are three ways to solve multi-classification problems.

1. one-to-one approach
   
Train one classifier by keeping only two categories of data at a time. If there are $N$ categories, then $C^2_N$ classifiers need to be trained. For example, if $N=3$, we need to train the $A|B, B|C, A|C$ classifiers.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/one_vs_one.png" />

Figure 7-3 One-to-one approach

As shown on the far left of Figure 7-3, this binary classifier only cares about classifying blue and green samples, regardless of the red samples, which means that only blue and green samples are fed into the network during training.
   
When the $(A|B)$ classifier tells you that something class A, you need to put it through the $(A|C)$ classifier and try again, and if it is still class A, then it is class A. If $(A|C)$ tells you it is class C, it is class C. It can't be class B. If you don't believe me, you can go to the $(B|C)$ classifier and test it again.

2. One-to-many approach
   
As in Figure 7-4, when dealing with one category, all other categories are temporarily considered as one category so that for the three-classification problem, three classifiers can be obtained.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/one_vs_multiple.png" />

Figure 7-4 One-to-many approach

As in the leftmost figure, the red samples are treated as one class, and the blue and green samples are mixed together as another category during training.

Three classifiers are called simultaneously, and the three results are combined to give the actual result.  For example, if the first classifier tells you it's a "red class", then it is indeed a red class; if it tells you it is anon-red class, you need to look at the result of the second classifier, green class or non-green class; and so on.

3. Many-to-many approach

Suppose there are 4 categories: ABCD. We can count AB as one class and CD as one class, and train a classifier 1; then count AC as one class and BD as one class, and train a classifier 2.
    
If the first classifier tells you class AB, and the second classifier tells you class BD, then perform the " AND " operation, which means it is class B.

#### Multi-classification and multi-label

In multi-classification learning, although there are multiple categories, each sample belongs to only one category.

For example, if a picture has a blue sky and white clouds and flowers and trees, there are two ways to label this picture.

- The picture is labeled as "landscape" instead of "portrait." This is classification, and the picture has been classified as a landscape.

- The picture is labeled as "blue sky," "white clouds," "flowers," "trees," etc. This kind of task is not called multi-classification learning but multi-label learning, which we do not address here.
