<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 7.1 Multi-classification function

This function works for both linear and non-linear multi-classifications.

Recall that for the binary classification problem, a logistic function is used to calculate the probability value of the sample after the linear calculation, thus dividing the sample into positive and negative categories. What method should we use to calculate the probability values of the samples belonging to each category in a multiclassification problem? And how does the backpropagation process work? We focus on these questions in this section.

### 7.1.1 Definition of multi-classification functions - Softmax

#### How to get the probability of classification results for multi-classification problems？

Logistic functions can yield binary probability values such as 0.8 and 0.3, the former being close to 1 and the latter being close to 0. How can similar probability values be obtained for multi-classification problems?

We still assume that the classification value for a sample is obtained using this linear formula.

$$
z = x \cdot w + b
$$

However, we require that $z$ be not a scalar but a vector. If it is a triple classification problem, we would need $z$ to be a three-dimensional vector, and the element values of each cell in the vector represent the value of that sample belonging to each of the three categories, wouldn't that work?

Specifically, suppose $x$ is a (1x2) vector, and design w as a (2x3) vector and b as a (1x3) vector, then z is a (1x3) vector. We assume that z is calculated as $[3,1,-3]$, which represents the values of sample x in each of the three categories, and we convert it into probability values below.

Some readers may wonder: can't we train the neural network to make its z-values directly into probability form? The answer is no, because z-values are obtained by linear computation, and linear computation cannot efficiently turn z-values into probability values directly.

#### Take the max value

The z-value is $[3,1,-3]$, which will become $[1,0,0]$ if the max operation is taken, which meets our classification needs. That is, the sum of the three values is 1, and the sample is considered to belong to the first class. But there are two shortcomings.

1. The classification result is $[1,0,0]$, which only retains 0 and 1, without information of how much the elements differ from each other. This can be interpreted as "Hard Max".
2. The max operation itself is not differentiable and cannot be used in backpropagation.

#### Introducing Softmax

Softmax adds a "soft" to simulate the behaviour of max but retains the relative size information.

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}}
$$

In the above equation:

- $z_j$ is the original value of the classification for the $j$th term, i.e., the result of the matrix operation
- $z_i$ is the original value of each category involved in the classification calculation
- $m$ is the total number of categories
- $a_j$ is the calculation result of the $j$th term

Assuming $j=1,m=3$, the above equation is：
  
$$a_1=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}$$

Figure 7-5 is used to graphically illustrate this process.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/softmax.png" />

Figure 7-5 Softmax working process

When the input data $[z_1,z_2,z_3]$ is $[3,1,-3]$, the calculation is performed according to the process shown in the figure. The probability distribution of the output can be derived as $[0.879,0.119,0.002]$.

Compare the difference between MAX operation and Softmax, as shown in Table 7-2.

Table 7-2 Differences between MAX operation and Softmax

|Original input|MAX operation|Softmax operation|
|:---:|:---:|:---:|
|$[3, 1, -3]$|$[1, 0, 0]$|$[0.879, 0.119, 0.002]$|

When there are (at least) three categories, by calculating their outputs using the Softmax formula and comparing the relative sizes, it is concluded that the sample belongs to the first category, since the value of 0.879 for the first category is the largest among the three. Note that this is the value calculated for one sample, not three samples, i.e. Softmax gives the probability that a given sample belongs to each of the three categories.

There are two characteristics of this formula.

1. the probabilities of the three categories add up to 1
2. the probability of each class is greater than 0

#### Working Principle of Back Propagation of Softmax

We still assume that the predicted data output from the network is $z=[3,1,-3]$ and the label value is $y=[1,0,0]$. When performing backpropagation, based on previous experience, we will use $z-y$ and get:

$$z-y=[2,1,-3]$$

This information is strange.

- The first item is 2. We have predicted accurately that this sample belongs to the first category, but the value of the reverse error is 2, i.e. the penalty value is 2
- The second term is 1. The penalty value is 1. The prediction is correct--there is still a penalty value
- The third item is -3. The penalty value is -3, which means the reward value is 3. Obviously, the prediction is wrong, but the reward is given

So, if we do not use a mechanism like Softmax, there is the following problem:

- The z-value and y-value, i.e., the predicted value and the labelled value, are not comparable. For example, $z_0=3$ cannot be compared with $y_0=1$.
- The three elements in the z-value are comparable, but they can only be compared in magnitude, not by difference, e.g. $z_0>z_1>z_2$, but the difference between 3 and 1 is 2, and the difference between 1 and -3 is 4, and these differences are meaningless.

After using Softmax, we get the value $a=[0.879,0.119,0.002]$. Using $a-y$:

$$a-y=[-0.121, 0.119, 0.002]$$

Let's analyze this information again:

- The first term, -0.121, rewards that category with 0.121 because the network got it correct, but it could make that probability value larger, preferably 1
- The second term, 0.119, is a penalty because it tries to give the second category a probability of 0.119, so it needs this probability value to be smaller, preferably 0
- The third term, 0.002, is a penalty because it tries to give a probability of 0.002 to the third category, so it needs this probability value to be smaller, preferably 0

This information is totally correct and can be used for backpropagation. Softmax first uses normalization to normalize the output value to be between [0,1] so it can be compared with the label value of 0 or 1 and the magnitude of the penalty or reward can be learned.

In terms of inheritance relation, the Softmax function can be seen as an extension of the Logistic function, such as in a binary classification problem:

$$
a_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2}} = \frac{1}{1 + e^{z_2 - z_1}}
$$

Isn't it very similar to the form of Logistic function? In fact, the logistic function also gives a probability value for the current sample, except that it relies on a bias close to 0 or close to 1 to determine whether it belongs to the positive or negative class.

### 7.1.2 Feed-forward propagation 

#### Matrix operations

$$
z=x \cdot w + b \tag{1}
$$

#### Classification calculation

$$
a_j = \frac{e^{z_j}}{\sum\limits_{i=1}^m e^{z_i}}=\frac{e^{z_j}}{e^{z_1}+e^{z_2}+\dots+e^{z_m}} \tag{2}
$$

#### Loss function calculation

When calculating a single sample, m is the number of categories:
$$
loss(w,b)=-\sum_{i=1}^m y_i \ln a_i \tag{3}
$$

When calculating multiple examples, m is the number of categories and n is number of samples:
$$J(w,b) =- \sum_{j=1}^n \sum_{i=1}^m y_{ji} \log a_{ji} \tag{4}$$

As shown in Figure 7-6.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/7/Loss-A-Z.jpg" ch="500" />

Figure 7-6 Illustration of Softmax in a neural network structure

### 7.1.3 Backpropagation

#### Instantiation deduction

We first deduct the backpropagation formula in terms of instantiation and then extend it to generalizations. Assume that there are three classes, then：

$$
z_1 = x \cdot w+ b_1 \tag{5}
$$
$$
z_2 = x \cdot w + b_2 \tag{6}
$$
$$
z_3 = x \cdot w + b_3 \tag{7}
$$
$$
a_1=\frac{e^{z_1}}{\sum_i e^{z_i}}=\frac{e^{z_1}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{8}
$$
$$
a_2=\frac{e^{z_2}}{\sum_i e^{z_i}}=\frac{e^{z_2}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{9}
$$
$$
a_3=\frac{e^{z_3}}{\sum_i e^{z_i}}=\frac{e^{z_3}}{e^{z_1}+e^{z_2}+e^{z_3}}  \tag{10}
$$

For ease of writing, we make：

$$
E ={e^{z_1}+e^{z_2}+e^{z_3}}
$$

$$
loss(w,b)=-(y_1 \ln a_1 + y_2 \ln a_2 + y_3 \ln a_3)  \tag{11}
$$

$$
\frac{\partial{loss}}{\partial{z_1}}= \frac{\partial{loss}}{\partial{a_1}}\frac{\partial{a_1}}{\partial{z_1}} + \frac{\partial{loss}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_1}} + \frac{\partial{loss}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_1}}  \tag{12}
$$

Solving the terms in Equation 12 in sequence:

$$
\frac{\partial loss}{\partial a_1}=- \frac{y_1}{a_1} \tag{13}
$$
$$
\frac{\partial loss}{\partial a_2}=- \frac{y_2}{a_2} \tag{14}
$$
$$
\frac{\partial loss}{\partial a_3}=- \frac{y_3}{a_3} \tag{15}
$$

$$
\begin{aligned}
\frac{\partial a_1}{\partial z_1}&=(\frac{\partial e^{z_1}}{\partial z_1} E -\frac{\partial E}{\partial z_1}e^{z_1})/E^2 \\\\
&=\frac{e^{z_1}E - e^{z_1}e^{z_1}}{E^2}=a_1(1-a_1)  
\end{aligned}
\tag{16}
$$

$$
\begin{aligned}
\frac{\partial a_2}{\partial z_1}&=(\frac{\partial e^{z_2}}{\partial z_1} E -\frac{\partial E}{\partial z_1}e^{z_2})/E^2 \\\\
&=\frac{0 - e^{z_1}e^{z_2}}{E^2}=-a_1 a_2 
\end{aligned}
\tag{17}
$$

$$
\begin{aligned}
\frac{\partial a_3}{\partial z_1}&=(\frac{\partial e^{z_3}}{\partial z_1} E -\frac{\partial E}{\partial z_1}e^{z_3})/E^2 \\\\
&=\frac{0 - e^{z_1}e^{z_3}}{E^2}=-a_1 a_3  
\end{aligned}
\tag{18}
$$

Combining equations 13 to 18 into 12:

$$
\begin{aligned}    
\frac{\partial loss}{\partial z_1}&=-\frac{y_1}{a_1}a_1(1-a_1)+\frac{y_2}{a_2}a_1a_2+\frac{y_3}{a_3}a_1a_3 \\\\
&=-y_1+y_1a_1+y_2a_1+y_3a_1 \\\\
&=-y_1+a_1(y_1+y_2+y_3) \\\\
&=a_1-y_1 
\end{aligned}
\tag{19}
$$

Without loss of generality, it follows from Equation 19 that:
$$
\frac{\partial loss}{\partial z_i}=a_i-y_i \tag{20}
$$

#### General deduction

1. Derivation of the Softmax function

Since Softmax involves summation, there are two cases:

- Find the derivative of the output term $a_1$ with respect to the input term $z_1$, where: $j=1, i=1, i=j$, which can be extended to any equal value of $i,j$
- Find the derivative of the output $a_2$ or $a_3$ with respect to the input $z_1$, where $j$ is $2$ or $3$, $i=1,i \neq j$, and can be extended to any unequal value of $i,j$

The numerator of Softmax function: Since $a_j$ is calculated, the numerator is $e^{z_j}$.

The denominator of Softmax function：
$$
\sum\limits_{i=1}^m e^{z_i} = e^{z_1} + \dots + e^{z_j} + \dots +e^{z_m} \Rightarrow E
$$

- When $i=j$ (e.g., the derivative of the output classification value $a_1$ with respect to $z_1$), find the derivative of $a_j$ with respect to $z_i$, where $e^{z_j}$ on the numerator has to be involved in the derivative process. Referring to the basic mathematical derivative formula 33:

$$
\begin{aligned}
\frac{\partial{a_j}}{\partial{z_i}} &= \frac{\partial{}}{\partial{z_i}}(e^{z_j}/E) \\\\
&= \frac{\partial{}}{\partial{z_j}}(e^{z_j}/E) \quad (因为z_i==z_i)\\\\
&=\frac{e^{z_j}E-e^{z_j}e^{z_j}}{E^2} 
=\frac{e^{z_j}}{E} - \frac{(e^{z_j})^2}{E^2} \\\\
&= a_j-a^2_j=a_j(1-a_j)  \\\\
\end{aligned}
\tag{21}
$$

- When $i \neq j$ (e.g., the derivative of the output categorical value $a_1$ to $z_2$, $j=1,i=2$), the derivative of $a_j$ to $z_i$, with $z_j$ on the numerator unrelated to $i$, the derivative is 0. The summation term in the denominator, $e^{z_i}$, is to be involved in the derivative. Again, with respect to Equation 33, since the derivative of the numerator $e^{z_j}$ with respect to $e^{z_i}$ results in 0:
$$
\frac{\partial{a_j}}{\partial{z_i}}=\frac{-(E)'e^{z_j}}{E^2}
$$
The derivative $(E)'$ of the summation formula with respect to $e^{z_i}$ is 0 except for the $e^{z_i}$ term:
$$
(E)' = (e^{z_1} + \dots + e^{z_i} + \dots +e^{z_m})'=e^{z_i}
$$
therefore:
$$
\begin{aligned}
\frac{\partial{a_j}}{\partial{z_i}}&=\frac{-(E)'e^{z_j}}{(E)^2}=-\frac{e^{z_j}e^{z_i}}{{(E)^2}} \\\\
&=-\frac{e^{z_j}}{{E}}\frac{e^{z_j}}{{E}}=-a_{i}a_{j} 
\end{aligned}
\tag{22}
$$

2. the overall backpropagation formula combined with the loss function

Looking at the figure above, we require the partial derivative of the Loss value with respect to Z1. Unlike the previous logistic function, that function is a z corresponding to an a, so the backward relationship is also one-to-one. And here, the calculation of a1 is involved in z1,z2,z3; the calculation of a2 is also involved in z1,z2,z3, i.e., all a's are calculated with respect to z in the previous layer, so it is also more complicated when considering the backward direction.

First, from the Loss formula, $loss=-(y_1lna_1+y_2lna_2+y_3lna_3)$, a1 is definitely related to z1. So, are a2,a3 related to z1?

Looking again at the form of the Softmax function:

Both a1, a2, a3, are related to z1. This is not a one-to-one relationship. So, to find the partial derivative of Loss to Z1, we must add up the results of Loss->A1->Z1, Loss->A2->Z1, Loss->A3->Z1，all three ways. Thus, the following equation is obtained:

$$
\begin{aligned}    
\frac{\partial{loss}}{\partial{z_i}} &= \frac{\partial{loss}}{\partial{a_1}}\frac{\partial{a_1}}{\partial{z_i}} + \frac{\partial{loss}}{\partial{a_2}}\frac{\partial{a_2}}{\partial{z_i}} + \frac{\partial{loss}}{\partial{a_3}}\frac{\partial{a_3}}{\partial{z_i}} \\\\
&=\sum_j \frac{\partial{loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}
\end{aligned}
$$

When $i=1,j=3$ in the above equation, it is fully consistent with our assumptions and does not lose generality.

As mentioned before, since Softmax involves the summing of terms, regardless of whether the classification result of A and the classification of label values of Y are consistent, it needs to be discussed by case:

$$
\frac{\partial{a_j}}{\partial{z_i}} = \begin{cases} a_j(1-a_j), & i = j \\\\ -a_ia_j, & i \neq j \end{cases}
$$

Thus, $\frac{\partial{loss}}{\partial{z_i}}$ should be the sum of the cases $i = j$ and $i \neq j$:

- For $i = j$, the loss is derived by $a_1$ with respect to $z_1$ (or by $a_2$ with respect to $z_2$):

$$
\begin{aligned}
\frac{\partial{loss}}{\partial{z_i}} &= \frac{\partial{loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=-\frac{y_j}{a_j}a_j(1-a_j) \\\\
&=y_j(a_j-1)=y_i(a_i-1) 
\end{aligned}
\tag{23}
$$

- $i \neq j$, loss is derived by $a_2+a_3$ with respect to $z_1$:

$$
\begin{aligned}    
\frac{\partial{loss}}{\partial{z_i}} &= \frac{\partial{loss}}{\partial{a_j}}\frac{\partial{a_j}}{\partial{z_i}}=\sum_j^m(-\frac{y_j}{a_j})(-a_ja_i) \\\\
&=\sum_j^m(y_ja_i)=a_i\sum_{j \neq i}{y_j} 
\end{aligned}
\tag{24}
$$

Adding up the two cases:

$$
\begin{aligned}    
\frac{\partial{loss}}{\partial{z_i}} &= y_i(a_i-1)+a_i\sum_{j \neq i}y_j \\\\
&=-y_i+a_iy_i+a_i\sum_{j \neq i}y_j \\\\
&=-y_i+a_i(y_i+\sum_{j \neq i}y_j) \\\\
&=-y_i + a_i*1 \\\\
&=a_i-y_i 
\end{aligned}
\tag{25}$$

Since $y_j$ takes the values $[1,0,0]$ or $[0,1,0]$ or $[0,0,1]$, these three add up to $[1,1,1]$, and multiplying by $[1,1,1]$ in the matrix multiplication operation is equivalent to doing nothing: it is equal to the original value.

We are surprised to find that the final backward calculation process is: $$a_i - y_i$$, assuming $$a_i=[0.879, 0.119, 0.002]$$ for the current sample and $$y_i=[0, 1, 0]$$, then:
$$a_i - y_i = [0.879, 0.119, 0.002]-[0,1,0]=[0.879,-0.881,0.002]$$

The implication is that the sample predicts the first category, but it is actually the second category, so a penalty value of 0.879 is given to the first category, a reward of 0.881 to the second category, and a penalty of 0.002 to the third category, and back propagates to the neural network.

The derivation of $z=wx+b$ is the same as for the binary classification and will not be repeated.

### 7.1.4 Code Implementation

The first one, which is written straightforwardly according to the equation:
```Python
def Softmax1(x):
    e_x = np.exp(x)
    v = np.exp(x) / np.sum(e_x)
    return v
```
The problem that may happen here is that when x is large, `np.exp(x)` can easily overflow because it is an exponential operation. So, there is this improved code as follows:
```Python
def Softmax2(Z):
    shift_Z = Z - np.max(Z)
    exp_Z = np.exp(shift_Z)
    A = exp_Z / np.sum(exp_Z)
    return A
```
To test it:
```Python
Z = np.array([3,0,-3])
print(Softmax1(Z))
print(Softmax2(Z))
```
The results of the two implementations are consistent:
```
[0.95033021 0.04731416 0.00235563]
[0.95033021 0.04731416 0.00235563]
```

Why is it the same? The code looks so different! Let's prove it:

Suppose there are 3 values, a, b, c, and a is the largest of the three. Then, the Softmax weight of b should be written as follows:

$$P(b)=\frac{e^b}{e^a+e^b+e^c}$$

If by subtracting the maximum the terms become a-a, b-a, c-a, then the weight of Softmax accounted for by b' should be written as follows:

$$
\begin{aligned}
P(b') &= \frac{e^{b-a}}{e^{a-a}+e^{b-a}+e^{c-a}} \\
&=\frac{e^b/e^a}{e^a/e^a+e^b/e^a+e^c/e^a} \\
&= \frac{e^b}{e^a+e^b+e^c}
\end{aligned}
$$
Therefore：
$$
P(b) == P(b')
$$

The way `Softmax2` is written is acceptable for a one-dimensional vector or array, but there is a problem if you encounter Z as a $M \times N$-dimensional (M, N>1) matrix because the function `np.sum(exp_Z)` adds all the elements in the $M \times N$ matrix together to get a scalar value, instead of adding the relevant column elements together.

So it should be written like this instead:

```Python
class Softmax(object):
    def forward(self, z):
        shift_z = z - np.max(z, axis=1, keepdims=True)
        exp_z = np.exp(shift_z)
        a = exp_z / np.sum(exp_z, axis=1, keepdims=True)
        return a

```

**The parameter `axis=1` is essential because if the input Z is a single-sample prediction, it should be an array of $3\times 1$ if the input Z is divided into three categories, and if:

- $z = [3,1,-3]$
- $a = [0.879,0.119,0.002]$

However, in the case of batch training, assuming that two samples are used at a time, then:
```
if __name__ == '__main__':
    z = np.array([[3,1,-3],[1,-3,3]]).reshape(2,3)
    a = Softmax().forward(z)
    print(a)
```
The result is：
```
[[0.87887824 0.11894324 0.00217852]
 [0.11894324 0.00217852 0.87887824]]
```
Where a is the softmax result containing two samples, and the three numbers inside each array add up to 1.

If `s = np.sum(exp_z)`, without specifying the `axis=1` parameter, then:
```
[[0.43943912 0.05947162 0.00108926]
 [0.05947162 0.00108926 0.43943912]]
```
A still contains two samples, but it becomes a sum of all 6 elements of the two samples to 1. This is not the intent of softmax, which only calculates the data in one example (one row).

### Thinking and Exercises

1. Is it possible that two or more classified values of a sample are the same among the three classified values, such as $[0.3,0.3,0.4]$?
2. How would you solve a problem like this?
