<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# Chapter 10 Multi-input single-output two-layer neural networks - Non-linear binary classification

## 10.0 Non-linear binary classification

### 10.0.1 Raising question I: The XOR Problem

In 1969, a well-known book, "Perceptrons"(Minsky, Papert, 1969), proved that it was impossible to use single-layer networks ( called perceptrons at that time) to represent the most basic XOR logic functions. This book had a devastating effect, as both financial support and interest in the nascent field of perceptrons disappeared.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/xor_source_data.png" ch="500" />

Figure 10-1 Sample data for the XOR problem

Figure 10-1 shows that the two types of sample points (red forks and blue dots) are distributed crosswise on the four corners of the [0,1] space. It is impossible to separate the two types of samples with a straight line. Neural networks are built based on perceptrons, so how do we solve the XOR problem with neural networks?

### 10.0.2 Raising the Question II: The Hyperbolic Problem

We give a problem that is slightly more complex than the XOR problem, as shown in Figure 10-2.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_data_source.png" ch="500" />

Figure 10-2 Two types of sample data distributed in an arc shape

There are two types of sample data on the plane, both distributed in arcs. The existence of arcs makes it impossible to use a straight line to separate the red and blue sample points, so can the neural network use a curve to separate them?
 
### 10.0.3 Evaluation criteria of the binary classification model

#### Accuracy

It can also be called precision, and we consider these two as the same in this book.

For the binary classification problem, assume that there are 1000 samples in the test set, of which 550 are positive cases, and 450 are negative cases. When testing a model, we obtain the following results: 521 positive cases are judged as the positive class, and 435 negative cases are considered as the negative class, then the correct rate is calculated as follows:

$$Accuracy=(521+435)/1000=0.956$$

That is, the accuracy is 95.6%. This approach is also valid for multi-classifications, i.e., the number of samples correctly discriminated in the three categories divided by the total number of samples is the accuracy rate.

However, this calculation method loses many details: Is the accuracy of positive or negative class higher? Therefore, we also have the following kind of evaluation criteria.

#### Confusion Matrix

Using the example above, if you drill down specifically to each category, it would be evaluated in 4 parts.

- Number of positive examples classified as positive (TP-True Positive): 521
- Number of positive samples classified in the negative category (FN-False Negative): 550-521=29
- Number of negative samples classified as negative (TN-True Negative): 435
- Number of negative samples determined to be positive (FP-False Positive): 450-435=15

Figure 10-3 can be used to help understand.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/TPFP.png"/>

Figure 10-3 Binary classification schematic diagram with four categories

- the solid dots on the left are the positive examples, and the hollow circles on the right are the negative examples;
- samples in the circles are judged by the model to be positive classes, and samples outside the circles are judged to be negative classes;
- points outside the left circle are positive samples but misjudged as the negative category. Points inside the right circle are negative samples but misclassified to the positive category;
- The points inside the left circle are positive cases and are correctly classified to the positive category. The points outside the right circle are negative cases and are correctly classified as negative.

A table describing the matrix would look like Table 10-1.

Table 10-1 Matrix relationships for the four types of samples

|Predicted value|Classified as positive|Classified as negative|Total|
|---|---|---|---|
|The sample is actually a positive example|TP-True Positive|FN-False Negative|Actual Positive=TP+FN|
|The sample is actually a negative example|FP-False Positive|TN-True Negative|Actual Negative=FP+TN|
|Total|Predicated Postivie=TP+FP|Predicated Negative=FN+TN|

The following statistical indicators can be obtained from the confusion matrix:

- Accuracy

$$
\begin{aligned}
Accuracy &= \frac{TP+TN}{TP+TN+FP+FN} \\\\
&=\frac{521+435}{521+29+435+15}=0.956
\end{aligned}
$$

This metric is the accuracy rate mentioned above, the greater the better.

- Precision

The numerator is the number of samples evaluated as positive categories and are indeed positive categories, and the denominator is the number of samples classified as positive categories. The larger the number, the better.

$$
Precision=\frac{TP}{TP+FP}=\frac{521}{521+15}=0.972
$$

- Recall

$$
Recall = \frac{TP}{TP+FN}=\frac{521}{521+29}=0.947
$$

The numerator is the number of samples evaluated as positive categories and indeed positive categories, and the denominator is the actual number of examples that are positive categories. The larger the number, the better.

- TPR - True Positive Rate 

$$
TPR = \frac{TP}{TP + FN}=Recall=0.947
$$

- FPR - False Positive Rate 

$$
FPR = \frac{FP}{FP+TN}=\frac{15}{15+435}=0.033
$$

The numerator is the number of negative cases evaluated as positive classes, and the denominator is the number of all negative examples. The smaller, the better.

- Harmonic mean F1

$$
\begin{aligned}
F1&=\frac{2 \times Precision \times Recall}{recision+Recall}\\\\
&=\frac{2 \times 0.972 \times 0.947}{0.972+0.947}=0.959
\end{aligned}
$$

The larger the value, the better.

- ROC Curve and AUC

ROC, Receiver Operating Characteristic, also known as Sensitivity Curve, is a composite indicator reflecting the continuous variables of sensitivity and specificity. The points on the curve reflect the same perceptibility, and they are all perceptive to the same signal stimulus.
The horizontal coordinate of the ROC curve is the FPR, and the vertical coordinate is the TPR.

AUC, Area Under Roc, which is the area under the ROC curve.

In a binary classifier, if a logistic function is used as the classification function, a series of different thresholds can be set, such as [0.1,0.2,0.3... .0.9], and input the test samples so as to get a series of TP, FP, TN, FN, and then the following curve can be plotted, as in Figure 10-4.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/ROC.png"/>

Figure 10-4 ROC Curve Plot

The red curve in the figure is the ROC curve, and the area under the curve is the AUC value, which takes the value range of $[0.5,1.0]$, the larger the area is, the better.

- The closer the ROC curve is to the upper left corner, the better the performance of the classifier.
- The diagonal line represents a random guess classifier.
- If the ROC curve of one learner is entirely enclosed by the curve of another learner, the latter can be considered to have better performance than the former.
- If the ROC curves of two learners have no inclusion relation, then the larger area under the ROC curve,  i. e. the AUC, is considered to have better performance.

In practice, of course, depending on the sampling interval of the threshold, the red curve would not be so smooth, as the sampling interval would result in a stepped shape of that curve.

Why use ROC and AUC when there are already so many criteria? Because the ROC curve has an excellent property: when the distribution of positive and negative samples in the test set is shifted, the ROC curve can remain unchanged. In the actual dataset, there is often an imbalance in the sample classes, i.e., a significant difference in the proportion of positive and negative samples, and the positive and negative examples in the test data may also change over time.

#### Kappa statistic

Kappa value, the coefficient of internal consistency (inter-rater, coefficient of internal consistency), is an important indicator used to evaluate the extent of agreement between predictions. The value is taken between 0 and 1.

$$
Kappa = \frac{p_o-p_e}{1-p_e}
$$

where $p_0$ is the sum of the number of correctly classified samples in each category divided by the total number of samples, which is the overall classification accuracy. The definition of $p_e$ is shown in the following equation.

- Kappa≥0.75, both of which are in good agreement;
- 0.75>Kappa≥0.4, both with a fair agreement;
- Kappa<0.4 are less consistent. 

This coefficient is usually used in multi-classification situations, for example:

||Actual category A|Actual category B|Actual category C|Total predictions|
|--|--|--|--|--|
|Predicted category A|239|21|16|276|
|Predicted category B|16|73|4|93|
|Predicted category C|6|9|280|295|
|Actual total|261|103|300|664|


$$
p_o=\frac{239+73+280}{664}=0.8916
$$
$$
p_e=\frac{261 \times 276 + 103 \times 93 + 300 \times 295}{664 \times 664}=0.3883
$$
$$
Kappa = \frac{0.8916-0.3883}{1-0.3883}=0.8228
$$

Good data consistency indicates good classifier performance.

#### Mean absolute error and Root mean squared error 

Mean absolute error and root mean squared error measure the difference between the predicted result of the classifier and the actual result. The smaller, the better.

#### Relative absolute error and Root relative squared error 

Relative absolute error and root relative squared error, sometimes the absolute error does not reflect the true magnitude of the error, while the relative error reflects the extent of the error by reflecting the proportion of the error to the actual value.
