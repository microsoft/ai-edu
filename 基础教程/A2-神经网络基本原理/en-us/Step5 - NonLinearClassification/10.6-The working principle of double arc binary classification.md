<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 10.6 The working principle of double arc binary classification

In the XOR problem, we learned that any complex classification problem in the plane can be solved if we use a three-dimensional coordinate system to analyze it. It is straightforward to separate the different categories of samples by simply raising them up through a three-dimensional linear transformation. But this explanation is a bit far-fetched, and I do not think the neural network has been smart enough to do so.

Therefore, I tried to continue my research in the two-dimensional plane to find the real answer and read some information about streaming learning, so I conducted the following experiments to verify the spatial transformation the neural network actually does in the two-dimensional plane.

### 10.6.1 Visualization of two-layer neural networks

#### A few helper functions
- `DrawSamplePoints(x1, x2, y, title, xlabel, ylabel, show=True)`
  
  Draw the sample points, plotting the positive examples as red `x` and the negative examples as blue dots. The input `x1` and `x2` form the horizontal and vertical coordinates, and `y` is the label value of the positive and negative examples.

- `Prepare3DData(net, count)

  Prepare the 3D data, divide the plane into a grid of `count` * `count`, and form the matrix. If the incoming `net` is not None, an inference is made using `net.inference()` in order to get the output value corresponding to the grid on the plane.

- `DrawGrid(Z, count)`

  Draw the grid. This grid is not necessarily square and may be distorted due to translational scaling of the matrix, aiming to observe the transformation of the space by the neural network.

- `ShowSourceData(dataReader)`

  Displays the original training sample data.

- `ShowTransformation(net, dr, epoch)`

  The result of the spatial transformation is plotted after the linear calculation of the first layer of the neural network, i.e., the activation function calculation. The second layer of the neural network completes the classification task on top of the spatial transformation of the first layer.

- `ShowResult2D(net, dr, epoch)`

  Displaying classification results on a 2D plane is actually showing the 2.5D classification results in contour lines.

#### The Train function

```Python
def train(dataReader, max_epoch):
    n_input = dataReader.num_feature
    n_hidden = 2
    n_output = 1
    eta, batch_size = 0.1, 5
    eps = 0.01

    hp = HyperParameters2(n_input, n_hidden, n_output, eta, max_epoch, batch_size, eps, NetType.BinaryClassifier, InitialMethod.Xavier)
    net = NeuralNet2(hp, "Arc_221_epoch")
    
    net.train(dataReader, 5, True)
    
    ShowTransformation(net, dataReader, max_epoch)
    ShowResult2D(net, dataReader, max_epoch)
```
Receive `max_epoch` as a parameter to control the number of training iterations of the neural network to observe the intermediate results. We use the following hyperparameters:

- `n_input=2`, the number of input feature values
- `n_hidden=2`, the number of neurons in the hidden layer
- `n_output=1`, the output is binary
- `eta=0.1`, the learning rate
- `batch_size=5`, the number of batch samples is 5
- `eps=0.01`, the stop condition
- `NetType.BinaryClassifier`, binary classification network
- `InitialMethod.Xavier`, initialization method is Xavier

The loss value calculation is done every 5 iterations, and the results are printed once. Finally, the intermediate state graph and the classification result graph are displayed.

#### The main process

```Python
if __name__ == '__main__':
    dataReader = DataReader(train_data_name, test_data_name)
    dataReader.ReadData()
    dataReader.NormalizeX()
    dataReader.Shuffle()
    dataReader.GenerateValidationSet()

    ShowSourceData(dataReader)
    plt.show()

    train(dataReader, 20)
    train(dataReader, 50)
    train(dataReader, 100)
    train(dataReader, 150)
    train(dataReader, 200)
    train(dataReader, 600)
```
After reading the data, using 20, 50, 100, 150, 200, 600 `epoch` for the training stop condition to observe the intermediate state, the author knew in advance through experimentation that 600 iterations would definitely achieve satisfactory results. And the value of the above `epoch` is determined by observing the descent curve of the loss function.

### 10.6.2 Runtime result

After running, a picture of the original sample locations is first displayed as in Figure 10-16 to determine if the training samples are correct and get a basic idea of the sample distribution.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_data_source.png" ch="500" />

Figure 10-16 Double arc-shaped sample data

With each call of the `train()` function, the following images are displayed sequentially at the end of each training session.

- The linear transformation result of the first layer of the neural network
- The activation function result of the first layer of the neural network
- Classification results of the second layer neural network

Table 10-15 Visualization of the training process

|iterations|linear transformations|activation results|classification results|
|---|---|---|---|
|20 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_z1_20.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a1_20.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a2_20.png'/>|
|100 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_z1_100.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a1_100.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a2_100.png'/>|
|200 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_z1_200.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a1_200.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a2_200.png'/>|
|600 iterations|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_z1_600.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a1_600.png'/>|<img src='https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a2_600.png'/>|

Analyzing the changes of the pictures in each column in Tables 10-15, we can get the following conclusions:

1. In the linear transformation of the first layer, the original samples are stretched oblique laterally with the angle gradually tilted 40 degrees to the left, and the sample spacing is also slowly stretched. The actual examples are normalized between [0,1] and finally extended to the range of [-5,15]. This lateral stretching is actually a preparation for the activation function.
2. In the activation function calculation, due to the non-linearity of the activation function, space is gradually distorted and deformed, causing the red sample points to slowly move to the lower right corner and become dense;. In contrast, the blue sample points gradually spread to the upper left, and it is believed that its limit must be the left and upper boundaries of the [0,1] space. Another point worth explaining is that the red and blue categories can be divided by a straight line through space distortion! This is a very magical thing.
3. The final classification results, from clueless to slowly arching upwards, then a wide and fuzzy classification boundary, and finally a very sharp boundary.

It seems that at this point, we can come to a conclusion: the neural network transforms the linearly inseparable samples into linearly separable pieces through spatial transformation. Thus the final classification becomes simple.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/10/sin_a1_line.png" ch="500" />

Figure 10-17 Sample data after spatial transformation

As in Figure 10-17, the green straight line can efficiently perform the binary classification task. This straight line will look like the last picture of the classification results in the fourth column in the above table if it is restored to the original sample picture.

### Thinking and exercises

1. Please use the same method to analyze the XOR problem.

### Code location

ch10, Level4
