<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# Chapter 11 Multiple-Input Multiple-Output Two-Layer Neural Networks - Non-linear Multiple Classification

## 11.0 Non-linear Multiple Classification Problem

### 11.0.1 Raising the question: the copper coin hole shape problem

We learned binary classification with the XOR problem and arc-shaped samples earlier. Now let's see how to use it for non-linear multiclassification.

We have 1000 samples and labels, as shown in Table 11-1.

Table 11-1 Sample data for multi-classification problem

|Sample|$x_1$|$x_2$|$y$|
|---|---|---|---|
|1|0.22825111|-0.34587097|2|
|2|0.20982606|0.43388447|3|
|...|...|...|...|
|1000|0.38230143|-0.16455377|2|

Fortunately, this data has only two features, so we can display it visually, as in Figure 11-1.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/data.png" ch="500" />

Figure 11-1 Visualization of sample data

There are 3 categories in total.

1. blue square
2. red crosses
3. green dots

The samples form a shape that looks like a copper coin, so let's call this problem the "copper coin-shaped classification" problem, which will be mentioned later.

**Problem: How to use a two-layer neural network to solve this three-classification problem? **

The three colours' points occupy different regions of $(-0.5,0.5)$ in a unit plane in a regular manner. It is clear from the graph that this is not a linearly separable problem, and a single-layer neural network can only do linear classification; if you want to do non-linear classification, you need at least two layers of neural networks to do it.

Red and green is a circular boundary division, and red and blue is a rectangular border. Both are in a regular pattern. However, to learn neural networks, we should forget the word "regular", the mathematical "regular" or "irregular" is meaningless for the neural network: all of them are irregular, and the training difficulty is the same.

In addition, the boundary is also meaningless and should be understood in terms of probability. There is no 0 or 1 dividing line to tell us which points should belong to which area. We can get is the probability that a sample point at a specific location belongs to three categories, and then we take the category with the highest probability as the final evaluation result.

### 11.0.2 Evaluation criteria for multi-classification models

Let's take a three-classification problem as an example. Assuming that there are 100 samples in each class and 300 samples in total, the final classification results are shown in Table 11-2.

Table 11-2 Confusion matrix of multi-classification results

|Category of sample|classified to class 1|classified to class 2|classified to class 3|total number of samples in each category|precision (accuracy) rate|
|---|---|---|---|---|---|
|Category 1|90|4|6|100|90%|
|Category 2|9|84|5|100|84%|
|Category 3|1|4|95|100|95%|
|Total number of sample|101|93|106|300|89.67%|

- Category 1 samples, 4 were misclassified to category 2, 6 were misclassified to category 3, and 90 were correct.
- Category 2 samples, 9 were misclassified to category 1, 5 were misclassified to category 3, and 84 were correct
- Category 3 samples, 1 was misclassified to category 1, 4 were misclassified to category 2, and 95 were correct.
 
The overall accuracy rate was 89.67%. The accuracy rates for the three categories are 90%, 84%, and 95%. In fact, Table 11-2 is also an extended form of the confusion matrix based on binary classification, which is characterized by the fact that the larger the value on the diagonal, the better.

Of course, it is possible to calculate Precision and Recall for each category, but only when needed. For example, when the confusion between category 2 and category 3 is more serious, the analysis of category 2 and category 3 is done separately to record the history of model training.

In this book, we use only the overall accuracy to measure how good a multi-classifier is.


