<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 11.1 Non-linear multi-classification

### 11.1.1 Defining the neural network structure

A network structure that can accomplish non-linear multi-classification is first designed, as shown in Figure 11-2.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/nn.png" />

Figure 11-2 Neural network structure of non-linear multi-classification

- Two feature values of input layer $x_1, x_2$
$$
x=
\begin{pmatrix}
    x_1 & x_2
\end{pmatrix}
$$
- The $2\times 3$ weighting matrix $W1$ of the hidden layer
$$
W1=
\begin{pmatrix}
    w1_{11} & w1_{12} & w1_{13} \\\\
    w1_{21} & w1_{22} & w1_{23}
\end{pmatrix}
$$

- The $1\times 3$ offset matrix $B1$ of the hidden layer

$$
B1=\begin{pmatrix}
    b1_1 & b1_2 & b1_3 
\end{pmatrix}
$$

- The hidden layer consists of 3 neurons
- The $3\times 3$ weight matrix $W2$ of the output layer 
$$
W2=\begin{pmatrix}
    w2_{11} & w2_{12} & w2_{13} \\\\
    w2_{21} & w2_{22} & w2_{23} \\\\
    w2_{31} & w2_{32} & w2_{33} 
\end{pmatrix}
$$

- $1\times 1$ offset matrix $B2$ of output layer 

$$
B2=\begin{pmatrix}
    b2_1 & b2_2 & b2_3 
  \end{pmatrix}
$$

- The output layer has 3 neurons for classification using the Softmax function

### 11.1.2 Feed-forward calculation

A feed-forward calculation diagram can be drawn based on the network structure, as shown in Figure 11-3.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_forward.png" />

Figure 11-3 Diagram of feed-forward calculation

#### The first layer 

- Linear calculation

$$
z1_1 = x_1 w1_{11} + x_2 w1_{21} + b1_1
$$
$$
z1_2 = x_1 w1_{12} + x_2 w1_{22} + b1_2
$$
$$
z1_3 = x_1 w1_{13} + x_2 w1_{23} + b1_3
$$
$$
Z1 = X \cdot W1 + B1
$$

- Activation function

$$
a1_1 = Sigmoid(z1_1) 
$$
$$
a1_2 = Sigmoid(z1_2) 
$$
$$
a1_3 = Sigmoid(z1_3) 
$$
$$
A1 = Sigmoid(Z1)
$$

#### The second layer

- Linear calculation

$$
z2_1 = a1_1 w2_{11} + a1_2 w2_{21} + a1_3 w2_{31} + b2_1
$$
$$
z2_2 = a1_1 w2_{12} + a1_2 w2_{22} + a1_3 w2_{32} + b2_2
$$
$$
z2_3 = a1_1 w2_{13} + a1_2 w2_{23} + a1_3 w2_{33} + b2_3
$$
$$
Z2 = A1 \cdot W2 + B2
$$

- Classification function 

$$
a2_1 = \frac{e^{z2_1}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
a2_2 = \frac{e^{z2_2}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
a2_3 = \frac{e^{z2_3}}{e^{z2_1} + e^{z2_2} + e^{z2_3}}
$$
$$
A2 = Softmax(Z2)
$$

#### Loss function

Use the multi-classification cross-entropy loss function:
$$
loss = -(y_1 \ln a2_1 + y_2 \ln a2_2 + y_3 \ln a2_3)
$$
$$
J(w,b) = -\frac{1}{m} \sum^m_{i=1} \sum^n_{j=1} y_{ij} \ln (a2_{ij})
$$

$m$ is the number of samples,$n$ is the number of categories.

### 11.1.3 Back-propagation

Based on the feed-forward calculation diagram, the backpropagation path can be plotted in Figures 11-4.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/multiple_backward.png" />

Figure 11-4 Diagram of backpropagation 

We learned the backpropagation derivation process for Softmax combined with multi-classification cross-entropy in Section 7.1 and ended with a very simple subtraction:

$$
\frac{\partial loss}{\partial Z2}=A2-y \rightarrow dZ2
$$

Starting from Z2 and moving forward, it is the same as section 10.2, so take the conclusion directly:

$$
\frac{\partial loss}{\partial W2}=A1^{\top} \cdot dZ2 \rightarrow dW2
$$
$$\frac{\partial{loss}}{\partial{B2}}=dZ2 \rightarrow dB2$$
$$
\frac{\partial A1}{\partial Z1}=A1 \odot (1-A1) \rightarrow dA1
$$
$$
\frac{\partial loss}{\partial Z1}=dZ2 \cdot W2^{\top} \odot dA1 \rightarrow dZ1 
$$
$$
dW1=X^{\top} \cdot dZ1
$$
$$
dB1=dZ1
$$

### 11.1.4 Code Implementation

The vast majority of the code is implemented in the basic classes in the `HelperClass2` directory, here is only the main process:

```Python
if __name__ == '__main__':
    ......
    n_input = dataReader.num_feature
    n_hidden = 3
    n_output = dataReader.num_category
    eta, batch_size, max_epoch = 0.1, 10, 5000
    eps = 0.1
    hp = HyperParameters2(n_input, n_hidden, n_output, eta, max_epoch, batch_size, eps, NetType.MultipleClassifier, InitialMethod.Xavier)
    # create net and train
    net = NeuralNet2(hp, "Bank_233")
    net.train(dataReader, 100, True)
    net.ShowTrainingTrace()
    # show result
    ......
```

Process Description:

1. read the data file
2. display the sample distribution of the original data
3. other data operations: normalize, disorder, create the validation set
4. set hyperparameters
5. build the neural network and start training
6. display the training results

### 11.1.5 Runtime results

The training process is shown in Figure 11-5.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/loss.png" />

Figure 11-5 Variation of loss function values and accuracy values during training

The condition that the loss function is less than 0.1 is not reached after 5000 iterations.

The classification results are shown in Figure 11-6.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/11/result.png" ch="500" />

Figure 11-6 Diagram of classification results

The classification results are average because the accuracy requirement is not met. From the classification result diagram, the outer circle is almost fitted, but the inner square is much worse.

Printout:

```
......
epoch=4999, total_iteration=449999
loss_train=0.225935, accuracy_train=0.800000
loss_valid=0.137970, accuracy_valid=0.960000
W= [[ -8.30315494   9.98115605   0.97148346]
 [ -5.84460922  -4.09908698 -11.18484376]]
B= [[ 4.85763475 -5.61827538  7.94815347]]
W= [[-32.28586038  -8.60177788  41.51614172]
 [-33.68897413  -7.93266621  42.09333288]
 [ 34.16449693   7.93537692 -41.19340947]]
B= [[-11.11937314   3.45172617   7.66764697]]
testing...
0.952
```
The final test classification accuracy was 0.952.

### Code location

ch11, Level1

### Thinking and exercises

1. Please try to improve the parameters to get a better classification result by making the inner circle an approximate square boundary.
