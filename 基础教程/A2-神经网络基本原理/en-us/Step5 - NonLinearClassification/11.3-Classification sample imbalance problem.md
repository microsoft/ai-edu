<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 11.3 Classification sample imbalance problem

### 11.3.1 What is imbalanced data

There is an assumption that the number of training samples for different categories is relatively balanced in general classification learning methods.

For example, in the case of binary classification, both positive and negative samples have about 1000 each. If the ratio is 1200:800, it is acceptable, but if it is 1900:100, some measures are needed to solve the imbalance problem. Otherwise, the final training result will likely ignore the negative cases and classify all samples as positive categories.

In the case of triple classification, it is acceptable to assume that the ratio of samples in the three categories is: 1000:800:600; but if it is 1000:300:100, it is unbalanced. It brings the result that the classifier over-fits the examples of the first category and under-fits the samples of the other two types. The test result will definitely be inaccurate.

The category imbalance problem is very common in reality. In most classification tasks, the number of data under each category is basically impossible to be exactly equal, but a slight difference does not have any impact or problem. In reality, there are many category imbalance problems, which are ordinary and reasonable to expect.

In the previous section, we used the accuracy as an indicator to evaluate the quality of classification, and we can see that accuracy as an evaluation metric does not work when the categories are unbalanced. For example, an extreme example is 98 positive cases and 2 negative cases in disease prediction. The classifier can get 98% accuracy as long as all samples are predicted as positive categories, which makes this classifier worthless.

### 11.3.2 How to solve the imbalanced Data problem

#### Balanced data sets

There is a saying that "more data often beats better algorithms." For example, if the current number of positive and negative samples is 1000:100, you can collect 2000 more data and finally get a ratio of 2800:300. You can discard some of the positive examples and turn them into 500:300, then you can train.

Some rules of thumb.

- Consider undersampling samples under large classes (over 10,000, 100,000 or even more), i.e., removing some of the examples.
- Consider oversampling samples under small classes (less than 10,000 or even less), i.e., adding copies of some examples.
- consider trying both random and non-random sampling methods of sampling.
- consider trying different sampling ratios for each category than 1:1; sometimes, 1:1 is not good because it is far from reality.
- Consider using both over-sampling and under-sampling.

#### Try other evaluation indicators 

From the previous analysis, it is clear that accuracy as an evaluation metric does not work in the category-unbalanced classification task and even performs misleading (the classifier does not work, but from this indicator, the classifier has a good evaluation metric score). Therefore, in category-imbalanced classification tasks, more convincing evaluation metrics need to be used to evaluate the classifier. See here for how to choose effective evaluation metrics for different problems. 

Conventional classification evaluation metrics may fail, e.g., if all samples are classified into large categories, accuracy, precision, etc., will be high. In this case, AUC is the best evaluation metric.

#### Attempt to generate artificial data samples 

A simple way to generate artificial sample data is to randomly select a new sample from the value space of each attribute feature for all samples under the class, i.e., random sampling of attribute values. You can construct a new artificial example using empirical random sampling of attribute values, or you can use something like a Naive Bayes approach assuming that the attributes are sampled independently of each other. This gives you more data but does not guarantee a linear relationship (if one exists) before the features. 

There is a systematic method for constructing artificial data samples SMOTE (Synthetic Minority Over-sampling Technique).SMOTE is an over-sampling algorithm that creates new examples of a minor class instead of generating copies of examples already in the minor category, i.e., the algorithm constructs data that are new samples that do not exist in the original data set. The algorithm selects two or more similar samples under the small class based on the distance metric. Then selects one of the samples and randomly selects a certain number of neighbouring samples to add noise to one attribute of the selected one, processing one feature at a time.  This way, more nascent data are constructed. Details can be found in the original paper. 

Use the following command:

```
pip install imblearn
```
The SMOTE algorithm package can be installed and used to achieve sample balancing.

#### Understand the problem from a new perspective 

We can solve the problem of imbalanced data from a different perspective than classification. We can treat those small classes of samples as outliers, so the problem is transformed into anomaly detection and change detection problems. 

Anomaly detection is the identification of those rare events. For example, machine failures are identified by vibrations of machine components, or malicious programs are recognized by sequences of system calls. These events are rare compared to the normal situation. 

Change detection is similar to anomaly detection, except that it identifies unusual changes by detecting them. For example, notable changes in user behaviour are detected by looking at user patterns or bank transactions. 

This shift in thinking about small classes of samples as anomalies can help consider new ways to separate or classify samples. These two approaches think from different perspectives and allow you to try new ways to solve problems.

#### Modify existing algorithms

- Let the number of samples in the large class be L times the number of samples in the small category. Then in the stochastic gradient descent (SGD, stochastic gradient descent) algorithm, each time an example in the small class is encountered for training, it is trained L times.
- The samples in the large class are divided into L clusters, and then L classifiers are trained, each using one cluster in the large category with all the examples in the small class. Finally, these L classifiers are classified by taking the minority rule for the unknown class data, and if they are continuous values (predictions), then the average value is used.
- Let there be N samples in the small class. Cluster the large class into N clusters and then use the centers of each collection to form N samples in the large class plus all the samples in the small category for training.

No matter which of the previous methods you use, it causes damage to one or more classes. To avoid damage, multiple classifiers can be obtained using the whole training set and multiple classification methods to build classifiers separately, using voting to classify data of unknown classes. In the case of continuous values (predictions), the average value is used.
The recent ICML paper shows that increasing the amount of data causes an increase in the error of a training set with a known distribution, i.e., it disrupts the distribution of the original training set, which can improve the performance of the classifier. This paper is not relevant to the imbalanced data problem because it implicitly uses mathematics to increase the data while leaving the dataset size unchanged. However, I think it is beneficial to corrupt the original distribution.

#### Ensemble learning

An excellent method to deal with the problem of unbalanced data has been proved theoretically. This method is "The strength of weak learnability" proposed by Robert E. Schapire in Machine Learning in 1990, which is a boosting algorithm that recursively trains three weak learners and then combines the three weak learners to form a strong learner. We can use the first step of this algorithm to solve the imbalanced data problem. 

1. first, train the first learner L1 using the original data set.
2. then, using 50% of those samples learned correctly in L1 and 50% of those learned incorrectly to train to get learner L2, i.e. the set of samples learned incorrectly from L1, and the set of samples learned correctly, recursively sampling one on each side.
3. next, train using those inconsistent samples between L1 and L2 to obtain learner L3.
4. finally, use the voting method as the final output. 

So how to use this algorithm to solve the imbalanced class problem? 

Suppose it is a binary classification problem where most of the samples are in the true category. Let the output of L1 always be true. Train L2 using 50% of the samples classified correctly in L1, and 50% of the samples classified incorrectly, i.e., the set of examples learned incorrectly from L1 and the set of examples learned correctly, sampling one side of the cycle. Thus, the training samples of L2 are balanced, and L is trained using those inconsistent with the classification of L1 and L2 to obtain L3, i.e., those samples classified as false in L2. Finally, combining these three classifiers, voting to decide the classification result is used. The final result is false only when both L2 and L3 are classified as false and true otherwise. 
