<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 12.1 Implementation of three-layer neural network

### 12.1.1 Defining a Neural Network

To achieve MNIST classification, we need to design a three-layer neural network structure, as shown in Figure 12-2.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/nn3.png" ch="500" />

Figure 12-2 Three-layer neural network structure

#### Input layer

Total $28\times 28=784$ feature value:

$$
X=\begin{pmatrix}
    x_1 & x_2 & \cdots & x_{784}
  \end{pmatrix}
$$

#### Hidden layer 1

- The weight matrix $W1$ is shaped as $784\times 64$

$$
W1=\begin{pmatrix}
    w1_{1,1} & w1_{1,2} & \cdots & w1_{1,64} \\\\
    \vdots & \vdots & \cdots & \vdots \\\\
    w1_{784,1} & w1_{784,2} & \cdots & w1_{784,64} 
  \end{pmatrix}
$$

- The offset matrix $B1$ has the shape $1\times 64$

$$
B1=\begin{pmatrix}
    b1_{1} & b1_{2} & \cdots & b1_{64}
  \end{pmatrix}
$$

- Hidden layer 1 consists of 64 neurons, which results in a matrix of $1\times 64$

$$
Z1=\begin{pmatrix}
    z1_{1} & z1_{2} & \cdots & z1_{64}
  \end{pmatrix}
$$
$$
A1=\begin{pmatrix}
    a1_{1} & a1_{2} & \cdots & a1_{64}
  \end{pmatrix}
$$

#### Hidden layer 2

- The weight matrix $w2$ is shaped as $64\times 16$

$$
W2=\begin{pmatrix}
    w2_{1,1} & w2_{1,2} & \cdots & w2_{1,16} \\\\
    \vdots & \vdots & \cdots & \vdots \\\\
    w2_{64,1} & w2_{64,2} & \cdots & w2_{64,16} 
  \end{pmatrix}
$$

- The shape of the offset matrix #B2# is $1\times 16$

$$
B2=\begin{pmatrix}
    b2_{1} & b2_{2} & \cdots & b2_{16}
  \end{pmatrix}
$$

- Hidden layer 2 consists of 16 neurons

$$
Z2=\begin{pmatrix}
    z2_{1} & z2_{2} & \cdots & z2_{16}
  \end{pmatrix}
$$
$$
A2=\begin{pmatrix}
    a2_{1} & a2_{2} & \cdots & a2_{16}
  \end{pmatrix}
$$

#### Output layer

- The shape of the weight matrix $W3$ is $16\times 10$

$$
W3=\begin{pmatrix}
    w3_{1,1} & w3_{1,2} & \cdots & w3_{1,10} \\\\
    \vdots & \vdots & \cdots & \vdots \\\\
    w3_{16,1} & w3_{16,2} & \cdots & w3_{16,10} 
  \end{pmatrix}
$$

- The offset matrix $B3$ of the output layer has the shape $1\times 10$

$$
B3=\begin{pmatrix}
    b3_{1}& b3_{2} & \cdots & b3_{10}
  \end{pmatrix}
$$

- The output layer has 10 neurons for classification using the Softmax function

$$
Z3=\begin{pmatrix}
    z3_{1} & z3_{2} & \cdots & z3_{10}
  \end{pmatrix}
$$
$$
A3=\begin{pmatrix}
    a3_{1} & a3_{2} & \cdots & a3_{10}
  \end{pmatrix}
$$

### 12.1.2 Feed-forward calculation

We describe the formulas in matrix form with capitalized symbols, and on the upper right corner of each matrix symbol is its shape.

#### Hidden layer 1

$$Z1 = X \cdot W1 + B1 \tag{1}$$

$$A1 = Sigmoid(Z1) \tag{2}$$

#### Hidden layer 2

$$Z2 = A1 \cdot W2 + B2 \tag{3}$$

$$A2 = Tanh(Z2) \tag{4}$$

#### Output layer

$$Z3 = A2 \cdot W3  + B3 \tag{5}$$

$$A3 = Softmax(Z3) \tag{6}$$

Our convention is behaviour samples, listed as all features of an example, here it is 784 features because the image height and width are both 28, a total of 784 points, make the value of each point as a feature vector.

Two hidden layers with 64 neurons and 16 neurons are defined respectively. The first hidden layer uses the Sigmoid activation function and the second hidden layer uses the Tanh activation function.

Together with a Softmax calculation, the output layer of 10 neurons ends up with $a1,a2,... .a10$ in total ten outputs, representing 10 numbers from 0-9.

### 12.1.3 Backward Propagation

There is not much difference with the previous two-layer network, except that there is one more layer, and the Tanh activation function is used. The purpose is to pass back more gradient values because the Tanh function is slightly better than the sigmoid function, such as the origin symmetry, zero-point gradient value is large.

#### Output layer

$$dZ3 = A3-Y \tag{7}$$
$$dW3 = A2^{\top} \cdot dZ3 \tag{8}$$
$$dB3=dZ3 \tag{9}$$

#### Hidden layer 2

$$dA2 = dZ3 \cdot W3^{\top} \tag{10}$$
$$dZ2 = dA2 \odot (1-A2 \odot A2) \tag{11}$$
$$dW2 = A1^{\top} \cdot dZ2 \tag{12}$$
$$dB2 = dZ2 \tag{13}$$

#### Hidden layer 1

$$dA1 = dZ2 \cdot W2^{\top} \tag{14}$$
$$dZ1 = dA1 \odot A1 \odot (1-A1) \tag{15}$$
$$dW1 = X^{\top} \cdot dZ1 \tag{16}$$
$$dB1 = dZ1 \tag{17}$$

### 12.1.4 Code Implementation

In `HelperClass3` / `NeuralNet3.py`, the code below mainly lists the code that differs from the two-layer network.

#### Initialization

```Python
class NeuralNet3(object):
    def __init__(self, hp, model_name):
        ...
        self.wb1 = WeightsBias(self.hp.num_input, self.hp.num_hidden1, self.hp.init_method, self.hp.eta)
        self.wb1.InitializeWeights(self.subfolder, False)
        self.wb2 = WeightsBias(self.hp.num_hidden1, self.hp.num_hidden2, self.hp.init_method, self.hp.eta)
        self.wb2.InitializeWeights(self.subfolder, False)
        self.wb3 = WeightsBias(self.hp.num_hidden2, self.hp.num_output, self.hp.init_method, self.hp.eta)
        self.wb3.InitializeWeights(self.subfolder, False)
```
The initialization section requires the construction of three groups of `WeightsBias` objects. Note that the number of inputs and outputs in each group determines the shape of the matrix.

#### Feed-forward calculation

```Python
    def forward(self, batch_x):
        # Equation 1
        self.Z1 = np.dot(batch_x, self.wb1.W) + self.wb1.B
        # Equation 2
        self.A1 = Sigmoid().forward(self.Z1)
        # Equation 3
        self.Z2 = np.dot(self.A1, self.wb2.W) + self.wb2.B
        # Equation 4
        self.A2 = Tanh().forward(self.Z2)
        # Equation 5
        self.Z3 = np.dot(self.A2, self.wb3.W) + self.wb3.B
        # Equation 6
        if self.hp.net_type == NetType.BinaryClassifier:
            self.A3 = Logistic().forward(self.Z3)
        elif self.hp.net_type == NetType.MultipleClassifier:
            self.A3 = Softmax().forward(self.Z3)
        else:   # NetType.Fitting
            self.A3 = self.Z3
        #end if
        self.output = self.A3
```
A layer is added to the feed-forward calculation part and `Tanh()` is used as the activation function.

- Backward Propagation
```Python
    def backward(self, batch_x, batch_y, batch_a):
        # Batch size drop, which needs to divide by the number of samples. Otherwise, it will cause exploding gradient problem
        m = batch_x.shape[0]

        # Gradient input for the third layer   Equation 7
        dZ3 = self.A3 - batch_y
        # Equation 8
        self.wb3.dW = np.dot(self.A2.T, dZ3)/m
        # Equation 9
        self.wb3.dB = np.sum(dZ3, axis=0, keepdims=True)/m 

        # Gradient input for the second layer   Equation 10
        dA2 = np.dot(dZ3, self.wb3.W.T)
        # Equation 11
        dZ2,_ = Tanh().backward(None, self.A2, dA2)
        # Equation 12
        self.wb2.dW = np.dot(self.A1.T, dZ2)/m 
        # Equation 13
        self.wb2.dB = np.sum(dZ2, axis=0, keepdims=True)/m 

        # Gradient input for the first layer   Equation 8
        dA1 = np.dot(dZ2, self.wb2.W.T) 
        # dZ of the first layer   Equation 10
        dZ1,_ = Sigmoid().backward(None, self.A1, dA1)
        # Weights and offsets for the first layer  Equation 11
        self.wb1.dW = np.dot(batch_x.T, dZ1)/m
        self.wb1.dB = np.sum(dZ1, axis=0, keepdims=True)/m 

    def update(self):
        self.wb1.Update()
        self.wb2.Update()
        self.wb3.Update()
```
The backpropagation also adds a layer accordingly, and note that the corresponding backward formula of `Tanh()` is to be used. The gradient is also updated with three sets of weight values at the same time.

- Main Process

```Python
if __name__ == '__main__':
    ......
    n_input = dataReader.num_feature
    n_hidden1 = 64
    n_hidden2 = 16
    n_output = dataReader.num_category
    eta = 0.2
    eps = 0.01
    batch_size = 128
    max_epoch = 40

    hp = HyperParameters3(n_input, n_hidden1, n_hidden2, n_output, eta, max_epoch, batch_size, eps, NetType.MultipleClassifier, InitialMethod.Xavier)
    net = NeuralNet3(hp, "MNIST_64_16")
    net.train(dataReader, 0.5, True)
    net.ShowTrainingTrace(xline="iteration")
```
Hyperparameter configuration: 64 neurons in the first hidden layer, 16 in the second hidden layer, learning rate 0.2, batch size 128, Xavier initialization, maximum training 40 epochs.

### 12.1.5 Runtime results

The curve of variation of loss function value and accuracy value is shown in Figure 12-3.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/loss.png" />

Figure 12-3 Variation of loss function and accuracy during training

Printout section:

```
...
epoch=38, total_iteration=16769
loss_train=0.012860, accuracy_train=1.000000
loss_valid=0.100281, accuracy_valid=0.969400
epoch=39, total_iteration=17199
loss_train=0.006867, accuracy_train=1.000000
loss_valid=0.098164, accuracy_valid=0.971000
time used: 25.697904109954834
testing...
0.9749
```

The accuracy obtained on the test set was 97.49%, which is ideal.

### Code location

ch12, Level1

### Thinking and Exercises

1. As we said earlier, the number of neurons in the hidden layer should be larger than the number of feature values in the input to handle multiple feature value inputs well. But in this problem, we have 784 eigenvalue inputs, but only 64 neurons are used in the hidden layer, which is far less than the number of eigenvalues. Why is this?
2. Would it be better to use 256 neurons in the hidden layer 1?
