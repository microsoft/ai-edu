<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 12.2 Gradient check

### 12.2.1 Why is a gradient check necessary?

The neural network algorithm uses backpropagation to calculate the gradient of the objective function for each parameter, which can be considered the analytic gradient. Due to the large number of parameters involved in the computation process, the gradient calculated by backpropagation implemented in code is prone to have errors, resulting in the final iteration yielding poorly valid parameter values.

The gradient check method can be used to confirm whether the gradient calculated by backpropagation in the code is correct. An approximate value of the gradient is obtained and then compared with the backpropagation gradient by calculating the numerical gradient. If the difference between the two is slight, the backpropagation code is proved to be correct.

### 12.2.2 Numerical differentiation

#### Recall the concept of derivative

$$
f'(x)=\lim_{h \to 0} \frac{f(x+h)-f(x)}{h} \tag{1}
$$

The implication is how much a slight change in $x$ ($h$ is an infinitely small value) will cause the function $f(x)$ to vary. This can be implemented in `Python` as follows.

```Python
def numerical_diff(f, x):
    h = 1e-5
    d = (f(x+h) - f(x))/h
    return d
```

Because of the rounding error of the computer, `h` cannot be too small, such as `1e-10`, which will cause errors in the calculation results, so we usually use values between `[1e-4,1e-7]`.

But there will be a problem if we use the above method, as shown in Figure 12-4.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/12/grad_check.png" ch="500" />

Figure 12-4 Numerical differentiation method

The solid red line is the real derivative tangent line, and the blue dashed line implements the above method, i.e., a straight line is drawn from $x$ to $x+h$ to simulate the actual derivative. But the slopes of the solid red line and the blue dashed line are not equal. Therefore, we usually use the green dashed line to simulate the actual derivative, and the formula becomes:

$$
f'(x) = \lim_{h \to 0} \frac{f(x+h)-f(x-h)}{2h} \tag{2}
$$

Equation 2 is known as the bilateral approximation method.

Using the bilateral approximation form will result in an error about 100 to 10,000 times smaller than the unilateral approximation form, demonstrated using Taylor's Formula.

#### Taylor's Formula

Taylor's formula is a way to approximate a function $f(x)$ with nth order derivative at $x=x_0, using nth order polynomial about $(x-x_0)$. If the function $f(x)$ has nth order derivative on some closed interval $[a,b]$ containing $x_0$ and $n+1$ order derivative on the open interval $(a,b)$, then for any point $x$ on the closed interval $[a,b]$, the following equation holds:

$$f(x)=\frac{f(x_0)}{0!} + \frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2 + ...+\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x) \tag{3}$$

where,$f^{(n)}(x)$ denotes the $n$th order derivative of $f(x)$, the polynomial after the equal sign is called the Taylor expansion of the function $f(x)$ at $x_0$, and the remaining $R_n(x)$ is the remainder term of the Taylor‘’s formula, which is a higher order infinitesimal of $(x-x_0)^n$. 

Using the Taylor expansion formula, such that $x = \theta + h, x_0 = \theta$, we have:

$$f(\theta + h)=f(\theta) + f'(\theta)h + O(h^2) \tag{4}$$

#### Unilateral Approximation Error

If a unilateral approximation is used, divide both sides of Eq. 4 by $h$ and then deform:

$$f'(\theta) + O(h)=\frac{f(\theta+h)-f(\theta)}{h} \tag{5}$$

Equation 5 is already very close to the definition of Equation 1, except that the second term on the left is the approximation error, which is an $O(h)$ level error term.

#### Bilateral Approximation Error

If the bilateral approximation is used, we use the third-order Taylor expansion:

Let $x=\theta + h, x_0=\theta$, We can get:

$$f(\theta + h)=f(\theta) + f'(\theta)h + f''(\theta)h^2 + O(h^3) \tag{6}$$

then let $x=\theta - h, x_0=\theta$ we can get:

$$f(\theta - h)=f(\theta) - f'(\theta)h + f''(\theta)h^2 - O(h^3) \tag{7}$$

Equation 6 minus Equation 7 gives:

$$f(\theta + h) - f(\theta - h)=2f'(\theta)h + 2O(h^3) \tag{8}$$

Dividing both sides by $2h$:

$$f'(\theta) + O(h^2)={f(\theta + h) - f(\theta - h) \over 2h} \tag{9}$$

In Equation 9, the second term on the left-hand side, which is the error of the bilateral approximation, is an $O(h^2)$ level error term, which is much less than the error term in Equation 5.

### 12.2.3 Example Illustration

Equation 2 is the theoretical basis for gradient checking. For example, a function:

$$f(x) = x^2 + 3x$$

Let us look at its numerical differentiation at $x=2$ such that $h = 0.001$.

$$
\begin{aligned}
f(x+h) &= f(2+0.001) \\\\
&= (2+0.001)^2 + 3 \times (2+0.001) \\\\
&=10.007001
\end{aligned}
$$
$$
\begin{aligned}
f(x-h) &= f(2-0.001) \\\\
&= (2-0.001)^2 + 3 \times (2-0.001) \\\\
&=9.993001
\end{aligned}
$$
$$
\frac{f(x+h)-f(x-h)}{2h}=\frac{10.007001-9.993001}{2 \times 0.001}=7 \tag{10}
$$

Looking again at its mathematical analytic solution:

$$f'(x)=2x+3$$
$$f'(x|_{x=2})=2 \times 2+3=7 \tag{11}$$

It can be seen that the results of Equation 10 and Equation 11 are identical. Of course, in practical applications, it is generally not possible to be exactly equal. As long as the error between the two is less than `1e-5`, we consider that the accuracy requirements are met.

### 12.2.4 Algorithm Implementation

In a neural network, we assume that the cross-entropy function using multiple classifications takes the form of:

$$J(w,b) =- \sum_{i=1}^m \sum_{j=1}^n y_{ij} \ln a_{ij}$$

m is the number of samples and n is the number of categories.

#### Parameter vectorization

We need to check the gradient about $W$ and $B$, and $W$ and $B$ are several matrices, not a scalar. So before the gradient check, we do the preparation work, which is to vectorize the matrices $W$ and $B$, and then join (concatenate) the vectorized $W$ and $B$ of all layers in the neural network into one large vector, which we call $J(\theta)$. We then do the same transformation on the results $d\theta_{real}$ of the derivatives of $W$ and $B$ obtained by the back-prop process, and then we will start to do the check.

After the vectorized $W,B$ are connected, they are uniformly called $\theta$ and distinguished by different subscripts in sequence, so we have the expression of $J(\theta)$ as:

$$J(w,b)=J(\theta_1,...,\theta_i,...,\theta_n)$$

For each vector in the above equation, we do the check sequentially using Equation 2, and so have the gradient check formula for the ith vector value:

$$\frac{\partial J}{\partial \theta_i}=\frac{J(\theta_1,...,\theta_i+h,...,\theta_n) - J(\theta_1,...,\theta_i-h,...,\theta_n)}{2h}$$

Since we are trying to compare the difference of the corresponding components of two vectors, this can be characterized by the square root of the sum squares of the difference between the corresponding components(Euclidean distance). But we do not want to get a specific value that inscribes the difference, but we want to get a ratio, which also facilitates a standard gradient check requirement.

Why do we say so? In fact, we can think of it this way: assuming that at the beginning of the iteration, the gradient of the parameters is very large, and with continuous iteration until convergence, the gradient of the parameters gradually converges to 0, that is, getting smaller and smaller. The numerator (Euclidean distance) is related to the value of the gradient, with the increase in the number of iterations will also be reduced. In the iteration process, there is no fixed standard to judge the effect of the gradient examination using only the numerator. But with the addition of a denominator, the sum of squares of the gradient is taken into account, the large value is compared to the large value, and the small value is compared to the small value, we can get a ratio. We can also get a definite standard to measure the effect of the gradient examination.

#### Algorithm

1. initialize all matrix parameters of the neural network (you can use random initialization or other non-zero initialization methods)
2. transform all layers of $W, B$ into vectors and sequentially store them in $\theta$.
3. set $X$ values randomly, preferably after normalization, between [0,1]
4. perform a feed-forward calculation, followed by a backward calculation to get the gradient $d\theta_{real}$ of each parameter
5. change the obtained gradient $d\theta_{real}$ into a vector form, whose dimensions should be the same as $\theta$ in step 2 and correspond to each other ($W$ corresponds to $dW$, $B$ corresponds to $dB$)
6. perform a bilateral approximation for each value of the $\theta$ vector in step 2 to obtain $d\theta_{approx}$
7. compare the values of $d\theta_{real}$ and $d\theta_{approx}$ by computing the Euclidean distance between the two vectors:
   
$$diff = \frac{\parallel d\theta_{real} - d\theta_{approx}\parallel_2}{\parallel d\theta_{approx}\parallel_2 + \parallel d\theta_{real}\parallel_2}$$

Result judgment:

1. $diff > 1e^{-2}$
   
   Something must have gone wrong with the gradient calculation.

2. $1e^{-2} > diff > 1e^{-4}$
   
   There may be a problem, and it needs to be checked.

3. $1e^{-4} \gt diff \gt 1e^{-7}$
   
   This is acceptable for non-smooth activation functions, but the result is too high if smooth activation functions such as tanh nonlinearities and softmax are used.

4. $1e^{-7} \gt diff$
   
   You should go grab a cup of tea and celebrate.

Also, note that as the depth of the network increases, the error accumulates. If a 10-layer network is used and the relative error obtained is `1e-2`, this result is also acceptable.
  
### 12.2.5 注意事项

1. first of all, do not use gradient check to train, which is not use the gradient check method to calculate the gradient because this is too slow. In the training process, we still use backpropagation to calculate the parameter gradient and use the gradient test to debug and check whether the backpropagation process is accurate.

2. Second, if we find a problem with the backpropagation process during the gradient check, we need to compute all the parameters to determine the source of the computational deviation. It may be a problem in solving $B$ or in solving $W$ at a certain level. The gradient check can help us determine the range where the problem occurs to help us debug it.

3. Don't forget the regularization. When using backpropagation to compute the parameter gradient,  do not forget that the form of the gradient has changed and add the regularization part if we add an L2 norm regularization. Similarly, when performing the gradient test, remember that the objective function $J$ has changed.

4. Note that if we use drop-out regularization, the gradient check is not available. Why is that? Because we know that the drop-out method retains some nodes randomly with a specific retention probability. Because of its randomness, the form of the objective function $J$ becomes very unclear, and then we can no longer use the gradient check to test backpropagation. If we have to use drop-out and want to test backpropagation, we can first set the retention probability to 1, i.e., retain all nodes, then use the gradient test to test the backpropagation process. If there is no problem, we can change the value of the retention probability to apply drop-out.

5. Finally, we introduce a particularly rare case. At the beginning of initializing W and b, the values of both W and b are still small, at which point the backpropagation process has no problem. As the iterative process proceeds and the values of $W$ and $B$ become larger and larger, the backpropagation process may have problems and an increasingly large gradient difference. To avoid this situation, we need to perform the gradient check several times. For example, once at the beginning, when initializing the weights, use the gradient check to verify the backpropagation process after iterating for some time.

### Code location

ch12, Level2

### Thinking and Exercises

1. Change any line of the backpropagation code and then run the gradient checking code to see the result.
