
<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 14.6 Multi-classification task - MNIST handwriting recognition

### 14.6.1 Reading Data

MNIST data is in the format of images, so we use `mode="vector"` to read it and convert it into the format of vectors.

```Python
def LoadData():
    print("reading data...")
    dr = MnistImageDataReader(mode="vector")
    ......
```

### 14.6.2 Build a model

A total of 4 hidden layers are connected with the ReLU activation function, and the last output layer is connected with the Softmax classification function.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/mnist_net.png" />

Figure 14-18 The abstract model for completing the MNIST classification task

The following are the main parameter settings:

```Python
if __name__ == '__main__':
    dataReader = LoadData()
    num_feature = dataReader.num_feature
    num_example = dataReader.num_example
    num_input = num_feature
    num_hidden1 = 128
    num_hidden2 = 64
    num_hidden3 = 32
    num_hidden4 = 16
    num_output = 10
    max_epoch = 10
    batch_size = 64
    learning_rate = 0.1

    params = HyperParameters_4_0(
        learning_rate, max_epoch, batch_size,
        net_type=NetType.MultipleClassifier,
        init_method=InitialMethod.MSRA,
        stopper=Stopper(StopCondition.StopLoss, 0.12))

    net = NeuralNet_4_0(params, "MNIST")

    fc1 = FcLayer_1_0(num_input, num_hidden1, params)
    net.add_layer(fc1, "fc1")
    r1 = ActivationLayer(Relu())
    net.add_layer(r1, "r1")
    ......
    fc5 = FcLayer_1_0(num_hidden4, num_output, params)
    net.add_layer(fc5, "fc5")
    softmax = ClassificationLayer(Softmax())
    net.add_layer(softmax, "softmax")

    net.train(dataReader, checkpoint=0.05, need_test=True)
    net.ShowLossHistory(xcoord=XCoordinate.Iteration)
```

### 14.6.3 Results of execution

The stop condition we chose is when the absolute loss value reaches 0.12. When the process reaches 6 epochs, the loss value of 0.119 is reached, and the training stops.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/14/mnist_loss.png" />

Figure 14-19 Changes in loss function value and accuracy during training

Figure 14-19 is an illustration of the training process. Below is the output of the last few lines.

```
......
epoch=6, total_iteration=5763
loss_train=0.005559, accuracy_train=1.000000
loss_valid=0.119701, accuracy_valid=0.971667
time used: 17.500738859176636
save parameters
testing...
0.9697
```

Finally, the accuracy rate obtained with the test set is 96.97%.

### Code Location

ch14, Level6
