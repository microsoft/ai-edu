<!--Copyright Â© Microsoft Corporation. All rights reserved.
  Applicable to [License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md) copyright license-->

## 15.1 Weight matrix initialization

The initialization of the weight matrix is a very important concept, and it is the first step in training the neural network. Choosing the correct initialization method will bring more results with less effort. This is like climbing the Himalayas. If you choose to climb from the southern slope, it will be much easier than from the northern slope. The initialization of the weight matrix is equivalent to choosing a different path when going down the mountain. You don't know the difficulty of this path before you choose, but know that it can reach the bottom of the mountain. This choice is random. Even if you use the correct initialization algorithm, each reinitialization will have a large impact on the training results.

For example, the weight value obtained during the first initialization is (0.12847, 0.36453), and the second initialization obtained (0.23334, 0.24352). After experimentation, the first initialization took 3000 iterations to reach a model with an accuracy of 96%. It only took 2000 iterations for the other initialization to reach the same accuracy. This exercise is common in practice.

### 15.1.1 Zero initialization

That is, the initial value of `W` in all layers is set to 0.

$$
W = 0
$$

But for multi-layer networks, it must not be initialized with zero. Otherwise, the weight value cannot learn and become a reasonable result. Look at the following printout of a zero-initialized weight matrix's values:
```
W1= [[-0.82452497 -0.82452497 -0.82452497]]
B1= [[-0.01143752 -0.01143752 -0.01143752]]
W2= [[-0.68583865]
 [-0.68583865]
 [-0.68583865]]
B2= [[0.68359678]]
```

The values of the three elements inside `W1`, `B1`, and `W2` are all the same. This is because the initial values are all 0. So, the gradient is returned uniformly, causing all the values of `W` to be updated synchronously. In this case, no matter how many rounds, the final result will not be correct.

### 15.1.2 Standard initialization

The standard/normal initialization method guarantees that the input mean of the activation function is 0 and the variance is 1. Initialize `W` according to the following formula:

$$
W \sim N \begin{bmatrix} 0, 1 \end{bmatrix}
$$

`W` is the weight matrix, `N` represents the Gaussian distribution. The Gaussian distribution is also called the normal distribution, so this initialization is also called normal initialization in some places.

The details of initialization are generally determined according to the number of inputs and outputs of the fully connected layer:

$$
W \sim N
\begin{pmatrix} 
0, \frac{1}{\sqrt{n_{in}}}
\end{pmatrix}
$$

$$
W \sim U
\begin{pmatrix} 
-\frac{1}{\sqrt{n_{in}}}, \frac{1}{\sqrt{n_{in}}}
\end{pmatrix}
$$

When the target problem is relatively simple, the network depth is not large, so standard initialization is sufficient. However, when using DNNs, you will encounter the problem shown in Figure 15-1.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/15/init_normal_sigmoid.png" ch="500" />

Figure 15-1 The performance of standard initialization on the Sigmoid activation function

Figure 15-1 is a 6-layer DNN, using a fully connected layer and Sigmoid activation function. The figure shows the histogram of the activation function of each layer. Note that the activation values of each layer are close to [0,1] on both sides. From the curve of the Sigmoid function, the derivative of these values is close to 0, and the gradient of the back propagation gradually disappears. The value in the middle section is relatively small, which is not favorable for parameter learning.

### 15.1.3 Xavier initialization method

Based on the above observations, Xavier Glorot et al. developed the following Xavier$^{[1]}$ initialization method.

Condition: During forward propagation, the variance of the activation value remains unchanged; during back propagation, the variance of the gradient of the state value remains unchanged.

$$
W \sim N
\begin{pmatrix}
0, \sqrt{\frac{2}{n_{in} + n_{out}}} 
\end{pmatrix}
$$

$$
W \sim U 
\begin{pmatrix}
 -\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}} 
\end{pmatrix}
$$

`W` is the weight matrix, `N` represents the normal distribution, and `U` represents the uniform distribution.

It is assumed that the activation function is symmetrical about 0 and is mainly aimed at fully connected neural networks - suitable for tanh and softsign.

That is, the weight matrix parameters should satisfy the uniform distribution in this domain.

Abstract of the paper: The neural network can't work as desired before 2006, and the main reason is the weight matrix initialization method. The Sigmoid function is not suitable for deep learning because it will cause gradient saturation. Based on the above reasons, we propose a parameter initialization method that can converge quickly.

The Xavier initialization method has the advantage of initializing W directly with Gaussian distribution: 

In general neural networks, the variance of neuron output values will continue to increase during forward propagation, and the use of Xavier and other methods can theoretically ensure that the input and output variances of each layer of neurons are consistent. 

Figure 15-2 shows the performance of the network with a depth of 6 layers. Note that the distribution of the activation function output values of the following layers conform to the normal distribution, which is conducive to the learning of the neural network.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/15/init_xavier_sigmoid.png" ch="500" />

Figure 15-2 The performance of Xavier initialization on the Sigmoid activation function

Table 15-1 Comparison of the activation values of each layer of random initialization and Xavier initialization with the back propagation gradient

| |Activation value of each layer|Back propagation gradient of each layer|
|---|---|---|
| Random initialization|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\15\forward_activation1.png"><br/>The activation value distribution gradually concentrates|<img src= "https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\15\backward_activation1.png"><br/>The strength of backpropagation declines layer by layer|
| Xavier initialization|<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\15\forward_activation2.png"><br/>The activation value is evenly distributed|<img src=" https://aiedugithub4a2.blob.core.windows.net/a2-images\Images\15\backward_activation2.png"><br/>The strength of backpropagation remains unchanged|

However, with the advancement of deep learning, people felt that the reverse power of Sigmoid was limited. So, the ReLU activation function was invented. Figure 15-3 shows the performance of Xavier initialization on the ReLU activation function.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/15/init_xavier_relu.png" ch="500" />

Figure 15-3 The performance of Xavier initialization on the ReLU activation function

Note that as the layer deepens, the activation value is gradually biased towards 0 when using ReLU, which will also cause the problem of gradient disappearance. So He Kaiming et al. developed the MSRA initialization method, also called He initialization method.

### 15.1.4 MSRA initialization method

The MSRA initialization method $^{[2]}$ is also called the He method because the author's last name is He.

Condition: During forward propagation, the variance of the state value remains unchanged; during back propagation, the variance of the gradient of the activation value remains unchanged.

Network initialization is a very important thing. However, the traditional initialization of Gaussian distribution with fixed variance makes it difficult for the model to converge when the network becomes deeper. The VGG team handled the initialization problem in this way: they first trained an 8-layer network, and then used this network to initialize a deeper network.

"Xavier" is a relatively good initialization method. However, when Xavier derived it, it is assumed that the activation function is linear near the zero point. Obviously, ReLU and PReLU that we currently use do not have this condition. Therefore, with MSRA initialization, the variance will change after using the ReLU activation function, so the method of initializing the weights should also be changed.

When only considering the number of inputs, the MSRA initialization is a Gaussian distribution with a mean value of 0 and a variance of 2/n, which is suitable for the ReLU activation function:

$$
W \sim N 
\begin{pmatrix} 
0, \sqrt{\frac{2}{n}} 
\end{pmatrix}
$$

$$
W \sim U 
\begin{pmatrix} 
-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{out}}} 
\end{pmatrix}
$$

The distribution of activation values from 0 to 1 in Figure 15-4 is very uniform in each layer, and the gradient will not disappear due to the deepening of the layer. Therefore, when using ReLU, it is recommended to use the MSRA method for initialization.

<img src="https://aiedugithub4a2.blob.core.windows.net/a2-images/Images/15/init_msra_relu.png" ch="500" />

Figure 15-4 The performance of MSRA initialization on the ReLU activation function

For Leaky ReLU:

$$
W \sim N \begin{bmatrix} 0, \sqrt{\frac{2}{(1+\alpha^2) \hat n_i}} \end{bmatrix}
\\\\ \hat n_i = h_i \cdot w_i \cdot d_i
\\\\ h_i: height of convolution kernel, w_i: width of convolution kernel, d_i: number of convolution kernel
$$

### 15.1.5 Summary

Table 15-2 Application scenarios of several initialization methods

|ID|Network Depth|Initialization Method|Activation Function|Description|
|---|---|---|---|---|
|1|Single layer|Zero initialization|None|Yes|
|2|Double Layer|Zero Initialization|Sigmoid|Error, unable to carry out correct backpropagation|
|3|Double Layer|Random Initialization|Sigmoid|Yes|
|4|Multilayer|Random initialization|Sigmoid|The activation value is distributed in a concave shape, which is not conducive to back propagation|
|5|Multilayer|Xavier initialization|Tanh|Correct|
|6|Multilayer|Xavier initialization|ReLU|The activation value distribution is biased towards 0, which is not conducive to back propagation|
|7|Multilayer|MSRA Initialization|ReLU|Correct|

From Table 15-2, due to changes in network depth and activation functions, people continue to study new initialization methods to adapt and finally get combinations 1, 3, 5, and 7.

### Code location

ch15, Level1

### Thinking and Practice

1. When there are multiple layers, initialization cannot be with zeros. But if all the values of the weight matrix are initialized to 0.1, is it okay?
2. Use the example in 14.6 to compare the training effects of Xavier and MSRA initialization.

### Reference

[1] Understanding the difficulty of training deep feedforward neural networks. link: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf

by Xavier Glorot, Yoshua Bengio in AISTATS 2010.

[This is the Chinese version] (https://blog.csdn.net/victoriaw/article/details/73000632), thanks to the translator.

[2] He Kaiming, Microsoft Research Asia, 2015. https://arxiv.org/pdf/1502.01852.pdf
