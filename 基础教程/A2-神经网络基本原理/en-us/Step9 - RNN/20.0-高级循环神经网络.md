<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

# 第20章 高级循环神经网络

## 20.0 高级循环神经网络概述

### 20.0.1 传统循环神经网络的不足

在上一章中，介绍了循环神经网络的由来和发展历史，传统循环神经网络的原理和应用。循环神经网络弥补了前馈神经网络的不足，可以更好的处理时序相关的问题，扩大了神经网络解决问题的范围。

但传统的循环神经网络也有自身的缺陷，由于容易产生梯度爆炸和梯度消失的问题，导致很难处理长距离的依赖。传统神经网络模型，不论是一对多、多对一、多对多，都很难处理不确定序列输出的问题，一般需要输出序列为1，或与输入相同。在机器翻译等问题上产生了局限性。

### 20.0.2 高级循环神经网络简介

针对上述问题，科学家们对普通循环神经网络进行改进，以便处理更复杂场景的数据的模型，提出了如LSTM, GRU, Seq2Seq等模型。此外，注意力（Attention）机制的引入，使得Seq2Seq模型的性能得到提升，关于Attention的内容，目前没有列入章节，以后将会进行补充。

下面简单介绍本章将会讲解的三种网络模型。

#### 长短时记忆网络（LSTM）
长短时记忆网络（LSTM）是最先提出的改进算法，由于门控单元的引入，从根本上解决了梯度爆炸和消失的问题，使网络可以处理长距离依赖。

#### 门控循环单元网络（GRU）
LSTM网络结构中有三个门控单元和两个状态，参数较多，实现复杂。为此，针对LSTM提出了许多变体，其中门控循环单元网络是最流行的一种，它将三个门减少为两个，状态也只保留一个，和普通循环神经网络保持一致。

#### 序列到序列网络（Sequence-to-Sequence）
LSTM与其变体很好地解决了网络中梯度爆炸和消失的问题。但LSTM有一个缺陷，无法处理输入和输出序列不等长的问题，为此提出了序列到序列（Sequence-to-Sequence, 简称Seq2Seq）模型，引入和编码解码机制（Encoder-Decoder），在机器翻译等领域取得了很大的成果，进一步提升了循环神经网络的处理范围。




