<!--Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可-->

## 20.1 LSTM基本原理

以下为本小节目录，详情请参阅《智能之门》正版图书，高等教育出版社。

### 20.1.1 提出问题

循环神经网络（RNN）的提出，使神经网络可以训练和解决带有时序信息的任务，大大拓宽了神经网络的使用范围。但是原始的RNN有明显的缺陷。不管是双向RNN，还是深度RNN，都有一个严重的缺陷：训练过程中经常会出现梯度爆炸和梯度消失的问题，以至于原始的RNN很难处理长距离的依赖。

#### 从实例角度

例如，在语言生成问题中：

佳佳今天帮助妈妈洗碗，帮助爸爸修理椅子，还帮助爷爷奶奶照顾小狗毛毛，大家都夸奖了 $\underline{\qquad\quad}$。

例句中出现了很多人，空白出要填谁呢？我们知道是“佳佳”，传统RNN无法很好学习这么远距离的依赖关系。

#### 从理论角度

根据循环神经网络的反向传播算法，可以得到任意时刻k, 误差项沿时间反向传播的公式如下：
$$
\delta^T_k=\delta^T_t \prod_{i=k}^{t-1} diag[f'(z_i)]W
$$

其中 $f$为激活函数，$z_i$为神经网络在第$i$时刻的加权输入， $W$为权重矩阵，$diag$表示一个对角矩阵。

注意，由于使用链式求导法则，式中有一个连乘项 $\prod_{i=k}^{t-1} diag[f'(z_i)]W$ , 如果激活函数是挤压型，例如 $Tanh$ 或 $sigmoid$ , 他们的导数值在 [0,1] 之间。我们再来看$W$。
1. 如果$W$的值在 (0,1) 的范围内， 则随着$t$的增大，连乘项会越来越趋近于0， 误差无法传播，这就导致了 **梯度消失** 的问题。
2. 如果$W$的值很大，使得$diag[f'(z_i)]W$的值大于$1$， 则随着$t$的增大，连乘项的值会呈指数增长，并趋向于无穷，产生 **梯度爆炸**。

梯度消失使得误差无法传递到较早的时刻，权重无法更新，网络停止学习。梯度爆炸又会使网络不稳定，梯度过大，权重变化太大，无法很好学习，最坏情况还会产生溢出（NaN）错误而无法更新权重。

#### 解决办法

为了解决这个问题，科学家们想了很多办法。

1. 采用半线性激活函数ReLU代替 挤压型激活函数，ReLU函数在定义域大于0的部分，导数恒等于1，来解决梯度消失问题。
2. 合理初始化权重$W$，使$diag[f'(z_i)]W$的值尽量趋近于1，避免梯度消失和梯度爆炸。

上面两种办法都有一定的缺陷，ReLU函数有自身的缺点，而初始化权重的策略也抵不过连乘操作带来的指数增长问题。要想根本解决问题，必须去掉连乘项。

科学家们冥思苦想，终于提出了新的模型 —— 长短时记忆网络（LSTM）。


### 20.1.2 LSTM网络
