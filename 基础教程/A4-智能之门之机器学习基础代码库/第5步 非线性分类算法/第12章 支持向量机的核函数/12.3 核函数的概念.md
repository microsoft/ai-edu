
## 12.3 核函数的概念

### 12.3.1 更高维的映射与内积计算

在 12.1 小节中，$x$ 从一维的 $[x_1]$ 变成了二维 $[x_1, x_1^2]$，映射函数为 $\phi(z) = [z, z^2]$。

在 12.2 小节中，$x$ 从二维的 $[x_{1}, x_{2}]$ 变成了三维的 $[x_{1}, x_{2}, x_{1}^2+x_{2}^2]$，映射函数为 $\phi(z) = [z_1, z_2, z_1^2+z_2^2]$。当然，也可以定义 $\phi(z)=[z_1,z_2,z_1^2,z_2^2,z_1 z_2]$，那就变成了五维特征。

现在看看三维的例子，然后可以推广到一般情况。

当样本有三维特征时，其一般形式为：$\boldsymbol{z}=[z_1,z_2,z_3]$。下面定义只包括二次项的映射函数：

$$
\phi(z)=[z_1^2,\ z_1 z_2,\ z_1 z_3,\ z_2 z_1,\ z_2^2,\ z_2 z_3,\ z_3 z_1,\ z_3 z_2,\ z_3^2]
\tag{12.3.1}
$$

其中 $z_i z_j$ 表示 $z_i$ 乘以 $z_j$，不考虑合并 $z_i z_j$ 与 $z_j z_i$ 项。

现在有两个样本：$\boldsymbol{x}_i=[1,2,3]$，$\boldsymbol{x}_j=[4,5,6]$。经过式 12.3.1 的映射后变成：

$\phi(\boldsymbol{x}_i)=[1,2,3,2,4,6,3,6,9]$

$\phi(\boldsymbol{x}_j)=[16,20,24,20,25,30,24,30,36]$


现在计算 $\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j)$ 的内积......稍等，有的读者会问：
1. 为什么要计算内积？
2. 为什么要计算映射函数 $\phi(\boldsymbol{x}_i)$ 和 $\phi(\boldsymbol{x}_j)$ 的内积？

因为式 SVM 的优化目标函数如下：
$$
D(\alpha)=\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i \alpha_j y_i y_j (\boldsymbol{x}_i \cdot \boldsymbol{x}_j) \tag{12.3.2}
$$

它需要计算样本数据 $(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)$ 的内积，这就回答了第 1 个问题，为什么要计算内积。

那为什么要计算映射函数 $\phi(\boldsymbol{x}_i)$ 和 $\phi(\boldsymbol{x}_j)$ 的内积呢？因为从 12.1 节和 12.2 节的两个例子中，我们已经得到了一个初步的认识：当样本在原始空间线性不可分时，如果映射到高维空间，是有可能变成线性可分的；而且维数越高，线性可分的可能性越大。**在映射之后，就可以使用 SVM 来做线性分类了，只不过要把原来的 $(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)$ 替换成 $\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j)$。**

好，可以继续了，接下来计算内积：
$$
\begin{aligned}
\phi(\boldsymbol{x}_i ) \cdot \phi(\boldsymbol{x}_j )&=[1,2,3,2,4,6,3,6,9] \cdot [16,20,24,20,25,30,24,30,36]
\\\\
&=1 \times 16 + 2  \times 20 + 3  \times 24 + 2  \times 20 + 4  \times 25 + 6  \times 30 + 3  \times 24 + 6  \times 30 + 9  \times 36
\\\\
&=16+40+72+40+100+180+72+180+324
\\\\
&=1024
\end{aligned}
\tag{12.3.3}
$$


可以看到式 12.3.3 的计算过程相当繁琐：
1. 首先要把原始是三维特征映射为九维特征。
2. 然后要在高维空间（九维）中计算内积。

实现上述过程的代码如下：

```python
def f(z):
    n = z.shape[1]
    Z = np.zeros(shape=(1,n*n))
    # 生成九维特征
    for i in range(3):
        for j in range(3):
            Z[0,i*3+j] = z[0,i]*z[0,j]
    return Z
```
计算映射后的内积计算代码如下：

```python
def fx_fy(x,y):
    fx = f(x)
    fy = f(y)
    result = np.inner(fx, fy)
    return result
```

幸好有np.inner() 函数可以帮助我们计算两个矢量的内积，否则要还要手工实现。

实际上，我们在代码 Code_12_1_1D.py 中已经实践过一次上述过程了：

```python
    X = transform(X_raw)
    model = linear_svc(X, Y)
```

第一行的 transform() 函数，就是做了高维特征映射；第二行代码把新的 X 特征样本送进了线性 SVM 分类器中做内积运算，从而得到模型 model。

### 12.3.2 核函数的出现

我们对式 12.3.3 进行一下反向推导：

$$
\begin{aligned}
\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j) &= [x_{i1}^2,\ x_{i1} x_{i2},\ x_{i1} x_{i3},\ x_{i2} x_{i1},\ x_{i2}^2,\ x_{i2} x_{i3},\ x_{i3} x_{i1},\ x_{i3} x_{i2},\ x_{i3}^2]
\\\\
& \cdot [x_{j1}^2,\ x_{j1} x_{j2},\ x_{j1} x_{j3},\ x_{j2} x_{j1},\ x_{j2}^2,\ x_{j2} x_{j3},\ x_{j3} x_{j1},\ x_{j3} x_{j2},\ x_{j3}^2]
\\\\
&=x_{i1}^2 x_{j1}^2 + x_{i2}^2 x_{j2}^2 + x_{i3}^2 x_{j3}^2 + 2 x_{i1} x_{j1} x_{i2} x_{j2}+ 2 x_{i1} x_{j1} x_{i3} x_{j3} + 2 x_{i2} x_{j2} x_{i3} x_{j3}
\\\\
&=(x_{i1} x_{j1} + x_{i2} x_{j2} + x_{i3} x_{j3})^2
\\\\
&=[(x_{i1},\ x_{i2},\ x_{i3}) \cdot (x_{j1},\ x_{j2},\ x_{j3})]^2
\\\\
&=(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)^2
\end{aligned}
\tag{12.3.4}
$$

所以，如果令：

$$
K(\boldsymbol{x}_i,\boldsymbol{x}_j)=\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j)=(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)^2 \tag{12.3.5}
$$

则式 12.3.5 就是核函数的一种表达方式。

将 $\boldsymbol{x}_i=[1,2,3],\boldsymbol{x}_j=[4,5,6]$ 的具体值带入式 12.3.5 进行验证：

$$
\begin{aligned}   
K(\boldsymbol{x}_i,\boldsymbol{x}_j)&=K([1,2,3],[4,5,6])=([1,2,3] \cdot [4,5,6])^2
\\\\
&=(1 \times 4 + 2 \times 5 + 3 \times 6)^2 = (4+10+18)^2=32^2
\\\\
&=1024
\end{aligned}
\tag{12.3.6}
$$

结果与式 12.3.3 的计算结果相同。

用代码直接实现核函数的话，会非常简单：

```python
def k(x,y):
    result = np.inner(x, y)**2
    return result
```

可以做一个对比，分别调用 12.3.1 小节中的 fx_fy(x,y) 函数和上面的 k(x,y) 函数各 100000 次，在笔者的电脑上，运行代码 Code_12_3_Mapping.py，得到如下运行信息：

```
原始输入维数: 3
映射空间维数: 9
[[1024.]] [[1024]]
fx_fy(x,y)运行时间: 1.4461944103240967
K(x,y)运行时间: 0.190537691116333
```

- 首先获得原始特征维数为 3，而映射后的特征维数为 9；
- 其次确定二者的计算结果都是 1024，相等；
- 最后比较运行时间，用映射函数的内积运算，需要 1.4 秒；而直接计算核函数只需要 0.19 秒。后者比前者快一个数量级。这里有两个原因：

    1. 函数 $\phi(z)$ 生成映射特征时需要花费时间；
    2. 映射后从 3 维特征变成了 9 维特征，计算内积需要花费更多的时间。

这就是核函数的优势所在：它不需要把原始样本特征映射到高维空间后再处理，而是直接在原始空间中做内积运算得到一个标量，然后再简单地做一个平方运算（后面会学习到除了平方运算以外的其它后续运算）。

> 以上结果请运行 Code_12_3_Mapping.py 得到。

### 12.3.3 核函数与映射函数的关系

核函数与映射函数并非是一一对应的，以 $K(x_i,x_j)=(x_i,x_i)^2$ 举例来说，三维空间（即生成三维特征，与平方计算无关）的映射函数可能是：

$$
\phi(z) = [z_1^2,\ \sqrt{2}z_1 z_2, \ z_2^2]
\tag{12.3.7}
$$

即 $(x_{i1}^2,x_{i1}x_{i2},x_{i2}^2) · (x_{j1}^2,x_{j1}x_{j2},x_{j2}^2)=x_{i1}^2 x_{j1}^2+2x_{i1}x_{i2}x_{j1}x_{j2}+x_{i2}^2 x_{j2}^2=(x_{i1} x_{j1}+x_{i2} x_{j2})^2=(x_i · x_j)^2=K(x_i,x_j)$ 。

也可能是：

$$
\phi(z) = \frac{1}{\sqrt{2}} [z_1^2-z_2^2,\ 2z_1 z_2, \ z_1^2+z_2^2]
\tag{12.3.8}
$$

如果映射到四维空间，将会是：

$$
\phi(z)=[z_1^2, \ z_1 z_2, \ z_1 z_2,\ z_2^2]
\tag{12.3.9}
$$

从另一方面讲，$[z_{1},z_{2}]$ 可以映射成：

- 二维：$[z_{1}^2,\ z_{2}^2]$ 或 $[z_{1}^2+z_{2}^2,\ z_1 z_2]$
- 三维：$[z_{1},\ z_{2},\ z_{1}^2+ z_{2}^2]$ 或 $[z_{1},\ z_{2},\ z_{1} z_{2}]$
- 四维：$[z_{1}^2,\ z_{2}^2,\ z_{1},\ z_{2}]$
- 五维：$[z_{1}^2,\ z_{2}^2,\ z_{1} z_{2},\ z_{1},\ z_{2}]$

所对应的核函数都不一样，有些有可能没有对应的核函数。

最麻烦的事情是，从一个 3 维的原始特征 $[z_{1},z_{2},z_{3}]$ 向量出发，如果最多只构造二次项的话，新的特征空间会有 9 维：

$$
\phi(x)=[\underbrace{ z_{1},z_{2},z_{3}}_{1次项3维}, \underbrace{z_{1}^2,z_{2}^2,z_{3}^2,z_{1} z_{2}, z_{1} z_{3}, z_{2} z_{3}}_{2次项6维}]
$$


如果最多构造三次项的话，可以映射成 19 维的新特征空间：

$$
\phi(x)=[\underbrace{ z_{1},z_{2},z_{3}}_{1次项3维}, \underbrace{z_{1}^2,z_{2}^2,z_{3}^2,z_{1} z_{2}, z_{1} z_{3}, z_{2} z_{3}}_{2次项6维},\underbrace{z_{1}^3,z_{2}^3,z_{3}^3,z_{1}^2 z_{2},z_{1}^2 z_{3},z_{2}^2 z_{1},z_{2}^2 z_{3},z_{3}^2 z_{1},z_{3}^2 z_{2}, z_1z_2z_3}_{3次项10维}]
$$

所以，映射函数只是我们理解核函数工作原理的一种方式，而不是要做实际的映射运算，否则多特征值加上高维空间的特征映射，如果样本量很大的话，将会带来巨大的计算开销。

### 12.3.4 常用核函数

什么样的函数可以当作是核函数呢？

**Mercer 定理：任何半正定的函数都可以作为核函数。**

所谓半正定的函数 $f(x_i,x_j)$，是指拥有训练数据集合 $(x_1,x_2,…x_n)$，定义一个矩阵的元素 $a_{ij}=f(x_i,x_j)$，这个矩阵是 $n×n$ 的。如果这个矩阵是半正定的，那么 $f(x_i,x_j)$ 就称为半正定的函数。

所谓半正定指的就是核矩阵 K 的特征值均为非负，核矩阵的定义如下：

$$
K_{n \times n}=
\begin{bmatrix}
K(x_1,x_1) & K(x_1,x_2) & \cdots & K(x_1,x_n)
\\\\
\vdots & \vdots & \ddots & \vdots
\\\\
K(x_n,x_1) & K(x_n,x_2) & \cdots & K(x_n,x_n)
\end{bmatrix}
$$

其中 $K(x_i,x_j) == K(x_j, x_i)$，所以核矩阵是一个对称矩阵。

Mercer 定理不是核函数必要条件，只是一个充分条件，即还有不满足Mercer定理的函数也可以是核函数。

表 12.3.1 常用核函数

|名称|表达式|说明|
|--|--|--|
|线性核|$\boldsymbol{x}_i \cdot \boldsymbol{x}_j  + r$|只能解决线性可分问题，不能把非线性问题转换成线性问题|
|多项式核|$[\gamma (\boldsymbol{x}_i \cdot \boldsymbol{x}_j) + r]^d$|$\gamma$ 缺省取值为特征数的倒数，$d$ 一般取值为 2，常用形式为 $[\gamma(\boldsymbol{x}_i \cdot \boldsymbol{x}_j)+1]^2$|
|高斯核|$e^{-\parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2/2 \sigma^2}$ |径向基（Radial Basis Function）核，对于样本噪音有较好的抗干扰能力|
|高斯核变形|$e^{-\gamma \parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2}$ | $\gamma$ 一般取值为特征数的倒数，值越大拟合能力越强，直至过拟合|
|双曲正切核|$\tanh(\gamma (\boldsymbol{x}_i \cdot \boldsymbol{x}_j) +r)$|$\gamma$ 缺省取值为特征数的倒数|
|指数核|$e^{-\parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel /2 \sigma^2}$|高斯核函数的变种，它将向量范数从 2 调整为 1，这样改动会对参数 $\sigma$ 的依赖性降低，但是适用范围较窄|
|拉普拉斯核|$e^{- \parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel)/\sigma}$|完全等价于指数核，区别在于对参数 $\sigma$ 的敏感性降低，也是一种径向基核函数|

核函数选择的基本原则：


- 如果特征的数量大（>1000）到和样本数量差不多，选用线性核。
- 如果特征的数量小（<100），样本的数量正常（<10000），选用高斯核函数。
- 如果特征的数量小，而样本的数量很大（>50000），选用线性核。分类效果不好的话，手工添加特征再做试验。
- 实际上可以对多个核函数进行测试，选择表现效果最好的核函数。


### 思考与练习

1. 推导式 12.3.8 和 12.3.9，验证其两个矢量的内积结果是否等于 $(x_i · x_j)^2$。
