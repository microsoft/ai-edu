
## 12.5 高斯核函数

### 12.5.1 高斯核函数

高斯核函数的定义：

$$
K(\boldsymbol{x}_i,\boldsymbol{x}_j)=e^{-\frac{\parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2}{2 \sigma^2}}=\exp(-\frac{\parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2}{2 \sigma^2})
\tag{12.5.1}
$$

但是由于 $\sigma$ 在分母上，理解起来要绕一下，所以一般写成式 12.5.2 的形式，即令：$\gamma = \frac{1}{2\sigma^2}$，变形为：

$$
K(\boldsymbol{x}_i,\boldsymbol{x}_j)=e ^ {-\gamma \parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2 }=\exp (-\gamma \parallel \boldsymbol{x}_i-\boldsymbol{x}_j \parallel^2 )
\tag{12.5.2}
$$

### 12.5.2 理论上的高斯核函数的映射函数

式 12.5.2 从形式上看，和前面学习的核函数都不一样，由此产生两个问题：

1. 内积计算在哪里体现？
2. 多维特征映射在哪里体现？

为了解释上面两个问题，下面我们把式 12.5.2 展开，为了方便，可以令 $\gamma=1$：

$$
\begin{aligned}
K(\boldsymbol{x}_i,\boldsymbol{x}_j)&=e^{-||\boldsymbol{x}_i-\boldsymbol{x}_j||^2} \quad(接下来展开求二范数的公式)
\\\\
&=e^{-\boldsymbol{x}_i^2}e^{-\boldsymbol{x}_j^2}e^{2 \boldsymbol{x}_i \cdot \boldsymbol{x}_j} \quad(接下来利用泰勒公式表示第三项)
\\\\
&=e^{-\boldsymbol{x}_i^2}e^{-\boldsymbol{x}_j^2}  \sum_{n=0}^\infty \frac{(2 \boldsymbol{x}_i \cdot \boldsymbol{x}_j)^n}{n!} \quad (接下来展开求和项)
\\\\
&=e^{-\boldsymbol{x}_i^2}e^{-\boldsymbol{x}_j^2} \Big [ 1 + \frac{2\boldsymbol{x}_i \cdot \boldsymbol{x}_j}{1!} + \frac{(2\boldsymbol{x}_i \cdot \boldsymbol{x}_j)^2}{2!} + \frac{(2\boldsymbol{x}_i \cdot \boldsymbol{x}_j)^3}{3!} + \cdots \Big ] \quad (接下来变成内积形式)
\\\\
&= \Big [ e^{-\boldsymbol{x}_i^2} \big (1 \quad \sqrt{\frac{2}{1!}}\boldsymbol{x}_i \quad \sqrt{\frac{2^2}{2!}}\boldsymbol{x}_i^2 \quad \sqrt{\frac{2^3}{3!}}\boldsymbol{x}_i^3 \cdots \big ) \Big ] \cdot \Big [ e^{-\boldsymbol{x}_j^2} \big (1 \quad \sqrt{\frac{2}{1!}}\boldsymbol{x}_j \quad \sqrt{\frac{2^2}{2!}}\boldsymbol{x}_j^2 \quad \sqrt{\frac{2^3}{3!}}\boldsymbol{x}_j^3 \cdots \big ) \Big ]
\\\\
&=\phi(\boldsymbol{x}_i) \cdot \phi(\boldsymbol{x}_j)
\end{aligned}
\tag{12.5.3}
$$

式 12.5.3 的最后一行，就是内积计算的形式了。所以，高斯核函数的映射函数可以表示为式 12.5.4：

$$
\phi(z)=e^{-z^2} \Big (1 \quad \sqrt{\frac{2}{1!}}z \quad \sqrt{\frac{2^2}{2!}}z^2 \quad \sqrt{\frac{2^3}{3!}}z^3 \cdots \Big )
\tag{12.5.4}
$$

如果带上 $\sigma$ 参数，则高斯核的映射函数为：

$$
\phi(z)=e^{-z^2/{2\sigma^2}} \Big [1 \quad \sqrt{\frac{2}{1!}}\frac{z}{\sigma} \quad \sqrt{\frac{2^2}{2!}}(\frac{z}{\sigma})^2 \quad \sqrt{\frac{2^3}{3!}}(\frac{z}{\sigma})^3 \quad \cdots \Big ]
\tag{12.5.5}
$$

其中的 $z=(\sum_{i=1}^n \boldsymbol{z}_i^2)^{\frac{1}{2}}$，即矢量 $\boldsymbol{z}$ 的模，如果 $\boldsymbol{z}$ 是三维矢量的话，则： 

$$
z = \big (\sum_{i=1}^3 \boldsymbol{z}_i^2 \big)^{\frac{1}{2}} = \sqrt{\boldsymbol{z}_1^2+\boldsymbol{z}_2^2+\boldsymbol{z}_3^2}
\tag{12.5.6}
$$

在式 12.5.5 的内部，每一项都可以看作是一维特征，一共是无穷维的特征。这样就解释了前面的两个问题。

### 12.5.3 验证理论上的映射函数

我们仍然使用异或问题来验证一下式 12.5.5 的映射函数可以得到什么？

异或问题的样本如下：

```
X 的原始值：
[[0 0]
 [1 1]
 [0 1]
 [1 0]]

Y 的原始值：
[-1 -1  1  1]
```

X 经过标准化后（均值为 0，方差为 1）得到：

```
X 标准化后的值：
[[-1. -1.]
 [ 1.  1.]
 [-1.  1.]
 [ 1. -1.]]
```

下面用式 12.5.5 的映射函数对 X 标准化后的数据做映射，可以取 n=4，以便得到 4 维特征。代码如下：

```python
# 理论上的映射函数，但实际上不能用
def mapping_function(X, sigma):
    n = X.shape[0]
    Z = np.zeros((n, 4))    # 做一个4维的特征映射，即式10中的n=0,1,2,3
    for i in range(n):
        # 求 x 矢量的模，是一个标量, 式 12.5.6
        z_norm = np.linalg.norm(X[i])
        # 第 0 维 = exp(-0.5*z^2/sigma^2)
        Z[i,0] = np.exp(-0.5*(z_norm**2)/sigma**2)
        # 第 1 维
        Z[i,1] =  Z[i,0] * np.sqrt(2) * (z_norm  / sigma)
        # 第 2 维
        Z[i,2] = Z[i,0] * np.sqrt(2**2/2) * (z_norm / sigma)**2
        # 第 3 维
        Z[i,3] = Z[i,0] * np.sqrt(2**3/6) * (z_norm / sigma)**3

    return Z
```

运行后得到的 Z 值：

```
X 标准化后映射的特征值：
[[0.77880078 0.77880078 0.55069531 0.31794409]
 [0.77880078 0.77880078 0.55069531 0.31794409]
 [0.77880078 0.77880078 0.55069531 0.31794409]
 [0.77880078 0.77880078 0.55069531 0.31794409]]
```

到这里可以停止了！因为 4 个样本的第一个特征值都是 0.77880078，第二个特征值也都相同，第三个、第四个也是如此，就是说经过式 12.5.5 的映射后，所有的样本都变成了四维空间中的同一个点，这样无论如何是无法做分类的。

这是为什么呢？因为对于公式 $||\boldsymbol{x}||=\sqrt{x_1^2 + x_2^2}$，无论 $x_1,x_2$ 是 1 还是 -1，总是等于 $\sqrt{2}$。

可能有读者怀疑是不是因为做了标准化造成的这个结果，因为标准化得到的值全都是 1 或 -1。那我们就不做标准化再做一次映射：

```
X 不做标准化直接做映射的特征值：
[[1.         0.         0.         0.        ]
 [0.77880078 0.77880078 0.55069531 0.31794409]
 [0.8824969  0.62401954 0.31200977 0.12737746]
 [0.8824969  0.62401954 0.31200977 0.12737746]]
```

从上面映射后的结果可以看到，映射函数把后两个正类样本映射到了一个点上，这是因为 [0,1] 和 [1,0] 这两个样本，经过映射后的值恰巧相同。这只是一种巧合，算法是不能依赖样本在数值上的特性来工作的。

这让笔者想起了当年对感知机理论的质疑：**感知机不能解决简单的异或问题。** 并由此使得对神经网络的进一步研究工作搁置了很多年。难度高斯核函数不能解决异或问题吗？

在此，我们先得出一个初步结论：**高斯核函数的映射函数只是一种理论解释，而不是实际的算法工作原理。** 高斯核函数的真正工作原理在下一小节中学习。


### 思考与练习

1. 以上代码在 Code_12_5_Xor.py 中，读者可以自行修改映射函数试验其它方法。

