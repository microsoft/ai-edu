## 11.2 对 C 值的理解

### 11.2.1 定义数据

在本节中，我们会通过定制一些特殊的情况，来体现式 11.1.1 中 C 值的作用。

首先定义样本数据：

```Python
def main():
    X = np.array([[0,3],[1,1],[1,2],[2,1],[3,0],[1,3],[1,4],[2,3],[3,1],[3,2],[3,3],[4,1]])
    Y = np.array([-1,-1,-1,-1,-1,-1,1,1,1,1,1,1])
```

图 11.2.1 显示了这些样本的分布情况。

<img src="./images/11-2-1.png" />

<center>图 11.2.1 样本数据分布</center>

通过观察，可以发现正负类样本实际上是成对称分布的，中间有一个很宽的分类间隔，但是由于负类样本点 5 和正类样本点 8 的存在，破坏了原有的分类间隔，所以必须重新计算分界线及分类间隔。

### 11.2.2 线性分类

接下来在代码中分别设置 C 的值为 10、1、0.1，看看分类效果如何：

```Python
    fig = plt.figure()
    ax = fig.add_subplot(131)
    C = 10
    svc(ax, C, X, Y)
    ax = fig.add_subplot(132)
    C = 1
    svc(ax, C, X, Y)
    ax = fig.add_subplot(133)
    C = 0.1
    svc(ax, C, X, Y)
    plt.show()
```

上述代码中的 svc 函数内部实际上是调用 sklearn 的 SVM 模块中的 SVC 方法来实现的：

```Python
from sklearn.svm import SVC

def svc(ax, C, X, Y):
    model = SVC(C, kernel='linear')
    model.fit(X,Y)
    ......
```
SVC 这个名字的含义是 Support Vector Classifier，支持向量分类器。SVM 中还有一个叫做 SVR 的方法，含义是 Support Vector Regression，支持向量回归，是用 SVM 算法做回归预测的。在 SVC 函数中，我们只指定了两个参数，第一个是 C 值，第二个 kernel = 'linear' 是使用线性分类器。

### 11.2.3 可视化分类效果

训练结束后绘制样本点，以便和分类结果比对：

```Python
    for i in range(Y.shape[0]):
        if (Y[i] == 1):
            # 正类样本
            ax.scatter(X[i,0], X[i,1], marker='x', color='r')
        else:
            # 负类样本
            ax.scatter(X[i,0], X[i,1], marker='o', color='b')
        # 样本编号
        ax.text(X[i,0]+0.1, X[i,1]+0.1, str(i))
```

同时，把分界线及分类间隔绘制出来：

- 分界线方程
  
  $w_1 x_1 + w_2 x_2 + b = 0$，转换成 $y=ax+b$ 的形式，即：
  
  $$x_2 = (-w_1 x_1-b)/w_2$$

- 分类间隔边界的方程

  $w_1 x_1 + w_2 x_2 + b = \pm 1$，转换成：
  
  $$x_2 = (-w_1 x_1-b \pm 1)/w_2$$


在下面的代码中，把 $x_2$ 看作 $y$，$w$ 矢量的下标是从 0 开始的，所以 $w_1 = w[0]，w_2=w[1]$：


```Python
    x = np.linspace(0,5,10)
    w = model.coef_[0]
    b = model.intercept_[0]

    # w[0] * x[0] + w[1] * x[1] + b = 0
    y0 = (-w[0] * x - b)/w[1]
    # w[0] * x[0] + w[1] * x[1] + b = 1
    y1 = (-w[0] * x - b + 1)/w[1]
    # w[0] * x[0] + w[1] * x[1] + b = -1
    y2 = (-w[0] * x - b - 1)/w[1]

    ax.plot(x,y0)
    ax.plot(x,y1,linestyle='--')
    ax.plot(x,y2,linestyle='--')
```

运行代码 Code_11_2_1.py 后得到图 11.2.1。

<img src="./images/11-2-2.png" />

<center>图 11.2.2 C 值的作用</center>


SVC() 函数返回的 model 中具有很多属性，其具体含义如下：

|属性|例子|用途|
|--|--|--|
|model.coef_|[3.0 2.0]|权重 $w$ 向量，只在 kernel='linear' 时有效|
|model.intercept_|[-10]|偏移 $b$，但是需要除以 -w[1] 才是几何意义上的偏移|
|model.support_|[4 5 8]|支持向量的样本序号，从0开始计数|
|model.support_vectors_|[[3 0],[1 3],[3 1]]|支持向量的样本值（坐标），顺序与支持向量样本序号相同|
|model.n_support_|[2 1]|正负类别各自拥有的支持向量数量，本例中负类为2，正类为1|
|model.dual_coef_|[[-5. -1.5 6.5]]|$\alpha_i y_i$ 的值，除以样本标签（$\pm 1$）后即为 $\alpha$ 值，顺序与支持向量样本序号相同|

其中三个属性有以下关系：
```
model.coef_ = np.dot(model.dual_coef_, model.support_vectors_)
```
其结果应该就是 $w$ 的值，相当于公式 $\boldsymbol{w} = \sum_{i=1}^n a_i y_i \boldsymbol{x}_i$。因为非支持向量的 $\alpha$ 值为 0，所以不在属性值中。

还有一个方法：
```
model.decision_function(X)
```
返回的是样本 X 到分界线的距离，单位是归一化过的，并非几何距离。比如，在图11.2.2 左图中，样本 5 的距离是 -1（因为是负类）；而在中图，样本 2 到分界线的距离也是 -1，但是其几何距离比前者大很多。

### 11.2.4 结果分析


表 11.2.1 不同 C 值的分类结果数据

||C=10|C=1|C=0.1|
|--|--|--|--|
|W|[3,2]|[1,1]|[0.66,0.33]|
|斜率 a=(-w1/w2)|-1.5|-1|-2|
|截距 b|5|4|6|
|支持向量|4,5,8|4,5,7,8|0,1,2,3,4,5,6,7,8,9,10,11|

1. C=10

  - 要求尽量分开两类样本，分类间隔会变窄，还属于硬间隔。
  - 支持向量只有 3 个，序号为 4、5、8 的三个样本，都处于间隔边界上。其实 6 也可以是支持向量，只不过有 4、5、8 三个样本点已经能够完全决定分类间隔的形状了。
  - 不参与计算的向量有 9 个。

表 11.2.2 C=10时的分类结果

|支持向量序号|类别$y_i$|$\alpha_i y_i$|$\alpha_i$|C=10|
|:--:|--:|--:|--|--|
|4|-1|-5.0|5|$\alpha < C$|
|5|-1|-1.5|1.5|$\alpha < C$|
|8|1|6.5|6.5|$\alpha < C$|

因为所有的支持向量都在分类间隔线上，所以都有 $\alpha < C$。

2. C=1

  - 属于比较中庸的选择，结果也比较合理，间隔宽度居中，属于软间隔。
  - 支持向量有 4 个，序号为 4、5、7、8。本来 5 号和 8 号样本点应该算是噪音，所以这种选择把 5、8 放在了分界线上，属于两可的分类结果。
  - 把 0、2、3、4 放在了分类间隔下界，同时把 6、7、9、11 放在了分类间隔上届。但是为什么只有 4、7 是支持向量呢？这与算法的实现有关系，与理论无关，原则上从 0、2、3、4 中任选一个，再从 6、7、9、11 中任选一个，都可以构成支持向量。

表 11.2.3 C=1时的分类结果

|支持向量序号|类别$y_i$|$\alpha_i y_i$|$\alpha_i$|C=1|
|:--:|--|--|--|--|
|4|-1|-1|1|$\alpha = C$|
|5|-1|-1|1|$\alpha = C$|
|7|1|1|1|$\alpha = C$|
|8|1|1|1|$\alpha = C$|

看到表 11.2.3 中所有的 $\alpha=C$，说明分类已经比较勉强了。

3. C=0.1

  - 要求分类间隔尽量宽，所以与左图相比，分界线顺时针旋转了一些，使得分类间隔在三种情况中最宽，属于软间隔。
  - 以牺牲 4、6 号样本点为代价（让它们处于模棱两可的状态中）。
  - 造成了所有样本点都处于分类间隔之内，即都是支持向量。


表 11.2.4 C=0.1时的分类结果

|支持向量序号|类别$y_i$|$\alpha_i y_i$|$\alpha_i$|C=0.1|
|:--:|--|--|--|--|
|0|-1|-0.1|0.1|$\alpha = C$|
|1|-1|-0.0445|0.0445|$\alpha < C$|
|2|-1|-0.1|0.1|$\alpha = C$|
|3|-1|-0.1|0.1|$\alpha = C$|
|4|-1|-0.1|0.1|$\alpha = C$|
|5|-1|-0.1|0.1|$\alpha = C$|
|6|1|0.1|0.1|$\alpha = C$|
|7|1|0.1|0.1|$\alpha = C$|
|8|1|0.1|0.1|$\alpha = C$|
|9|1|0.1|0.1|$\alpha = C$|
|10|1|0.0667|0.0667|$\alpha < C$|
|11|1|0.0778|0.0778|$\alpha < C$|

从图 11.2.2 右图中，可以看到绝大多数支持向量都在分类间隔以内了，分类结果已经相当勉强了，只有几个处于边缘地带的样本的 $\alpha$ 值小于 C，其它的都等于 C。

### 思考与练习

1. 设置其它 C 值，重新运行代码，通过观察结果来理解 C 值的作用。
  