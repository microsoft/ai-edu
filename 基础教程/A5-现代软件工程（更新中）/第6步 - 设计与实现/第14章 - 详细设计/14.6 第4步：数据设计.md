## 14.6 第四步：数据设计

### 14.6.1 数据设计

下面我们针对整体数据流图 14.4 中的各个数据存储图例来进行分析设计。

首先要把存储图例转换成 Azure 支持的存储技术，这就需要做技术选型，这一点我们在 11.2 节中做过详细介绍，最后使用了两种技术：

- 对于数据文件，使用 Azure Blob。
- 对于模型，使用 ML-flow，它后台使用了 MySql 作为索引，Azure Blob 作为存储。

数据文件存储的细节如图 14.6.1 所示。

<img src="img/Slide13.SVG"/>

图 14.6.1 数据存储设计

#### 1. 数据文件

客户每周都要进行一次推理，即上传一次股票数据文件，下载一次推理结果文件。如何区分批次呢？最简单的办法就是使用时间戳，而且是使用系统（服务器端）的时间戳，因为客户端的计算机时间可能不准确。

如图 14.6.1 所示，数据文件存放在以下层级目录下：

1. 股票数据根目录（蓝色）
   
   建立根目录，为了把股票数据与模型数据分开。

2. 用户上传文件时的时间戳，如“2021-01-11-07-15-07”
   
   由于客户是每周上传一次数据，具有时效性。并且股票数据也是按天汇集在一起的，所以我们可以利用这种较强的时间关系，设计出用时间戳为目录名称的存储结构，比如，就使用客户上传文件的时间，格式是“YYYYMMDDHHmmSS”。

3. 三个子目录
   - Data：上传的股票数据文件，一般情况下会有一周五天的.csv和.mat文件，每天一个。
   - Report：推理过程产生的日志，工程师会根据此文件中的内容来判断预测结果是否正确。
   - Output：推理结果文件，客户最后从此目录中下载结果。

#### 2. 数据传输

如图 14.6.1 右侧所示过程：

1. 客户端通过 REST API 即可获得上下文（即时间戳）SessionID。
2. 客户端使用 AzCopy.exe 上传文件，指定本地文件夹，就可以把该文件夹内的一批文件上传到指定的目标文件夹（以SessionID命名）的 Data 目录中。
3. 确定上传完毕后（AzCopy.exe 有进度条可以显示是否上传结束），客户端再通过 REST API 发送一条通知来触发推理，并附带 SessionID。
4. 系统从 SessionID 拼接下载地址，下载股票数据，并开始推理。
5. 系统把推理结果上传到与 SessionID 相关的 Output 目录中，把推理过程日志上传到 Report 目录中。
6. 系统发送邮件通知给客户。
7. 客户从客户端下载推理结果文件。


### 14.6.2 模型文件

如图 14.6.1 所示，模型文件存放在以下层级目录下：

1. 模型数据根目录（橙色）
   
   建立根目录，为了把股票数据与模型数据分开。

2. 从0开始累加的整数值，用于存放训练试验数据
   
   在训练模型时，需要做多次试验，测试各种参数的组合，所以每次都放在一个不重名的目录里即可。ML-flow有一个内置的计数器，初始化一次试验时，会返回一个唯一的整数 ID，比如 12、13 等等依次递加。

3. Artifacts子文件夹，存放Pytorch的模型数据文件
   - npy：数据文件
   - pkl：压缩文件
   - pth：路径信息





#### 3. 数据库设计

模型文件的索引信息

|试验序号|时间戳|文件夹名称|实验结果|模型位置|
|--|--|--|--|--|
|12|2020-12-20 11:34:23|model-12|0.936|...|
|13|2020-12-22 12:47:35|model-13|0.941|...|


### 14.6.2 模型管理



### 模型设计

可以简单描述一下