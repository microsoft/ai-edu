
## 17.6 基于测试用例生成的代码生成方法

原文标题：CODET: Code Generation with Generated Tests
原文链接：https://arxiv.org/pdf/2207.10397.pdf
原文作者：
```txt
· Bei Chen, Microsoft Research Beijing China, beichen@microsoft.com
· Fengji Zhang, v-fengjzhang@microsoft.com
· Anh Nguyen, anhnguyen@microsoft.com
· Daoguang Zan, v-dazan@microsoft.com
· Zeqi Lin, Microsoft Research Beijing China, zeqi.lin@microsoft.com
· Jian-Guang Lou, Microsoft Research Beijing China, jlou@microsoft.com
· Weizhu Chen, wzchen@microsoft.com
```
主要作者简介：

陈蓓，微软亚洲研究院主管研究员。博士毕业于清华大学计算机系，师从张钹院士及朱军教授。她的研究兴趣主要为自然语言处理、大规模语言模型及其应用，例如语义解析、对话系统、代码智能等。在领域顶级会议如 NeurIPS，ICLR，ACL，EMNLP，IJCAI，AAAI，KDD 等发表论文 30 余篇。近年来在语言模型与软件工程交叉领域发表多篇论文，包括代码生成，代码修复等任务。

### 摘要

得益于预训练语言模型如 CodeX 等的发展，我们能够为给定的编程问题自动生成代码解决方案。一般来说，预训练语言模型会生成多个不同的解决方案，而从中选择出最为准确的解决方案仍然是一个很大的挑战（例如，pass@100通常比pass@1高很多）。验证一个解决方案准确性最简单的方法是使用一组测试用例来执行它。然而，人工构建这样的测试用例是费时费力的。本报告旨在探索这样一个问题：预训练语言模型可以进行自我验证吗？我们提出了一个有趣的方法CODET，它能够利用相同的预训练语言模型为代码解决方案生成多个测试用例，从而降低人工成本。CODET将生成的多个代码解决方案在测试用例上执行，并进行双向执行一致性判断，从而挑选出最为准确的代码解决方案。我们使用 5 个不同大小和能力的预训练语言模型，在 4 个有名的代码生成数据集上做了实验。实验结果证实 CODET 能够极大地提升代码生成的性能，其中 CODET 将 HumanEval 数据集的 pass@1 分数提升至 65.8%，比现有最高分数还要高 20+%。

### 17.6.1 引言

尽管代码生成的预训练技术取得了显着进步，但从大型语言模型生成的多个候选者中选择正确的解决方案仍然是一个难题。例如，CodeX，一种最先进的代码预训练语言模型生成，可以实现 pass@100（如果给定的 100 个生成的解决方案中有一个或多个可以通过相应的测试用例）为 77.4%，但在 HumanEval 基准上的 pass@1（单个解决方案的正确率）仅为 33.5%。这个巨大的差距限制了代码生成模型的实用性，并激励我们探索如何选择正确的或多个候选的最佳解决方案。

验证解决方案正确性的一种直接方法是执行代码并检查它是否通过所有相应的测试用例。这种以执行为导向的方法已在各种领域得到广泛采用与代码相关的任务，例如代码生成、代码翻译、和程序合成。然而，这种方法在很大程度上依赖于测试用例的质量和数量，而这通常是昂贵的创建和维护非常耗时。此外，在像 Copilot2 这样的真实应用中，辅助开发者编写代码的代码生成工具，指望用户提供每个问题的测试用例是不现实的。因此，我们建议自动生成任意编程问题的测试用例，并使用它们快速验证任何解决方案。

<img src="img/17-6-1.png">

图 17-6 CODET 协议

在本文中，我们提出了 CODET 协议，如图 17-6 所示。首先，我们利用相同的预训练语言模型生成代码解决方案，例如 CodeX，通过提供详细说明作为提示，为每个编程问题生成大量测试用例。接下来，受经典 RANSAC 算法启发的协议方法，使用双重执行代码和测试用例。我们在每个生成的测试用例上执行每个生成的代码解决方案，并迭代地找到多个组代码解决方案和测试用例对。每个小组或共识集都有通过相同的解决方案测试用例，表明它们具有相同的功能，即使它们在实现上不同。我们期望通过更多测试用例的解决方案更正确，并且具有更多相似的解决方案，即同一共识集中的解决方案，与问题更一致规格。因此，我们根据其中的测试用例和解决方案的数量对每个共识集进行排名，并从排名最高的共识集中选择最佳解决方案。

这种方法简单高效，因为它不需要任何标记数据或额外的排序器，但它实现了令人惊讶的卓越性能。我们在五个不同的方面评估我们的方法用于代码生成的预训练语言模型：三个 OpenAI CodeX 模型，INCODER 和 CODEGEN，以及四个已建立的代码生成基准：HumanEval、MBPP、APPS 和 CodeContests。实验结果表明我们的方法可以有效地从多个候选者中选择正确的解决方案，提高 pass@1 在零样本设置的所有基准测试中得分显著。例如，CODET 实现了使用 code-davinci-002 的改进：HumanEval (47.0% → 65.8%)，MBPP (58.1% → 67.7%)，应用介绍 (27.2% → 34.6%) 和代码竞赛 (0.7% → 2.1%)。此外，当我们将最强大的预训练模型 code-davinci-002 和 CODET 结合起来，我们表现出色大幅领先于之前最先进的方法，例如 HumanEval：42.7% → 65.8%。

### 17.6.2 方法

代码生成的任务是解决一个编程问题：在上下文中 c 生成代码 x。 如图 17-7 所示，上下文 c 中包含自然语言问题描述代码注释的形式，以及包含 imports 和 function 等语句的代码片段标头。通常，我们采样一组代码解决方案，表示为 X = {x1, x2, · · ·, xN }，基于在上下文 c 上使用预训练的语言模型 M，可以表示为 X = M(c)。我们的目标是从一组生成的代码解决方案 X 中选择最佳代码解决方案 x，其中 x 是正确解决给定编程问题的最有可能的解决方案。为此，我们提出 CODET，希望释放预训练语言模型 M 的内在力量。具体来说，我们使用 M 为编程问题生成测试用例（17.6.1-1），然后根据双重执行协议选择最佳代码解决方案 x（17.6.2-2）。

<img src="img/17-6-2.png">

图 17-7 代码生成任务

#### 1. 测试用例生成

除了生成代码解决方案，我们还需要生成测试用例来评估其正确性代码解决方案。测试用例是定义在函数中的一对输入和预期输出语境。例如，在图 17-7 中，一个用于检查是否存在的编程问题的测试用例列表中存在小于阈值的接近元素。为了生成测试用例，我们使用相同的我们用于生成代码解决方案的预训练语言模型 M，但我们添加了一条指令p 到上下文 c 作为提示，表明我们需要测试用例而不是代码解决方案。指令p由三部分组成：（1）一个“pass”语句作为占位符函数体，这表明我们不需要为函数生成代码，(2)评论“检查[入口点]的正确性”以阐明生成测试用例的意图，其中“[入口点]”是函数的名称，以及 (3) 启动测试用例的“断言”语句generation，它将测试用例的格式指定为输入输出对。

然后，我们将连接的上下文和指令 $concat(c, p)$ 提供给语言模型 $M$，以及从模型输出中抽取一组测试用例，表示为 $Y = \{y_1, y_2, ..., y_M\}$。过程测试用例的生成可以表示为 $Y = M(concat(c, p))$。语言模型会尝试通过为函数生成合理的输入输出对来完成指令。注意在生成代码解决方案之前，我们从上下文 c 中删除所有示例输入输出案例，并且测试用例，以避免将真实的测试用例暴露给语言模型并增加多样性和生成的测试用例的难度。

#### 2. 双重执行协议

在本小节中，我们将解释如何从生成的代码集 $X = \{x_1, x_2,..., x_N \}$ 中选择最佳代码解决方案 x，使用生成的测试用例集 $$Y = \{y_1, y_2, ..., y_M\}$$ 作为一个标准。我们可以在测试用例 y 上执行代码解决方案 x，这意味着运行函数由 x 在 y 的输入部分定义，并将输出与 y 的输出部分进行比较。如果代码解决方案 x 可以无错误地执行并且输出与预期输出匹配，那么我们说代码解决方案 x 可以通过测试用例 y。此外，我们说有一个功能协议在两个代码解决方案 xi 和 xj 之间，如果它们可以通过 Y 中的同一组测试用例。我们的方法是基于以下假设：（1）代码解决方案和测试用例是独立且给定某个编程问题，从预训练语言模型 M 中随机抽样，(2) 不正确的代码解决方案往往是多种多样的，并且具有功能的概率偶然出现的两个错误代码解决方案之间的一致性非常低。这些假设是类似于经典的 RANSAC 算法，这是一个稳健的在嘈杂数据中找到共识的方法。受 RANSAC 的启发，我们提出了我们的方法CODET 执行双重执行协议，这是一种迭代方法，如下所示：

- 我们从所有可能对 $D = \{(x, y)|x ∈ X, y ∈Y\}$。然后我们尝试在测试用例 y 上执行代码解决方案 x。如果 x 可以通过 y，那么我们说 $(x, y)$ 对是一个假设的内点，因为它假设地描述了编程问题的正确功能。否则，我们说 $(x, y)$ 是异常值，因为它没有描述正确的功能。图 3 显示了一个简单的示例编程问题“返回数字的平方”。$(x_1, y_1)$ 和 $(x_3, y_2)$ 是两个的假设异常值，而 $(x_1, y_4)$ 和 $(x_3, y_1)$ 是两个异常值。

- 如果 $(x, y)$ 是一个假设的内点，我们从 $D$ 中收集所有其他与这个假设的内点一致的对，形成一个集合S，称为共识集。找到符合的对 $(x, y)$，我们首先找到x可以通过的所有测试用例，记为S_y。然后，我们找到所有代码可以通过与 x 完全相同的测试用例的解决方案，表示为S_x。最后，共识集是包含来自 S_x 的代码解决方案和测试用例的所有对的集合来自 S_y，即 $S = \{(x, y)|x ∈ S_x, y ∈ S_y\}$。例如在图 3 中，我们可以得到 $S_x = \{x1, x2\}, S_y = \{y_1, y_2, y_3\}$ 来自假设的内点 $(x_1, y_1)$（以绿色显示框）和 $S_x = \{x_3\}，S_y = \{y_2, y_3, y_4, y5\}$ 来自 $(x_3, y_2)$（显示在紫色框中）。

<img src="img/17-6-3.png">

图 17-8


- 我们将共识集评分为 $f(S) = |S_x||S_y|$，其中 $|S_x|$ 是码解的个数在 $S_x$ 和 $|S_y|$ 是 $S_y$ 中的测试用例数。这个分数等于对数在共识集中。直觉是，与假设一致的对越多根据我们的假设，此功能越有可能是正确的。按照图 3 中的示例，假设的共识集分数为 6 和 4内点 $(x_1, y_1)$ 和 $(x_3, y_2)$，分别我们重复上述过程固定次数，每次产生一个共识集它的分数。最后，我们通过从共识中选择任何代码解决方案来获得最佳代码解决方案 x^设置最高分。如果我们想要获得k个代码解，我们可以选择top k个共识得分最高的集合，并从 k 个共识集合中的每一个中挑选一个代码解决方案。

实际中，当 $D$ 中的代码解数不多时，我们可以简化上面的方法通过检查 $D$ 中所有可能的对，而不是从 $D$ 中采样对。特别是，对于每个代码解决方案 $x ∈ X$，我们对 $Y$ 中的每个测试用例运行它，并跟踪它通过了哪些测试用例。我们将通过相同测试用例的代码解决方案组合在一起，因为它们具有相同的功能。这样，我们根据功能将 $X$ 中的所有代码解决方案分成几组，我们编写
作为 $X = \{S^1_x, S^2_x,···,S^K_x \}$，其中 $K$ 是代码解组的数量。每个组 $S_x$ 都有一个集合它通过的测试用例的数量，我们将其写为 $S_y$。然后，我们得到 $K$ 个共识集，每个共识集都有形式 $S = \{(x, y)|x ∈ S_x, y ∈ S_y\}$。我们可以对由 $f(S) = |S_x||S_y|$ 设置的每个共识进行评分，如前。这个朴素的版本捕捉到了相同的下划线直觉，但发现所有共识集都是正确的离开，无需重复采样对。

### 17.6.3 试验步骤

#### 1. 模型

我们的实验基于 CodeX、INCODER 和 CODEGEN。CodeX 是 GPT-3 的后代，并且精通理解所提供的上下文并生成功能程序。我们用三个OpenAI 提供的具有不同功能的 CodeX 模型：code-cushman-001、code-davinci 001 和 code-davinci-002。INCODER 是一个统一的生成模型，可以执行从左到右代码生成和代码填充，而 CODEGEN 是一个大型语言模型家族执行会话程序合成。我们使用 INCODER 6.7B 版本 (INCODER 6B) 和 CODEGEN 16B Python 单语版本 (CODEGEN-MONO-16B)。

#### 2. 指标和基线

我们使用指标 pass@k（有 n 个样本）进行性能评估和利用真实测试用例来确定代码解决方案的功能正确性。对于每个问题，我们采样 n 个代码解决方案，然后选择其中的 k 个进行评估。如果有的话k 个代码解决方案中的一个通过了所有地面实况测试用例，则认为问题已解决。然后pass@k 是已解决问题的百分比。我们使用 pass@k 的无偏定义作为我们的
baseline，其中 k 个解决方案是从 n 个样本中随机选取的。我们的代码T使用双重执行协议机制从 n 个样本中选择 k 个解决方案，如2.2. 此外，我们还包括了 Li 等人的聚类方法。(2022b) 作为比较，表示为AlphaCode-C。我们的复制是使用 CODET 生成的测试输入，在测试输入，按测试输出对解决方案进行分组，并按大小对集群进行排序（附录 I 中有详细信息）。

#### 3. 基准

 我们在零镜头设置中对四个公共代码生成基准进行了实验。基准的统计数据如表 17-16 所示。(1) HumanEval 由手写的 Python 编程问题组成。原始上下文包括示例输入输出案例，这些案例在我们的实验中被删除以避免暴露真实的测试案例。
附录 B 中的实验表明，这种去除操作是合理且必不可少的。(2) MBPP 包含众包 Python 编程问题，我们遵循 HumanEval 为其构建上下文。(3) APPS 包括从开放访问编码网站收集的编码问题，这些网站有不同的难度级别。(4) CodeContests包括从 Codeforces 平台上抓取的竞争性编程问题。为了启用零样本推理，我们为 APPS 和 CodeContests 构建上下文如下：原始问题描述为被视为删除输入输出示例的注释，以及一个简单的函数头“def
solution(stdin : str) → str :” 放置在注释之后以容纳输入/输出数据格式。更多实施细节可以在附录 A 中找到

表 17-16 

<img src="img/17-6-1Table.png" height=180>



### 17.6.4 试验结果

在本节中，我们在五个不同的预训练模型和四个基准上评估 CODET验证其有效性，然后通过测试用例分析和案例研究提供更多见解。

#### 1. 人类和 MBPP 的结果

各种模型在 HumanEval 和 MBPP 基准上的实验结果总结在表 17-17 中。如果我们比较 baseline 列上的 pass@100 和 pass@1，很明显前者明显优于后者，表明 选择最佳代码解决方案的潜力来自 100 个生成的样本。

表 17-17

<img src="img/17-6-2Table.png">


对于三个 CodeX 模型，当我们将 CODET 列与 Baseline 列进行比较时，CODET
pass@1 比基线 pass@1 实现了大约 10% 的绝对改进。HumanEval 的改进始终超过 10%。令人惊讶的是，即使是最强的基线，code-davinci-002，改进为 18.8%，将 pass@1 提高到 65.8%，这比之前报告的最佳结果绝对改进了 20+%。我们将此归因于对 code-davinci-002 生成的更高质量的测试用例进行了更大的改进，提供了在第 4.3 节中进行更深入的分析。CODET 在 MBPP 基准测试中也取得了卓越的性能，尽管改进幅度略小于 HumanEval。使用以code-davinci-002为例，pass@1提升了9.6%。我们还报告 pass@2 和通过CODET的@10进一步显示其优越性。CODET的pass@2结果接近基线 pass@10 结果。同时，对 pass@10 的改进也不断结束HumanEval 基准测试的 10%。

INCODER-6B和CODEGEN-MONO-16B的实验结果进一步验证了CODET的有效性。很明显，CODET 可以显着提高 pass@1，绝对提高范围为 4.2% 到 13.1%。INCODER-6B 实现了最大的改进比 MBPP 基准上涨 13.1%。与CodeX的实验结果相似，pass@2结果接近基线 pass@10。所有结果都表明 CODET 可以提高各种预训练语言模型的性能一致。

至于 AlphaCode-C，它在使用不同模型的两个基准测试中始终不如 CODET，证明了我们采用测试用例信息的双重执行协议的优越性考虑在内。此外，我们注意到生成的代码解决方案中存在重复，并且测试用例。我们在附录 D 中进行了一项消融研究，以表明去重对 CODET 的结果几乎没有影响。此外，我们讨论了 CODET 对 t 的敏感性

#### 2. 应用程序和代码竞赛的结果

我们还在两个更具挑战性的基准测试 APPS 和 CodeContests 上进行了实验。我们构建 APPS 和 CodeContests 的零样本版本，以符合我们对 HumanEval 的设置和 MBPP 通过删除问题描述中的示例输入输出案例。我们雇用code-davinci-002 用于代码解决方案和测试用例生成。采样数设置为50APPS 在 5, 000 个测试问题上节省计算成本，而对于 CodeContests，如下李等。(2022b)，采样数设置为1, 000，解决特别难的问题。来自结果总结在表 17-18 中，我们可以清楚地观察到一致的性能改进在使用 CODET 的两个基准测试中。介绍性的绝对 pass@1 改进为 7.4%APPS 中的问题，而对于比赛级别的问题进并不显着APPS和CodeContest，说明了他们的难处。此外，我们注意到代码-davinci-002由于这两个基准的难度更高。我们在附录 F 中进行了全面研究，以证明 CODET 对此问题的稳健性。

表 17-18 

<img src="img/17-6-3Table.png">


#### 3. 测试用例分析

测试用例对 CODET 至关重要，因为其核心思想是基于测试驱动的执行协议。因此，在本小节中，我们通过回答以下研究问题来分析测试用例。

- Q1：生成的测试用例的质量如何？

我们使用规范解决方案评估生成的测试用例的正确性。一个测试用例是如果规范解决方案可以通过它，则认为是正确的。图 4a 总结了分布 HumanEval 上的测试用例准确度，其中横轴表示每个的准确度值问题，纵轴表示对应问题的概率密度精度值。我们可以看到 CodeX 模型生成的测试用例要高得多精度高于 CODEGEN/INCODER。除了准确率，我们还引入了测试用例毒性率作为质量的衡量标准。如果任何生成的代码解决方案都可以，我们认为测试用例是“有毒的”通过它而规范的解决方案不能。有毒的测试用例可能会阻碍共识的评分设置并导致 CODET 失败。如图 4b 所示，我们可以发现毒性率与不同模型的测试用例准确性高度相关，其中比例CodeX 模型的毒性测试用例数量小于 CODEGEN/INCODER。我们还评估了使用附录 H.2 中的两个覆盖标准生成的测试用例的代码覆盖，其中 CodeX模型仍然优于 CODEGEN/INCODER，平均覆盖率超过 95%。比较表2所示的测试用例质量和CODET的性能，我们可以发现质量测试用例的数量与使用关于不同模型的 CODET 的性能增益密切相关。

<img src="img/17-6-4.png">

图 17-9

- Q2：更好的测试用例能否进一步提升平庸模型的性能？

从上面与图4的讨论，我们可以发现code-davinci-002是最有能力的生成高质量测试用例的模型。因此，我们进行了一项实验以提高其他四种模型（code-cushman-001、ode-davinci-001、INCODER 和 CODEGEN）的性能使用 code-davinci-002 生成的测试用例。表 17-19 总结了性能增益尊重 HumanEval 和 MBPP 基准的不同模型。一般来说，使用测试 code-davinci-002 生成的案例可以显着提高使用测试的性能由能力较差的模型本身生成的案例。对于 code-cushman-001 和 code-davinci-001，pass@1 的绝对改进在 1.8% 到 4.3% 之间，而对于 INCODER 和 CODEGEN，范围从 6.2% 到 15.9%。以上结果表明正确的代码通过采用更好的测试用例，可以进一步利用平庸模型生成的解决方案。

表 17-19 

<img src="img/17-6-4Table.png">


- Q3：当测试用例较少时，CODET 的效果如何？

在为 HumanEval 基准生成测试用例时，我们对每个问题抽样 100 次，每个样本可能
包括多个断言语句（即测试用例），记为Sampling Number = 100。然后我们提取第一个
来自每个样本的 5 个句法正确的测试用例，表示为as Limit = 5. 这意味着每个问题都配备了最多 500 个测试用例。提取测试的实际数量案例总结在附录 H.1 中。我们通过减少采样数和限制来对测试用例的数量进行消融研究。如表 17-20 所示，我们可以得出结论，在 CODET 中使用更多的测试用例通常可以导致更好的性能，而当 Sampling Number ≥ 50 且 Limit ≥ 3 时，性能差距缩小。此外，CODET 将 pass@1 提高了 9.5%，仅 10测试用例使用code-davinci-002，建议高考案例效率。我们可以在中使用较小的采样数实际应用程序以平衡性能和计算成本。

表 17-20 

<img src="img/17-6-5Table.png" height=180>


#### 4. 案例研究

在 CODET 中，我们基于好的代码解决方案可以通过最多的测试用例并同意最多的相同功能的解决方案。我们使用“双”因为代码解决方案和测试用例都很关键。图 17-10a 显示了一个案例使用 code-cushman-001 的 HumanEval 基准测试。得分最高的共识集具有正确的如果列表中的所有数字都低于阈值 t，则返回 true 的功能，而共识集排在第 2 位的人并没有完全理解边界条件。第二次共识中的解决方案set 可以通过比第一个共识集（即 218）更多的测试用例（即 226）。然而，考虑到代码解决方案和测试用例，CODET 可以成功地对共识集进行排名，并且找到正确的解决方案。这种情况并不少见，说明我们设计的双重执行约定合理。为了进一步的统计证明，我们进行了一项消融研究来评分通过仅考虑代码解决方案或测试用例的数量来达成共识。结果又来了支持我们的主张，详见附录 I。

<img src="img/17-6-5.png">

图 17-10

CODET 由预训练的语言模型赋能，但也受到它们的限制。所以，第 2.2 节中的第二个假设并不总是成立，导致错误情况8个生成了正确的代码解决方案，但不在前 1 个共识集中。对于在 HumanEval 基准测试中代码为 cushman-001 的 CODET，我们发现 164 个编程问题中有 53 个是属于这种情况。我们手动调查了这些问题，发现其中 20% 可以被归咎于诸如问题描述不明确、角落案例未被发现和缺乏导入语句，而其余问题归因于模型的失败理解问题描述。图 5b 显示了由歧义引起的错误情况。这正确理解描述“sum(first index value, last index value)”是把第一个加起来和最后一个值，而将所有值从第一个到最后一个求和的代码解决方案排名第一。

*由于篇幅有限，对原文有所删减，参考文章链接没有列出，有兴趣的读者可以阅读原文。*
