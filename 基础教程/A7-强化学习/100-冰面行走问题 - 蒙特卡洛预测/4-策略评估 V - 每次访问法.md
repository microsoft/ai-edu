## 10.4 策略评估（预测）$V_\pi$

本节将会针对无模型问题使用蒙特卡洛法做策略评估（policy evaluation），也叫做预测（Predication）。

### 10.4.1 估算 $v_\pi$ 和 $q_\pi$

在 10.2 节中，我们使用**首次访问法**，估算了 MRP 问题中的状态价值函数 $V$，也在一开始就提到过，这个方法同样适用于估算 MDP 问题中的价值函数 $v_\pi$ 和 $q_\pi$。

与 MRP 不同的是，在 MDP 中由于引入了**策略**和**动作**的概念，所以，在当前状态 $s$ 下，要根据策略（policy）选择动作（action），执行动作与环境交互，得到下一个状态、奖励值等信息。

表 10.4.1 MRP 与 MDP 的比较

||马尔科夫奖励过程 MRP|马尔科夫决策过程 MDP|
|-|-|-|
|目的|价值评估|策略评估|
|方法|根据 $s$ 直接得到 $s'$|根据 $s$ 查询策略 $\pi \to$ 得到动作 $a$ 并执行 $\to$ 获得奖励 $r$ 和 $s'$。|
|伪代码|s',r = env.step(s)|a = policy(s) # 策略决定动作<br> s',r = env.step(s,a)|
|结果|奖励值累积求平均，得到状态价值函数 $V$|奖励值累积求平均，得到状态价值函数 $V_\pi$ 和动作价值函数 $Q_\pi$|

首先看状态价值函数的定义：

$$
v_\pi(s) \doteq \mathbb E [G_t \mid S_t=s] \tag{10.4.1}
$$

这和式（10.2.2）没有区别，所以，算法所需要的参数以及过程都和 10.2 节中一样，只不过是运行在一个 MDP 环境下而已。

另外一个问题是：只有 $V_\pi$ 就足够了吗？答案是否定的。

<center>
<img src="./img/fromV2Q.png">

图 10.4.1 状态价值函数 $V_\pi$ 决定动作价值函数 $Q_\pi$
</center>

如图 10.4.1 中，一个 9 个单元的方格世界中，假设已经知道了 $v(s_1)=1.6,v(s_3)=1.3,v(s_5)=1.2,v(s_7)=1.9$，当智能体处于 $s_4$ 的位置时，它一定应该向具有最大价值的 $s_7$ 移动吗？

不一定！看动作价值函数的定义式（8.4.2）：

$$
\begin{aligned}
q_\pi(s,a) & \doteq \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=P^a_{ss'}[R^a_{ss'}+\gamma V_\pi(s')]
\end{aligned}
\tag{10.4.2}
$$

在有模型的情况下，动作价值由三个因子组成：$R, P, V$。虽然 $v(s_7)$ 最大，但是 $R,P$ 都未知的情况下，不能确定就应该向 $s_7$ 移动。假设 $R^a_{s_4,s_7}=R^a_{s_4,s_5}=1, P^a_{s_4,s_7}=0.5, P^a_{s_4,s_5}=1, \gamma=1$，则：

- $q(s_4,a_{RIGHT})=P^a_{s_4,s_5}[R^a_{s_4,s_5}+\gamma v(s_5)]=1 \cdot [1+1\cdot 1.2]=2.2 $
- $q(s_4,a_{DOWN})=P^a_{s_4,s_7}[R^a_{s_4,s_7}+\gamma v(s_7)]=0.5 \cdot [1+1\cdot 1.9]=1.45 $

由于 $2.2 > 1.45$，那么智能体应该选择向右移动。

在无模型的情况下，如果利用式（10.4.1）估算出了 $v_\pi$ 后，能直接算出 $q_\pi$ 吗？不能！虽然 $R$ 可以通过与环境交互得到，但是因为不知道 $P$，所以无法计算 $q_\pi$。

在这种情况下，我们只能通过式（10.4.2）的第一行的定义来估算 $q_\pi$，然后才能按照从 $q_\pi$ 中提出的策略 $\pi(s) \doteq \argmax_a q_\pi(s,a)$ 来行动。所以，在本章后续的内容中，都会把估算 $q_\pi$ 作为目标。

所以，在无模型的情况下，计算动作价值 $q_\pi$ 比起计算状态价值 $v_\pi$ 更有用。在本小节中，我们先学习预测 $V_\pi$，下一小节再学习预测 $Q_\pi$。

### 10.4.2 每次访问法估算 $V_\pi$

#### 算法描述

【算法 10.4】每次访问型蒙特卡洛法。下面的伪代码中，$\leftarrow$ 表示赋值，$\Leftarrow$ 表示追加列表。

---

输入：起始状态 $s$，策略 $\pi$，折扣 $\gamma$, 幕数 Episodes
初始化数组：$G(S) \leftarrow 0, N(S) \leftarrow 0$，$S$ 为状态空间
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略 $\pi$ 得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,r)$，相当于是 $(s_t,r_{t+1})$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $(s_t,r_{t+1})$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t) \leftarrow G(s_t)+G_t$
　　　　$N(s_t) \leftarrow N(s_t)+1$
$V(S) \leftarrow G(S) / N(S)$
输出：$V(S)$

---

为什么叫做**每次访问法**呢？区别于**首次访问法**，在算法中，在遍历过程中，每次只要遇到 $s_t$ 就要计算它的 $G$ 值，而不管它出现在哪里或者出现了多少次。Sutton 认为这两者具有不同的理论基础，首次访问法被研究得很多，具有很强的理论基础；而每次访问法可以过渡到后面要学到的函数近似与资格迹方法，而且从实际效果来看，每次访问法能稍微快一些地收敛，因为样本利用效率更高，相当于采样次数增加了 2~3 倍。

在算法中，这两者的唯一区别就是在对 $Episode$ 从后向前遍历时，是否要检查 $s_t$ 是否曾经出现过。抛开这个区别，算法 10.4 与算法 10.2 的区别是：
- 算法 10.4 要输入策略 $\pi$, 因为要估算 $V_\pi$
- 算法 10.4 要从策略 $\pi$ 选择动作 $a$ 并执行，但是并不会把 $a$ 加入到分幕采样序列中，因为计算 $V_\pi$ 不需要动作信息。



#### 算法实现

【代码位置】Algorithm.Algo_MC_Policy_Evaulation.py

下面我们把代码分成几个小片段来讲解。

函数定义即初始化部分：


```python
# MC 策略评估（预测）：每次访问法估算 V_pi
def MC_EveryVisit_V_Policy(env, start_state, episodes, gamma, policy):
    nS = env.observation_space.n    # 状态空间
    nA = env.action_space.n         # 动作空间
    Value = np.zeros(nS)            # G 的总和
    Count = np.zeros(nS)            # G 的数量
```

输入：

- env - 环境。
- start_state - 起始状态。
- episodes - 最大循环幕数。
- gamma - 折扣。
- policy - 策略。

初始化部分：

- 获得环境的状态空间，以便初始化数组。
- 获得动作空间，以便在后面选择动作时使用。
- Value 数组保存每个状态上的累积 G 值，除以下面的 Count，就会得到平均值 V。
- Count 数组保存每个状态上的累积数量。

多幕循环部分：

```python
    for episode in tqdm.trange(episodes):   # 多幕循环
        Episode = []        # 一幕内的(状态,奖励)序列
        s = env.reset()     # 重置环境，开始新的一幕采样
        done = False
```

这个片段进行多幕循环，为每一幕初始化：
- Episode 保存分幕采样序列。
- s=env.reset() 初始化为起始状态。
- done 用于判断是否结束本幕。

幕内循环部分：

```python
        while (done is False):              # 幕内循环
            action = np.random.choice(nA, p=policy[s])  # 根据策略选择动作
            next_s, reward, done, _ = env.step(action)   # 与环境交互
            Episode.append((s, reward))     # 保存数据序列
            s = next_s      # 迭代
```

这一片段进行分幕采样，从策略得到动作，执行动作得到 $s',r_{t+1}$，并保存到序列中。当 done 为 True 时表示分幕结束，到达终止状态。

$G$ 值计算部分：

```python
        # 从后向前遍历计算 G 值
        G = 0   # 不要忘记清零
        for t in range(len(Episode)-1, -1, -1):
            s, r = Episode[t]   # 取出数据
            G = gamma * G + r   # 计算 G_t
            Value[s] += G       # 值累加
            Count[s] += 1       # 数量加 1
```

这一片段进行倒序遍历，计算每个时刻关于 $s$ 的 $G_t$，保存在该 $s$ 的 Value 数组内，计数加 1。

主程序部分：

【代码位置】MC_104_FrozenLake_V_Evaluation.py

```python
if __name__=="__main__":
    gamma = 1
    episodes = 50000
    env = gym.make("FrozenLake-v1", desc=None, map_name = "4x4", is_slippery=False)
    # 随机策略
    nA = env.action_space.n
    nS = env.observation_space.n
    policy = np.ones(shape=(nS, nA)) / nA # 每个状态上的每个动作都有0.25的备选概率
    # DP 方法
    V_real = get_groud_truth(env, policy, gamma)
    # MC 方法
    start_state, info = env.reset(seed=5, return_info=True)
    V = algoMC.MC_EveryVisit_V_Policy(env, start_state, episodes, gamma, policy)
    env.close()
```

主程序如上，设置参数，初始化环境，生成策略，用 DP 方法得到准确解，再用 MC 方法循环多次做比对，最后关闭环境。

### 10.4.3 误差的平均值 vs. 平均值的误差

很多文献都提到了蒙特卡洛方法是具有高方差低偏差的特点，但是如何证明呢？笔者在这里做了一个实验，来大致说明偏差方差的问题。

假设有一个均值为 3.1 的不知名分布，我们从中采样四次，得到序列 $x=[1,2,4,5]$，则：

- 均值为：$\mu=(1+2+4+5)/4=3$
- 方差为：$var(x)=[(1-3)^2+(2-3)^2+(4-3)^2+(5-3)^2]/4=2.5$
- 偏差为：$bias(x)=\sqrt{(3-3.1)^2}=0.1$

方差很大，偏差很小。

根据这个提示，我们可以设计这样一个试验：

1. 循环 2000 幕，根据这 2000 幕的采样数据，计算出 V 值；
2. 做 step 1 十次，就会得到 $V_1,V_2,\cdots,V_{10}$ 共 10 个值；
3. 用动态规划迭代法计算出 $V_*$，作为计算 RMSE 误差的基准；
4. 用 $V_1,V_2,\cdots,V_{10}$ 分别与 $V_*$ 计算 RMSE，得到 10 个误差值 $E_1,E_2,\cdots,E_{10}$，然后计算它们的均值 $E_0=(E_1+\cdots+E_{10})/10$，这就是误差的平均值；
5. 计算 $V_0=(V_1+\cdots+V_{10})/10$，得到 $V$ 的平均值，然后计算 $V_0$ 与 $V_*$ 的 RMSE，得到 $E'_0$，这就是平均值的误差；
6. 比较 $E_0$ 与 $E'_0$ 的大小。

另外，我们在 2000 幕循环中，每隔 100 次就计算一次上述的 $E_0,E'_0$，这样一共会得到 20 组数据，绘制变化曲线如图 10.4.2。

<center>
<image src="./img/MC-104-V-Error.png">

图 10.4.2 循环 2000 次的误差
（左图：10 次运行得到的 V 的平均值的误差 $E'_0$；右图：10 次运行得到的 V 的误差的平均值 $E_0$）
</center>

比较左右两图，可以看到左图所示的平均值的误差，起点就比较小，在 0.03，后面的误差可以小到 0.005；右图的误差起点在 0.1 左右，最后的误差是 0.02。所以，$E'_0 < E_0$，而且是小一个数量级。

- $E_1,\cdots,E_{10}$ 每一个的误差值都很大，说明 $V_1,\cdots,V_{10}$ 的方差较大。
- $V_0$ 是 $V_1,\cdots,V_{10}$ 的均值，虽然它们每一个距离 $V_*$ 都比较远，但是它们的平均值 $V_0$ 距离 $V_*$ 比较近，说明偏差较小。

打印出两组试验的最后一步的数据如下：

```
====================平均值的误差 E'0 ====================
----- 状态价值 V 的平均值 -----
[[0.014 0.012 0.021 0.013]
 [0.016 0.    0.041 0.   ]
 [0.034 0.085 0.145 0.   ]
 [0.    0.163 0.438 0.   ]]
----- 状态价值 V 的平均值的误差 -----
0.0034371081743170516

====================误差的平均值 E0 ====================
----- 最后一个状态价值 V -----
[[0.012 0.008 0.018 0.014]
 [0.018 0.    0.033 0.   ]
 [0.042 0.094 0.146 0.   ]
 [0.    0.145 0.45  0.   ]]
----- 最后一个状态价值 V 的误差 -----
0.01752455053676336
```

有些读者可能会怀疑循环次数不够，造成右图还没有收敛到最佳状态，所以我们把循环次数从 2000 次增加到 10000 次，再看结果如图 10.4.3 所示。

<center>
<image src="./img/MC-104-V-Error2.png">

图 10.4.3 循环 10000 次的误差
（左图：10 次运行得到的 V 的平均值的误差 $E'_0$；右图：10 次运行得到的 V 的误差的平均值 $E_0$）
</center>

可以看到两者都收敛了，左图的误差可以到 0.002 左右，而右图的误差在 0.015 左右，还是相差了一个数量级。

所以，我们得到的结论是，对于这种高方差低偏差问题，某一次的结果很难让人满意。如何减少方差呢？有兴趣的读者可以自行查询一下以下概念：

- 分层采样法
- 重要采样法
- 相关采样法
- 对偶变量法

当然，最简单的办法，是利用其低偏差的特点，多次试验取平均，往往可以事半功倍。
