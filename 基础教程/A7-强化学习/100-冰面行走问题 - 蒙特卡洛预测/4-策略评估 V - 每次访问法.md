## 10.4 策略评估（预测）$V_\pi$

在 8.5 节中，曾经使用贝尔曼期望方程来针对有模型的问题做策略评估，在本节中，将会针对无模型问题做策略评估，也叫做预测（Predication）。

### 10.4.1 估算 $v_\pi$ 和 $q_\pi$

在 10.2 节中，我们是使用**首次访问法**，估算了 MRP 问题中的状态价值函数 $V$，也在一开始就提到过，这个方法同样适用于估算 MDP 问题中的价值函数 $v_\pi$ 和 $q_\pi$。

与 MRP 不同的是，在 MDP 中由于引入了**策略**和**动作**的概念，所以，在当前状态 $s$ 下，要根据策略（policy）选择动作（action），执行动作与环境交互，得到下一个状态、奖励值等信息。

表 10.4.1 MRP 与 MDP 的比较

||MRP|MDP|
|-|-|-|
|概念|价值评估|策略评估|
|方法|根据 $s$ 直接得到 $s'$|根据 $s$ 查询策略 $\pi \to$ 得到动作 $a$ 并执行 $\to$ 获得奖励 $r$ 和 $s'$。|
|伪代码|s',r = env.step(s)|a = policy(s) # 策略决定动作<br> s',r = env.step(s,a)|
|结果|状态价值函数 $V$|状态价值函数 $V_\pi$ 和动作价值函数 $Q_\pi$|

首先看状态价值函数的定义：

$$
v_\pi(s) \doteq \mathbb E [G_t \mid S_t=s] \tag{10.4.1}
$$

这和式（10.2.2）没有区别，所以，算法所需要的参数以及过程都和 10.2 节中一样，只不过是运行在一个 MDP 环境下而已。

另外一个问题是：只有 $V_\pi$ 就足够了吗？答案是否定的。

<center>
<img src="./img/fromV2Q.png">

图 10.4.1 状态价值函数 $V_\pi$ 决定动作价值函数 $Q_\pi$
</center>

如图 10.4.1 中，一个 9 个单元的方格世界中，假设已经知道了 $v(s_1)=1.6,v(s_3)=1.3,v(s_5)=1.2,v(s_7)=1.9$，当智能体处于 $s_4$ 的位置时，它一定应该向具有最大价值的 $s_7$ 移动吗？

不一定！看动作价值函数的定义：

$$
\begin{aligned}
q_\pi(s,a) & \doteq \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=P^a_{ss'}[R^a_{ss'}+\gamma V_*(s')]
\end{aligned}
\tag{10.4.2}
$$

在有模型的情况下，动作价值由三个因子组成：$R, P, V$。虽然 $v(s_7)$ 最大，但是 $R,P$ 都未知的情况下，不能确定就应该向 $s_7$ 移动。假设 $R^a_{s_4,s_7}=R^a_{s_4,s_5}=1, P^a_{s_4,s_7}=0.5, P^a_{s_4,s_5}=1, \gamma=1$，则：

- $q(s_4,a_{RIGHT})=1 \cdot [1+1\cdot 1.2]=2.2 $
- $q(s_4,a_{DOWN})=0.5 \cdot [1+1\cdot 1.9]=1.45 $

由于 $2.2 > 1.45$，那么智能体应该选择向右移动。

在无模型的情况下，如果利用式（10.4.1）估算出了 $v_\pi$ 后，能直接算出 $q_\pi$ 吗？不能！虽然 $R$ 可以通过与环境交互得到，但是因为不知道 $P$，所以无法计算 $q_\pi$。

在这种情况下，我们只能通过式（10.4.2）的第一行的定义来估算 $q_\pi$。估算出 $q_\pi$ 后，才能指导智能体按照从 $q_\pi$ 中提出的策略 $\pi(s) \doteq \argmax_a q_\pi(s,a)$ 来行动。所以，在本章后续的内容中，都会把估算 $q_\pi$ 作为目标。

所以，在无模型的情况下，计算动作价值 $q_\pi$ 比起计算状态价值 $v_\pi$ 更有用。在本小节中，我们先学习预测 $V_\pi$，下一小节再学习预测 $Q_\pi$。

### 10.4.2 每次访问法估算 $V_\pi$

#### 算法描述

【算法 10.4.1】每次访问型蒙特卡洛法。下面的伪代码中，$\leftarrow$ 表示赋值，$\Leftarrow$ 表示追加列表。

---

输入：起始状态 $s$，策略 $\pi$，折扣 $\gamma$, 幕数 Episodes
初始化数组：$G(S) \leftarrow 0, N(S) \leftarrow 0$，$S$ 为状态空间
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略 $\pi$ 得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,r)$，相当于是 $(s_t,r_{t+1})$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $(s_t,r_{t+1})$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t) \leftarrow G(s_t)+G_t$
　　　　$N(s_t) \leftarrow N(s_t)+1$
$V(S) \leftarrow G(S) / N(S)$
输出：$V(S)$

---

为什么叫做**每次访问法**呢？区别于**首次访问法**，在算法中，在遍历过程中，每次只要遇到 $s_t$ 就要计算它的 $G$ 值，而不管它出现在哪里或者出现了多少次。Sutton 认为这两者具有不同的理论基础，首次访问法被研究得很多，具有很强的理论基础；而每次访问法可以过渡到后面要学到的函数近似与资格迹方法，而且从实际效果来看，每次访问法能稍微快一些地收敛，因为样本利用效率更高，相当于采样次数增加了 2~3 倍。

在算法中，这两者的唯一区别就是在对 $Episode$ 从后向前遍历时，是否要检查 $s_t$ 是否曾经出现过。抛开这个区别，算法 10.4.1 与算法 10.2 的区别是：
- 算法 10.4.1 要输入策略 $\pi$, 因为要估算 $V_\pi$
- 算法 10.4.1 要从策略 $\pi$ 选择动作 $a$ 并执行，但是并不会把 $a$ 加入到分幕序列中。



#### 算法实现

【代码位置】MC_104_FrozenLake_V_Evaulation.py

下面我们把代码分成几个小片段来讲解。

函数定义即初始化部分：


```python
# MC 策略评估（预测）：每次访问法估算 V_pi
def MC_EveryVisit_V_Policy(env, start_state, episodes, gamma, policy, checkpoint=1000, delta=1e-3):
    nS = env.observation_space.n    # 状态空间
    nA = env.action_space.n         # 动作空间
    Value = np.zeros(nS)            # G 的总和
    Count = np.zeros(nS)            # G 的数量
    V_old = np.zeros(nS)            # 保存旧值便于比较是否收敛
```

输入：

- env - 环境。
- start_state - 起始状态。
- episodes - 最大循环幕数。
- gamma - 折扣。
- policy - 策略。
- checkpoint - 检查点，缺省每 1000 次循环检查一次。
- delta - 误差，缺省值 1e-3。

初始化部分：

- 获得环境的状态空间，以便初始化数组。
- 获得动作空间，以便在后面选择动作时使用。
- Value 数组保存每个状态上的累积 G 值，除以下面的 Count，就会得到平均值 V。
- Count 数组保存每个状态上的累积数量。
- V_old 数组保存迭代中上一次检查时的数据，将会与最新数据比较，以确定是否收敛。

多幕循环部分：

```python
    for episode in tqdm.trange(episodes):   # 多幕循环
        Episode = []        # 一幕内的(状态,奖励)序列
        s = start_state     # 每次重置 s
        done = False
```

这个片段进行多幕循环，为每一幕初始化：
- Episode 保存分幕采样序列。
- s 指定起始状态。
- done 用于判断是否结束本幕。

幕内循环部分：

```python
        while (done is False):              # 幕内循环
            action = np.random.choice(nA, p=policy[s])  # 根据策略选择动作
            next_s, reward, done, _ = env.step(action)   # 与环境交互
            Episode.append((s, reward))     # 保存数据序列
            s = next_s      # 迭代
```

这一片段进行分幕采样，从策略得到动作，执行动作得到 $s',r_{t+1}$，并保存到序列中。当 done 为 True 时表示分幕结束，到达终止状态。

G 值计算部分：

```python
        # 从后向前遍历计算 G 值
        G = 0   # 不要忘记清零
        for t in range(len(Episode)-1, -1, -1):
            s, r = Episode[t]   # 取出数据
            G = gamma * G + r   # 计算 G_t
            Value[s] += G       # 值累加
            Count[s] += 1       # 数量加 1
        # 重置环境，开始新的一幕采样
        s, info = env.reset(return_info=True)
```

这一片段进行倒序遍历，计算每个时刻关于 $s$ 的 $G_t$，保存在该 $s$ 的 Value 数组内，计数加 1。然后 reset 环境准备下一幕。

收敛检测部分：

```python
        # 检查是否收敛
        if (episode + 1)%checkpoint == 0: 
            Count[Count==0] = 1 # 把分母为0的填成1，主要是对终止状态
            V = Value / Count
            if abs(V-V_old).max() < delta:   # 小于此值认为收敛
                break
            V_old = V.copy()
```

这一片段是可选的，用于判断何时可以结束多幕循环。但是里面有两个技术点需要注意：

- 阈值 delta=1e-3 是一个经验值，针对此问题，这个精度已经足够了。如果再小的话，会明显增加循环次数，没有多大意义；如果太大的话，估算的精度不够。
- checkpoint=1000 这个数是指每循环 1000 次（幕）检查一下收敛情况。这个值不能太小，因为分幕采样很有可能掉入冰洞而得到很多的 0 奖励，这样的话 V 将会等于 V_old，从而失去收敛判断的意义。

主程序部分：

```python
if __name__=="__main__":
    gamma = 1
    episodes = 50000
    env = gym.make("FrozenLake-v1", desc=None, map_name = "4x4", is_slippery=False)
    # 随机策略
    nA = env.action_space.n
    nS = env.observation_space.n
    policy = np.ones(shape=(nS, nA)) / nA # 每个状态上的每个动作都有0.25的备选概率
    # DP 方法
    V_real = get_groud_truth(env, policy, gamma)
    # MC 方法
    start_state, info = env.reset(seed=5, return_info=True)
    V = algoMC.MC_EveryVisit_V_Policy(env, start_state, episodes, gamma, policy)
    env.close()
```

主程序如上，设置参数，初始化环境，生成策略，用 DP 方法得到准确解，再用 MC 方法循环多次做比对，最后关闭环境。

运行结果：

```
循环幕数 = 42000
------ 状态价值函数 -----
[[0.014 0.013 0.021 0.008]
 [0.014 0.    0.042 0.   ]
 [0.031 0.082 0.142 0.   ]
 [0.    0.172 0.445 0.   ]]
误差 = 0.002499810403461569
```

注意，每次运行得到循环幕数都会不一样，无法控制。理想的情况是运行多次后取误差的平均值（注意不是取 $V_\pi$ 的平均值）。

如果每 1000 个循环都记录一下误差的话，可以得到图 10.4.2 的曲线。

<center>
<image src="./img/MC-104-V-Error.png">

图 10.4.2 误差与循环次数的关系
</center>

可以看到误差情况很不平稳，这也是蒙特卡洛法的一个特点，不经过几万次的循环，无法得到比较准确的估算值。

