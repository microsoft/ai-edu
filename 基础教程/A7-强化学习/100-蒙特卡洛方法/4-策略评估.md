## 10.4 策略评估（预测）

在 8.5 节中，曾经使用贝尔曼期望方程来针对有模型的问题做策略评估，在本节中，将会针对无模型问题做策略评估，也叫做预测（Predication）。

### 10.4.1 估算 $v_\pi$ 和 $q_\pi$

在 10.2 节中，我们是使用**首次访问法**，估算了 MRP 问题中的状态价值函数 $V$，也在一开始就提到过，这个方法同样适用于估算 MDP 问题中的价值函数 $v_\pi$ 和 $q_\pi$。

与 MRP 不同的是，在 MDP 中由于引入了**策略**和**动作**的概念，所以，在当前状态 $s$ 下，要根据策略（policy）选择动作（action），执行动作与环境交互，得到下一个状态、奖励值等信息。

表 10.4.1 MRP 与 MDP 的比较

||MRP|MDP|
|-|-|-|
|概念|价值评估|策略评估|
|方法|根据 $s$ 直接得到 $s'$|根据 $s$ 查询策略 $\pi \to$ 得到动作 $a$ 并执行 $\to$ 获得奖励 $r$ 和 $s'$。|
|伪代码|s',r = env.step(s)|a = policy(s) # 策略决定动作<br> s',r = env.step(s,a)|
|结果|状态价值函数 $V$|状态价值函数 $V_\pi$ 和动作价值函数 $Q_\pi$|

首先看状态价值函数的定义：

$$
v_\pi(s) \doteq \mathbb E [G_t \mid S_t=s] \tag{10.4.1}
$$

这和式（10.2.2）没有区别，所以，算法所需要的参数以及过程都和 10.2 节中一样，只不过是运行在一个 MDP 环境下而已。

另外一个问题是：只有 $V_\pi$ 就足够了吗？答案是否定的。

<center>
<img src="./img/fromV2Q.png">

图 10.4.1 状态价值函数 $V_\pi$ 决定动作价值函数 $Q_\pi$
</center>

如图 10.4.1 中，一个 9 个单元的方格世界中，假设已经知道了 $v(s_1)=1.6,v(s_3)=1.3,v(s_5)=1.2,v(s_7)=1.9$，当智能体处于 $s_4$ 的位置时，它一定应该向具有最大价值的 $s_7$ 移动吗？

不一定！看动作价值函数的定义：

$$
\begin{aligned}
q_\pi(s,a) & \doteq \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=P^a_{ss'}[R^a_{ss'}+\gamma V_*(s')]
\end{aligned}
\tag{10.4.2}
$$

在有模型的情况下，动作价值由三个因子组成：$R, P, V$。虽然 $v(s_7)$ 最大，但是 $R,P$ 都未知的情况下，不能确定就应该向 $s_7$ 移动。假设 $R^a_{s_4,s_7}=R^a_{s_4,s_5}=1, P^a_{s_4,s_7}=0.5, P^a_{s_4,s_5}=1, \gamma=1$，则：

- $q(s_4,a_{RIGHT})=1 \cdot [1+1\cdot 1.2]=2.2 $
- $q(s_4,a_{DOWN})=0.5 \cdot [1+1\cdot 1.9]=1.45 $

由于 $2.2 > 1.45$，那么智能体应该选择向右移动。

在无模型的情况下，如果利用式（10.4.1）估算出了 $v_\pi$ 后，能直接算出 $q_\pi$ 吗？不能！虽然 $R$ 可以通过与环境交互得到，但是因为不知道 $P$，所以无法计算 $q_\pi$。

在这种情况下，我们只能通过式（10.4.2）的第一行的定义来估算 $q_\pi$。估算出 $q_\pi$ 后，才能指导智能体按照从 $q_\pi$ 中提出的策略 $\pi(s) \doteq \argmax_a q_\pi(s,a)$ 来行动。所以，在本章后续的内容中，都会把估算 $q_\pi$ 作为目标。

所以，在无模型的情况下，计算动作价值 $q_\pi$ 比起计算状态价值 $v_\pi$ 更有用。


### 10.4.2 每次访问法估算 $V_\pi$

#### 算法描述

【算法 10.2.1】每次访问型蒙特卡洛法。下面的伪代码中，$\leftarrow$ 表示赋值，$\Leftarrow$ 表示追加列表。

---

输入：起始状态 $s,\gamma$, Episodes
初始化数组：$G(S) \leftarrow 0, N(S) \leftarrow 0$
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,r)$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $s_t,r_{t+1}$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t) \leftarrow G(s_t)+G_t$
　　　　$N(s_t) \leftarrow N(s_t)+1$
$V(S) \leftarrow G(S) / N(S)$
输出：$V(S)$

---

为什么叫做“每次访问法”呢？因为在算法中，在遍历过程中，每次只要遇到 $s_t$ 就要计算它的 $G$ 值，而不管它出现在哪里或者出现了多少次。


#### 算法实现

【代码位置】MC_102_EveryVisit.py

```python
# MC2-EveryVisit - 每次访问法
def MC_EveryVisit(dataModel, start_state, episodes, gamma):
    Value = np.zeros(dataModel.nS)  # G 的总和
    Count = np.zeros(dataModel.nS)  # G 的数量
    for episode in tqdm.trange(episodes):   # 多幕循环
        Trajectory = []     # 一幕内的(状态,奖励)序列
        s = start_state
        is_end = False
        while (is_end is False):            # 幕内循环
            next_s, reward, is_end = dataModel.step(s)   # 从环境获得下一个状态和奖励
            Trajectory.append((s.value, reward))
            s = next_s

        num_step = len(Trajectory)
        G = 0
        # 从后向前遍历计算 G 值
        for t in range(num_step-1, -1, -1):
            s, r = Trajectory[t]
            G = gamma * G + r
            Value[s] += G     # 值累加
            Count[s] += 1     # 数量加 1
    
    Count[Count==0] = 1 # 把分母为0的填成1，主要是终止状态
    return Value / Count    # 求均值
```
运行结果：

```
gamma = 1.0
状态价值函数计算结果(数组) : [ 1.03  1.7   2.71  2.83 -5.2  -6.71  6.   -2.3  -1.    5.    0.  ]
Start:       1.03
Normal:      1.7
Pedestrians: 2.71
DownSpeed:   2.83
ExceedSpeed:-5.2
RedLight:   -6.71
LowSpeed:    6.0
MobilePhone:-2.3
Crash:      -1.0
Goal:        5.0
End:         0.0
耗时 : 4.566272258758545
误差 = 0.06109795839487419
```


### 10.4.3 每次访问法估算 $Q_\pi$


给两个策略，一个随机，一个定向


---

输入：起始状态 $s,\gamma$, Episodes
初始化数组：$G(S,A) \leftarrow 0, N(S,A) \leftarrow 0$
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,a,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,a,r)$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $s_t,a_t,r_{t+1}$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t,a_t) \leftarrow G(s_t,a_t)+G_t$
　　　　$N(s_t,a_t) \leftarrow N(s_t,a_t)+1$
$Q(S,A) \leftarrow G(S,A) / N(S,A)$
输出：$Q(S,A)$

---

