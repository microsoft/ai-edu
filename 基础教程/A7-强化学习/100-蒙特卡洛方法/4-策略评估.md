## 10.4 策略评估


#### 估算 $v_\pi$ 和 $q_\pi$

在 10.2 节中，我们是使用**首次访问法**和**每次访问法**，估算了 MRP 问题中的状态价值函数，也在一开始就提到过，这两个方法同样适用于估算 MDP 问题中的状态价值函数，包括状态价值函数和动作价值函数。

但是在 MDP 中，由于引入了**策略**和**动作**的概念，所以，在当前状态 $s$ 下，要根据策略（policy）选择动作（action），执行动作与环境交互，得到下一个状态、奖励值等信息。


||MRP|MDP|
|-|-|-|
|概念|价值评估|策略评估|
|方法|根据 $s$ 直接得到 $s'$|根据 $s$ 查询策略 $\pi \to$ 得到动作 $a$ 并执行 $\to$ 获得奖励 $r$ 和 $s'$。|
|结果|状态价值函数 $V$|状态价值函数 $V_\pi$ 和动作价值函数 $Q_\pi$|

首先看状态价值函数的定义：

$$
v_\pi(s) \doteq \mathbb E [G_t \mid S_t=s] \tag{10.4.1}
$$

这和式（10.2.2）没有区别，所以，算法所需要的参数以及过程都和 10.2 节中一样，只不过是运行在一个 MDP 环境下而已。

另外一个问题是：只有 $V_\pi$ 就足够了吗？答案是否定的。

<center>
<img src="./img/fromV2Q.png">

图 10.4.1 状态价值函数 $V_\pi$ 决定动作价值函数 $Q_\pi$
</center>

如图 10.4.1 中，一个 9 个单元的方格世界中，假设已经知道了 $v(s_1)=1.6,v(s_3)=1.3,v(s_5)=1.2,v(s_7)=1.9$，当智能体处于 $s_4$ 的位置时，它一定应该向具有最大价值的 $s_7$ 移动吗？

不一定！看动作价值函数的定义：

$$
\begin{aligned}
q_\pi(s,a) & \doteq \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=P^a_{ss'}[R^a_{ss'}+\gamma V_*(s')]
\end{aligned}
\tag{10.4.2}
$$

在有模型的情况下，动作价值由三个因子组成：$R, P, V$。虽然 $v(s_7)$ 最大，但是 $R,P$ 都未知的情况下，不能确定就应该向 $s_7$ 移动。假设 $R^a_{s_4,s_7}=R^a_{s_4,s_5}=1, P^a_{s_4,s_7}=0.5, P^a_{s_4,s_5}=1, \gamma=1$，则：

- $q(s_4,a_{RIGHT})=1 \cdot [1+1\cdot 1.2]=2.2 $
- $q(s_4,a_{DOWN})=0.5 \cdot [1+1\cdot 1.9]=1.45 $

由于 $2.2 > 1.45$，那么智能体应该选择向右移动。

在无模型的情况下，如果利用式（10.4.1）估算出了 $v_\pi$ 后，能直接算出 $q_\pi$ 吗？不能！虽然 $R$ 可以通过与环境交互得到，但是因为不知道 $P$，所以无法计算 $q_\pi$。

在这种情况下，我们只能通过式（10.4.2）的第一行的定义来估算 $q_\pi$。估算出 $q_\pi$ 后，才能指导智能体按照从 $q_\pi$ 中提出的策略 $\pi(s) \doteq \argmax_a q_\pi(s,a)$ 来行动。所以，在本章后续的内容中，都会把估算 $q_\pi$ 作为目标。






#### 探索性出发（Exploring Starts）

在 8.4 节中，我们学习过 Q 函数的定义。如果应用到冰面行走问题上，Q 函数表格会如表 10.4.1 所示。

表 10.4.1 冰面行走问题的 Q 函数表格

|状态 $\to$ 动作|UP|LEFT|DOWN|RIGHT|
|:-:|:-:|:-:|:-:|:-:|
|$s_0$|$q_\pi(s_0,a_0)$|$q_\pi(s_0,a_1)$|$q_\pi(s_0,a_2)$|$q_\pi(s_0,a_3)$|
|$s_1$|$q_\pi(s_1,a_0)$|$q_\pi(s_1,a_1)$|$q_\pi(s_1,a_2)$|$q_\pi(s_1,a_3)$|
|...|...|...|...|...|
|$s_{14}$|$q_\pi(s_{14},a_0)$|$q_\pi(s_{14},a_1)$|$q_\pi(s_{14},a_2)$|$q_\pi(s_{14},a_3)$|
|$s_{15}$|$q_\pi(s_{15},a_0)$|$q_\pi(s_{15},a_1)$|$q_\pi(s_{15},a_2)$|$q_\pi(s_{15},a_3)$|

状态-动作的组合构成 Q 表格，一共有 16x4=64 个组合。如果想评价在某个状态上哪个动作最好，那么最起码要在该状态上尝试完所有动作后才会有评价的基础。

而在另外一些问题中，可以在表 10.4.1 中遍历所有单元格作为问题的起点，然后开始与环境交互进行分幕采样，比如 21 点游戏，就可以任意指定初始时的手牌点数；但是在冰面行走问题中，由于环境的 env.reset() 函数总会返回定义好的 $s_0$ 状态为起点，所以我们无法进行上述组合遍历。所以探索性出发这条路走不通。



