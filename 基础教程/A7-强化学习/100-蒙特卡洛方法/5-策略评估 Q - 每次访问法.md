## 10.5 策略评估（预测）$Q_\pi$

在 10.4 小节中，我们学习如何预测 $V_\pi$，而且也提到过，预测 $V_\pi$ 的意义其实不是很大，更有用的还是预测 $Q_\pi$。所以，在本小节中，我们将会


### 10.5.1 每次访问法估算 $Q_\pi$



【算法 10.5】

---

输入：起始状态 $s$，策略 $\pi$，折扣 $\gamma$, 幕数 Episodes
初始化数组：$G(S,A) \leftarrow 0, N(S,A) \leftarrow 0$，$S$ 为状态空间，$A$ 为动作空间
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,a,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略 $\pi$ 得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,a,r)$，相当于是 $(s_t,a_t,r_{t+1})$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $(s_t,a_t,r_{t+1})$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t,a_t) \leftarrow G(s_t,a_t)+G_t$
　　　　$N(s_t,a_t) \leftarrow N(s_t,a_t)+1$
$Q(S,A) \leftarrow G(S,A) / N(S,A)$
输出：$Q(S,A)$

---

与算法 10.4 的重要区别：

- 初始化的数组为二维 (S,A)，记录每个状态 $s$ 下的每个动作 $a$ 的 G 值。
- Episode 中保存 $s,a,r$，比算法 10.4 多一个动作 $a$。



给两个策略，一个随机，一个定向


```python
    policy_names = ["正确方向", "随机方向","错误方向"]
    policies = [
        # left, down, right, up
        (0.10, 0.40, 0.40, 0.10),
        (0.25, 0.25, 0.25, 0.25), 
        (0.40, 0.10, 0.10, 0.40)
    ]
```



<center>
<image src="./img/MC-105-3Policy.png">

图 10.5.1 三种策略
</center>

- 正确方向

    因为起点在左上角，终点在右下角，所以给

- 随机方向
- 错误方向
