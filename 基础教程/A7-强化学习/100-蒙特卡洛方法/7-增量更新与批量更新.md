
- 每次访问，首次访问，估算 MRP 的 V
- MDP 平稳
    - 试探性出发，可以被 e-soft 代替
    - GLIM,  e=1/k, k 为迭代次数，k 越大，信心越强，e越小
- 非平稳
    - 同轨策略控制 
    - 离轨策略 与 重要度采样



### 非平稳环境

什么是非平稳环境？

我们前面学习的例子，基本上都是平稳环境，即：到达状态 $s_t$ 后，一定可以得到奖励 $R_{t+1}$。下面我们举一些非平稳环境的例子。

- 良好路况下驾驶汽车，因为路面的摩擦力（可以认为是）恒定，司机可以按照通常的经验进行加速、转弯、刹车等动作。但是一旦遇到雨雪天气，以上动作都会导致车俩不会按照预期行驶，加速时前轮会空转，转弯时车辆会侧滑，刹车时需要更长的制动距离。

- 打游戏与程序（Program）对战时，程序会按照预先设计好的一些套路来进攻或者防守，摸清这些套路，人类玩家就会胜利。但是如果与人工智能（AI）对战时，对手会根据人类玩家的应对方式实时地修改策略，这样前面学习的经验就没用了，必须随机应变。

- 在股票市场中，一般的散户或者小机构可以按照自己的策略进行正常投资，然后根据市场行情进行调整、验证。但是大的机构会遇到一些麻烦，比如一个基金公司有 100 亿的规模，那么它的买卖行为会对某只股票、某个板块、甚至对大盘造成影响，它的策略必须考虑到这种影响。

以上这些例子，如果用到强化学习的环境中，都会造成当遇到 $s_t$ 状态时，有时会有很高的正向奖励，有时却是很低的正向奖励，甚至是负向奖励。

### 增量更新



### 批量更新算法


$$
V(s) = V(s) + \alpha (G_t - V)
$$


