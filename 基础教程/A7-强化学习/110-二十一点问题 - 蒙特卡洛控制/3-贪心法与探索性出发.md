## 11.3 贪心法与探索性出发

### 11.3.1 贪心法

仍然用上一小节中的简单的方格世界举例，来理解**策略改进**是如何工作的。

根据图 11.1.2 所示，我们的算法可以这样写


【算法 11.3】

---

输入：起始状态 $s$，策略 $\pi$，折扣 $\gamma$, 幕数 Episodes
初始化数组：$G(S,A) \leftarrow 0, N(S,A) \leftarrow 0$，$S$ 为状态空间，$A$ 为动作空间
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,a,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略 $\pi$ 得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,a,r)$，相当于是 $(s_t,a_t,r_{t+1})$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $(s_t,a_t,r_{t+1})$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t,a_t) \leftarrow G(s_t,a_t)+G_t$
　　　　$N(s_t,a_t) \leftarrow N(s_t,a_t)+1$
　　　　$Q(s_t,A) \leftarrow G(s_t,A) / N(s_t,A)$
　　　　$\pi(s_t) \leftarrow \argmax_a Q(s_t,A)$ # 取出最大值对应的动作序号
输出：$Q(S,A)$

---


对比算法 10.5，前面的部分都是相同的，因为同样是计算 $Q$ 值。不同之处在于新增加了两行：

- 倒数第三行，$Q(s_t,A) \leftarrow G(s_t,A) / N(s_t,A)$，大写 $A$ 表示数组操作，同时计算某个状态下的所有动作的 $Q$ 值，便于后面对于该状态下的所有动作做比较；
- 倒数第二行，修改策略 $\pi$

当我们信心满满地用代码实现了这个算法后，实际运行时，会发现一个问题：运行了几幕后，程序就停在某处不动了！Debug 后发现，有两种情况导致程序在幕内循环时，始终不能到达终止状态：

1. 在某个边角位置的状态，智能体出界后回到原地，下一步又出界，还是回到原地，始终不能离开这个状态；
2. 在某几个状态内来回移动，形成闭环。比如从 $s_4$ 向右移动到 $s_5$；在 $s_5$ 向左移动，又回到 $s_4$。




#### 探索性出发（Exploring Starts）

在 8.4 节中，我们学习过 Q 函数的定义。如果应用到冰面行走问题上，Q 函数表格会如表 10.4.1 所示。

表 10.4.1 二十一点问题的 Q 函数表格

|状态 $\downarrow$ 动作$\to$|HIT|Stick|
|:-:|:-:|:-:|
|12|$q_\pi(12,a_0)$|$q_\pi(12,a_1)$|$q_\pi(s_0,a_2)$|$q_\pi(s_0,a_3)$|
|13|$q_\pi(13,a_0)$|$q_\pi(13,a_1)$|$q_\pi(s_1,a_2)$|$q_\pi(s_1,a_3)$|
|...|...|...|...|...|
|20|$q_\pi(20,a_0)$|$q_\pi(20,a_1)$|
|21|$q_\pi(21,a_0)$|$q_\pi(21,a_1)$|

状态-动作的组合构成 Q 表格，一共有 10x2=20 个组合。如果想评价在某个状态上哪个动作最好，那么最起码要在该状态上尝试完所有动作后才会有评价的基础。



