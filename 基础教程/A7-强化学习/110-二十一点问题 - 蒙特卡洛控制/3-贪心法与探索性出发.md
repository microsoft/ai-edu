## 11.3 贪心法与探索性出发

### 11.3.1 贪心法（Greedy）

仍然用上一小节中的简单的方格世界举例，来理解**策略改进**是如何工作的。

根据图 11.1.2 所示，我们的算法可以这样写


【算法 11.3】

---

输入：起始状态 $s$，策略 $\pi$，折扣 $\gamma$, 幕数 Episodes
初始化数组：$G(S,A) \leftarrow 0, N(S,A) \leftarrow 0$，$S$ 为状态空间，$A$ 为动作空间
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,a,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略 $\pi$ 得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,a,r)$，相当于是 $(s_t,a_t,r_{t+1})$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $(s_t,a_t,r_{t+1})$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t,a_t) \leftarrow G(s_t,a_t)+G_t$
　　　　$N(s_t,a_t) \leftarrow N(s_t,a_t)+1$
　　　　$Q(s_t,A) \leftarrow G(s_t,A) / N(s_t,A)$
　　　　$\pi(s_t) \leftarrow \argmax_a Q(s_t,A)$ # 取出最大值对应的动作序号
输出：$Q(S,A)$

---


对比算法 10.5，前面的部分都是相同的，因为同样是计算 $Q$ 值。不同之处在于新增加了两行：

- 倒数第三行，$Q(s_t,A) \leftarrow G(s_t,A) / N(s_t,A)$，大写 $A$ 表示数组操作，同时计算某个状态下的所有动作的 $Q$ 值，便于后面对于该状态下的所有动作做比较；
- 倒数第二行，修改策略 $\pi$

以下代码为初始化环境部分：

```python
    env = model.GridWorld(
        GridWidth, GridHeight, StartStates, EndStates,  # 关于状态的参数
        Actions, Policy, Transition,                    # 关于动作的参数
        StepReward, SpecialReward,                      # 关于奖励的参数
        SpecialMove, Blocks)                            # 关于移动的限制
    #model.print_P(env.P_S_R)
    policy = helper.create_policy(env.nS, env.nA, (0.25, 0.25, 0.25, 0.25))
    gamma = 1
    max_iteration = 2000
```

可以看到初始的策略是随机的，每个动作的选择概率都是 0.25。折扣为 1（无折扣）。最大采样幕数为 2000（当然可以再多些）。

采样部分的代码和以前的相同：

```python
        # 重置环境，开始新的一幕采样
        s = env.reset()
        Episode = []     # 一幕内的(状态,动作,奖励)序列
        done = False
        while (done is False):            # 幕内循环
            action = np.random.choice(nA, p=policy[s])  # 策略：选择动作
            next_s, reward, done, _ = env.step(action)  # 与环境交互
            Episode.append((s, action, reward))         # 保存S,A,R
            s = next_s                                  # 迭代
```

其中，我们用概率来表示策略。

下面的代码是计算 $Q$ 值及策略改进：

```python
def MC_EveryVisit_Q_Policy_test(env, episodes, gamma, policy):
    ......
        num_step = len(Episode)
        G = 0
        # 从后向前遍历计算 G 值
        for t in range(num_step-1, -1, -1):
            s, a, r = Episode[t]
            G = gamma * G + r
            Value[s,a] += G     # 值累加
            Count[s,a] += 1     # 数量加 1
            # 做策略改进，贪心算法
            Q[s] = Value[s] / Count[s]  # 得到该状态下所有动作的 q 值
            policy[s] = 0               # 先设置该状态所有策略为 0（后面再把有效的动作设置为非 0）
            argmax = np.argwhere(Q[s]==np.max(Q[s]))    # 取最大值所在的位置（可能不止一个）
            p = 1 / argmax.shape[0]                     # 概率平均分配给几个最大值
            for arg in argmax:                          # 每个最大值位置都设置为相同的概率
                policy[s][arg[0]] = p
```

其中，“概率平均分配”的意思是：
- 如果最大值只有一个，比如 [0.23, 0., 0., 0. ]，那么策略会是 [1, 0, 0, 0]；
- 如果最大值只有两个，比如 [0.54, 0., 0.54., 0. ]，那么策略会是 [0.5, 0, 0.5, 0]；
- 以此类推，三个时概率是 0.333，四个时概率是 0.25。

当我们信心满满地用代码实现了这个算法后，实际运行时，会发现一个问题：运行了几幕后，程序就停在某处不动了！通过审查算法可以知道，能造成死循环的只有分幕采样部分。Debug 后发现，有两种情况导致程序在幕内循环时，始终不能到达终止状态：

1. 在某个边角位置的状态。智能体出界后回到原地，下一步又出界，还是回到原地，始终不能离开这个状态；
2. 在某几个状态内来回移动，形成闭环。比如从 $s_4$ 向右移动到 $s_5$；在 $s_5$ 向左移动，又回到 $s_4$。

为什么会造成这种情况呢？

还是要看算法：在每一次分幕采样结束后，立刻就开始进行策略更新。因为是从后向前遍历，所以在某个状态中，即使采取了错误方向的动作，由于后续的动作是正确的，那么开始的错误动作也会被认为正确的动作。

<center>
<img src="./img/greedy.png">

图 11.3.1 
</center>


举例来说，在图 11.3.1 中（我们暂时忽略从 $s_0$ 到 $s_{12}$ 的过程），$s_{12}$ 状态时：
1. 随机选择了动作 1 向左走，回到原地；
2. 随机选择了动作 2 向右走到 $s_{13}$；
3. 随机选择动作 3 向下走回到原地；
4. 随机选择动作 4 向右走到 $s_{14}$；
5. 随机选择动作 5 向右走到 $s_{15}$，分幕结束。

当然，由于随机性较强，真实的采样过程可能比上面的例子更复杂。

由于最终是到达了终点，所以前面的一系列动作都被认为是有效的，包括错误的动作 1 和动作 3。在倒序遍历时：
1. 看动作 5，能够确定状态 $s_{14}$ 时向右走；
2. 看动作 4，能够确定状态 $s_{13}$ 时向右走；
3. 看动作 3，认为状态 $s_{13}$ 时应该向下走，这就把动作 4 的策略给覆盖了；
4. 看动作 2，能够确定状态 $s_{12}$ 时向右走；
5. 看动作 1，认为状态 $s_{12}$ 时应该向左走，这就把动作 2 的策略给覆盖了。

所以，最终确定的策略是在状态 $s_{12}$ 时向左走！这样就永远不能跳出分幕循环。原因很简单，就是分幕采样的样本太少了，不能做出正确的判断。

#### 探索性出发（Exploring Starts）

除了上面说的错误动作外，在图 11.3.1 中还展示了另外一个问题：右上方的 9 个状态（1,2,3,5,6,7,9,10,11）没有被遍历到，或者是被遍历的次数极少，不能正确体现出其实际的动作价值。在 8.4 节中，我们学习过 Q 函数的定义。如果应用到简单的方格世界问题上，Q 函数表格会如表 11.3.1 所示。

表 11.3.1 简单方格问题的 Q 函数表格

|状态 $\downarrow$ 动作$\to$|左|下|右|上|
|:-:|:-:|:-:|:-:|:-:|
|0|$q_\pi(s_0,a_0)$|$q_\pi(s_0,a_1)$|$q_\pi(s_0,a_2)$|$q_\pi(s_0,a_3)$|
|1|$q_\pi(s_1,a_0)$|$q_\pi(s_1,a_1)$|$q_\pi(s_1,a_2)$|$q_\pi(s_1,a_3)$|
|...|...|...|...|...|
|14|$q_\pi(s_{14},a_0)$|$q_\pi(s_{14},a_1)$|$q_\pi(s_{14},a_2)$|$q_\pi(s_{14},a_3)$|
|15|$q_\pi(s_{15},a_0)$|$q_\pi(s_{15},a_1)$|$q_\pi(s_{15},a_2)$|$q_\pi(s_{15},a_3)$|

状态-动作的组合构成 Q 表格，一共有 16x4=64 个组合。如果想评价在某个状态上哪个动作最好，那么最起码要在该状态上尝试完所有动作后才会有评价的基础。

但是还有一个问题，如果总是从 $s_0$ 出发的，状态 $s_5$ 有可能会被绕过去，永远没有机会被访问到，虽然概率很小，但不可避免。所以我们需要以 $s_5$ 为起点，和 $s_0$ 一样做很多次分幕采样。

我们把这种方法叫做探索性出发，它包括三个含义：
1. 从所有可能的状态出发；
2. 尝试所有可能的动作；
3. 多次采样。

首先改一下方格世界的起始状态定义：

```python
# 起点，可以多个
StartStates = list(range(15))
```

上述代码可以产生一个列表，从 0 到 14，不包括 15。

然后在方格世界的基础代码中，修改 reset() 函数，让它随机从 0-14 中选择起始状态。

```python
    def reset(self):
        self.curr_state = np.random.choice(self.StartStates)
        return self.curr_state
```

关于策略，我们仍然在开始时使用随即策略，可以保证每个动作都被选择。

```python
policy = helper.create_policy(env.nS, env.nA, (0.25, 0.25, 0.25, 0.25))
```

还需要指定一个我们期望的探索次数，即每个状态上的每个动作被选择的次数。

```python
exploration = 100
```

算法改进：

```python
def MC_EveryVisit_Q_Policy_test(env, episodes, gamma, policy, exploration):
    ......
        num_step = len(Episode)
        G = 0
        # 从后向前遍历计算 G 值
        for t in range(num_step-1, -1, -1):
            s, a, r = Episode[t]
            G = gamma * G + r
            Value[s,a] += G     # 值累加
            Count[s,a] += 1     # 数量加 1
            # 判断该状态下被选择动作的最小次数
            if np.min(Count[s]) <= exploration:
                continue        # 如果次数不够，则不做策略改进
            # 做策略改进，贪心算法
            Q[s] = Value[s] / Count[s]  # 得到该状态下所有动作的 q 值
            policy[s] = 0               # 先设置该状态所有策略为 0（后面再把有效的动作设置为非 0）
            argmax = np.argwhere(Q[s]==np.max(Q[s]))    # 取最大值所在的位置（可能不止一个）
            p = 1 / argmax.shape[0]                     # 概率平均分配给几个最大值
            for arg in argmax:                          # 每个最大值位置都设置为相同的概率
                policy[s][arg[0]] = p
```

主要是在做策略改进之前，先判断是不是探索次数足够多。有可能某些状态被访问了很多次，那么可以在这些状态上先做策略改进，而那些次数不够的状态，依然保持随机策略。

首先我们尝试 exploration = [10, 50]，即每个动作的最小访问次数为 10 次和 50 次时，都会造成分幕采样时的死循环，所以最终令 exploration = 100，可以避免这个问题。如果设置成 200 甚至 500，效果更佳，要针对具体情况来分析。



策略中箭头的指向与 Q 函数结果中右侧的字符表格一致，它不是最优策略，但是已经是可以到达终点的策略了，虽然在 $s_3,s_{12}$ 处还有问题。增加 max_iteration 的次数，并不会改善上面的问题，因为使用的是贪心策略，一旦定型，很难更改。此时需要增加 exploration 的数值，才能得到更好的策略。

图 11.3.2 为不同的探索次数与最终的策略结果的对应关系。

<center>
<img src="./img/GridWorld44_Exploration.png">

图 11.3.2
</center>

- 真实值，在对角线上是可以向下和向右两个方向移动，在对角线右上方都是向下移动，在对角线左下方都是向右移动；
- 探索 20 幕，$s_3$ 有问题；
- 探索 40 幕，$s_3$ 有问题；
- 探索 60 幕，没有问题；
- 探索 80 幕，没有问题。由于是蒙特卡洛方法，所以不能要求非常精确地在对角线上是两个方向，只要看对角线上方和下方的箭头方向正确即可。

