
## 10.4 软性策略

### 10.4.1 $\epsilon$-贪心策略

在 11.3 节中，我们使用贪心算法配合探索性出发，来评估动作价值。因为探索性出发可以提供足够的机会让备选动作来表现出价值，然后才能贪心，这与第二章的躲避强盗问题是相同的。但是它的缺点是显而易见的：
1. 不知道要探索多少次才算合理。
2. 需要每个状态都可以作为起点。

在本章中，我们将学习软性策略，来避开探索性出发的缺点。比如在二十一点问题中，Gym 的交互环境不可能让我们指定每个状态作为起点，虽然它具有一定的随机性，但仍然不能满足均等的机会。

我们已经不是第一次接触软性策略了，在第二章的多臂强盗问题中，曾经学习过梯度上升法，里面使用了 Softmax 函数，根据各个动作的价值计算出备选概率，而不是使用非黑即白的硬性策略 argmax() 来选择后续动作。这样做的好处是：一方面在以最大概率选择（利用）了历史表现最好的动作的同时，给其它表现不好的动作一定的机会来进行探索。

具体到策略问题上来，与 4.4.2 节的 $\epsilon$-贪心策略差不多，使用一个概率值来贪心地执行当前已知的最佳动作，而用另外一个概率值来进行探索。其策略表示为：

$$
\pi'(a \mid s)=
\begin{cases}
1-\epsilon+\frac{\epsilon}{|A|}, & a=\argmax_a Q(s,a)
\\
\frac{\epsilon}{|A|}, & a \ne \argmax_a Q(s,a)
\end{cases}
$$

假设 $\epsilon=0.4,|A|=4$，则：
- 当 $a$ 是四个动作的最大值时，$\pi(a|s)=1-0.4+0.4/4=0.7$
- 当 $a$ 不是四个动作的最大值时，$\pi(a|s)=0.4/4=0.1$

很容易看到，$0.7+0.1 \times 3=1.0$ 是一个全概率值，则后续的策略是在那个目前最佳的动作上，以 0.7 的概率继续贪婪执行，以 0.1 的概率选择执行其它三个动作。




【算法 11.3.2】

---

输入：起始状态 $s$，策略 $\pi$，折扣 $\gamma$，幕数 Episodes，贪心系数 $\epsilon$
初始化数组：$G(S,A) \leftarrow 0, N(S,A) \leftarrow 0$，$S$ 为状态空间，$A$ 为动作空间
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,a,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略 $\pi$ 得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,a,r)$，相当于是 $(s_t,a_t,r_{t+1})$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $(s_t,a_t,r_{t+1})$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t,a_t) \leftarrow G(s_t,a_t)+G_t$
　　　　$N(s_t,a_t) \leftarrow N(s_t,a_t)+1$
　　　　$Q(s_t,A) \leftarrow G(s_t,A)/N(s_t,A)$
　　　　$A^* \leftarrow \argmax_a Q(s_t,A)$
　　　　$\pi(a \mid s)=
\begin{cases}
1-\epsilon+\frac{\epsilon}{|A|}, & if \ a=A^*
\\
\frac{\epsilon}{|A|}, & if \ a\ne A^*
\end{cases}
$

输出：$Q(S,A)$

---


```python
# MC e-贪心策略
def MC_Soft_Greedy_test(env, episodes, gamma, policy, epsilon):
    ......
    best_p = 1 - epsilon + epsilon / nA 
    other_p = epsilon / nA

    for episode in tqdm.trange(episodes):   # 多幕循环
        ......
        num_step = len(Episode)
        G = 0
        # 从后向前遍历计算 G 值
        for t in range(num_step-1, -1, -1):
            s, a, r = Episode[t]
            G = gamma * G + r
            Value[s,a] += G     # 值累加
            Count[s,a] += 1     # 数量加 1
            # 做策略改进，贪心算法
            if np.min(Count[s]) == 0:
                continue
            Q[s] = Value[s] / Count[s]  # 得到该状态下所有动作的 q 值
            policy[s] = other_p         # 先设置该状态所有策略为 epsilong/nA
            argmax = np.argmax(Q[s])
            policy[s, argmax] = best_p
    Count[Count==0] = 1 # 把分母为0的填成1，主要是针对终止状态Count为0
    Q = Value / Count   # 求均值
    return Q
```

### 10.4.3 GLIE 方法

