
## 2.4 两种贪心算法

### 2.4.1 先试探后贪心

#### 算法描述

【算法 2.4.1】

---
初始化：$T \leftarrow$ 试探次数，动作集 $A$ 的价值 $Q(A)=0$
$r \leftarrow 0$，循环 2000 次：
　　初始化奖励分布和计数器
　　$t \leftarrow 0$，迭代 1000 轮：
　　　　如果 $t < T$，随机选择动作 $a = random\{A\}$；
　　　　否则 $a = \argmax_a \ Q(A)$
　　　　执行 $a$ 得到奖励 $r$
　　　　更新 $a$ 的动作价值 $Q_n(a)$
　　　　$t \leftarrow t+1$
　　$r \leftarrow r+1$

---

在本例中动作集数量为 10，如果只试探 10 次，一是可能没有选到所有的动作进行试探，二是即使选到了，由于概率分布，该动作的收益在当时的表现不佳，都有可能错过最佳动作的确定。

如果试探次数足够多，就大概率可以找到最佳动作。


#### 算法实现

【代码位置】

```python
class KAB_Greedy(kab_base.KArmBandit):
    def __init__(self, k_arms=10, mu=0, sigma=1, try_steps=10):
        super().__init__(k_arms=k_arms, mu=mu, sigma=sigma)
        self.try_steps = try_steps  # 试探次数

    def select_action(self):
        if (self.step < self.try_steps):
            action = np.random.randint(self.k_arms) # 随机选择动作
        else:
            action = np.argmax(self.Q)  # 贪心选择目前最好的动作
        return action
```

- 试探次数 $n$ 定义为 self.try_steps；
- 时间步 $t$ 定义为 self.step；
- $t \leftarrow t+1$ 的操作在基类 KArmBandit 中实现。

#### 算法结果与分析

<center>
<img src='./img/algo-greedy.png'/>

图 2.4.1 贪心算法
</center>

读者第一次看到图 2.4.1 这张图，在本章中全是类似格式的图，所以有必要详细说明一下。

- 第一部分：0-100 步内的平均收益。

    这一部分衡量算法的**探索效率**。比如红色线条，参数为 Greedy(80)，表示 try_steps=80，这个参数设置可以保证后期取得最好的效果，但是前期 80 步内几乎没有收益，而在有些实际应用中，希望马上能看到算法效果，这个参数就不合适了，20 或 40 可能比较合适。

    这一部分的图例是四种参数的总平均收益，Greedy(40) 最好，为 1.410。

- 第二部分：300-500 步内的平均收益。

    这一部分衡量算法的**收敛效率**。所谓收敛，就是看这一部分的曲线，是继续向上攀升呢，还是一直横向振动。如果是横向振动，说明收敛的速度还不错；如果是向上攀升，说明还没有收敛，有继续提高的空间。

- 第三部分：700-900 步内的平均收益。

    这一部分衡量算法的**利用能力**。如果持续在高位横盘，就是有很好的利用能力；如果持续在低位横盘，说明算法能力有限，或者前期的最佳动作判断不准确。

- 第四部分：0-1000 步内的最优动作选择比例。

    这一部分衡量算法的**长期收益**。如果以 1000 步为界限的话，比较四条线在 1000 步时的高低，


- 第五部分：所有动作被选择的次数。

    这一部分包括该算法的 4 个不同的参数设置下的每个动作的执行次数。由于我们在 KArmBandit() 类中“偷偷地”对十个动作的收益期望值进行了由小到大的排序，所以，四个柱状图都是梯形的左低右高分布，靠右端的柱子越高越好，过渡过程越陡峭越好。



#### 关于探索与利用（EE - Exploration and Exploitation）


比如参数 Greedy(80)，动作 9（从 0 开始数实际上就是动作 10）数字为 782，即在 1000 次内被选择了 782 次；而参数 Greedy(10) 的动作 9 只被选择了 528 次。这样的话，前者的最后总平均收益肯定比后者要高很多。



### 2.4.2 $\epsilon$-贪心算法


【算法 2.4.2】

---
初始化：$\epsilon \leftarrow$ 探索概率 $\in [0,1]$，动作集 $A$ 的价值 $Q(A)=0$
$r \leftarrow 0$，循环 2000 次：
　　初始化奖励分布和计数器
　　$t \leftarrow 0$，迭代 1000 轮：
　　　　得到一个随机数 $m \in [0,1]$
　　　　如果 $m < \epsilon$，随机选择动作 $a = random\{A\}$
　　　　否则 $a = \argmax_a \ Q(A)$
　　　　执行 $a$ 得到奖励 $r$
　　　　更新 $a$ 的动作价值 $Q_n(a)$
　　　　$t \leftarrow t+1$
　　$r \leftarrow r+1$

---

算法实现
【代码位置】

```python
class KAB_E_Greedy(kab_base.KArmBandit):
    def __init__(self, k_arms=10, epsilon=0.1):
        super().__init__(k_arms=k_arms)
        self.epsilon = epsilon  # 探索概率

    def select_action(self):
        if (np.random.random_sample() < self.epsilon):
            action = np.random.randint(self.k_arms) # 随机选择动作
        else:
            action = np.argmax(self.Q)  # 贪心选择目前最好的动作
        return action
```

<center>
<img src='./img/algo-E-greedy.png'/>

图 2.4.2 $\epsilon$-贪心算法
</center>

