## 2.5 梯度上升法


### 2.5.1 Softmax 分布

在 2.4 节中，贪心法使用了 argmax() 操作，非黑即白地选出了一个最大值来执行动作，然后又不得不引入 $\epsilon$ 来提供探索的机会。那么有没有一种操作可以同时兼顾探索与利用呢？我们知道 np.random.choice(n, p=) 可以通过指定概率 p 来从 n 个项目中做出选择，所以把动作价值转换成概率就可以实现这个目标。

假设有 3 个动作 $a_1,a_2,a_3$，它们到目前为止的动作价值 Q 分别为 1,2,3，按照贪心法，动作 $a_3$ 肯是下一个备选。转换成概率的话，有几个方法。

#### 方法一

$$
p_{a_1}=\frac{1}{1+2+3}=\frac{1}{6}, \quad p_{a_2}=\frac{2}{6}, \quad p_{a_3}=\frac{3}{6}
$$

满足 $p_{a_1}+p_{a_2}+p_{a_3}=1$ 的条件。但是当其中有一个负数的时候，就不能这样做了。

#### 方法二

$$
p_{a_1}=\frac{1^2}{1^2+2^2+3^2}=\frac{1}{14}, \quad p_{a_2}=\frac{4}{14}, \quad p_{a_3}=\frac{9}{14}
$$

也满足 $p_{a_1}+p_{a_2}+p_{a_3}=1$ 的条件。但是当其中有一个值为 0 时，就不能这样做了。

#### 方法三

$$
p_{a_1}=\frac{e^1}{e^1+e^2+e^3} \approx 0.09, \quad p_{a_2} \approx 0.24, \quad p_{a_3} \approx 0.67
$$

这种方法可以克服前两种方法的缺陷，并满足 $p_{a_1}+p_{a_2}+p_{a_3}=1$ 的条件，被称作 Softmax，在神经网络中做分类时广泛采用。它的泛化形式是：

$$
\mathbb P[A_t=a] =\frac{e^{a}}{\sum_{x \in A} e^x} \tag{2.5.1}
$$

其中，$a$ 表示一个具体的动作，$A$ 是动作集合，$x$ 是 $A$ 中的每个动作，$a$ 也是 $A$ 中的某个动作。

#### 策略 $\pi$

在将几个备选动作的价值转换为概率后，就可以用 np.random.choice(n, p=) 轻松地从中选择一个动作了，动作值大的概率也大，反之亦然，兼顾了探索与利用。那么在每一次选择动作时的策略 $\pi$，就可以认为是使用 np.random.choice() 函数实现的，每个动作被选择的概率就是式（2.5.1）。

因此，策略可以定义为：

$$
\pi(a) \doteq \mathbb P[A_t=a] =\frac{e^{a}}{\sum_{x \in A} e^x} \tag{2.5.2}
$$

由于策略 $\pi$ 是一个概率形式，所以在该策略下的 Q 函数就是一个奖励的期望：

$$
R_t = \sum_{x \in A} \pi_t(x) R_x \tag{2.5.3}
$$

### 2.5.2 梯度上升（Gradient Ascent）

在赌博机问题中，收益越大越好。在开始阶段，算法需要探索，逐步找到最佳动作后，平均收益会上升，但是不是无限上升，而是在逐步逼近一个最大值。这就满足用梯度上升法解决问题的条件。

<center>
<img src="./img/GradientAscent.png"/>

图 2.5.1 梯度上升
</center>

在图 2.5.1 中，假设一个函数 $y=f(x)$ 的曲线是一个类似抛物线的形状，有最大值点 $p_*$。目前我们处于 $p_0$ 点，在该点处求出导数，即梯度 $\nabla f(x_0)$，乘以一个步进值 $\alpha$，再加上 $x_0$，得到 $x_1$，如式（2.5.1）所示：

$$
x_1 = x_0 + \alpha \cdot \nabla f(x_0) \tag{2.5.1}
$$

如果 $\alpha$ 值足够小的话，经过几轮这样的计算，就会最终达到 $x_*$，从而得到最大值。

具体到赌博机问题上，$x$ 就相当于动作价值函数 $Q(x)$，$\alpha$ 是式（2.2.6）中所述的非平稳状态下的步长值，而 $\nabla f(x)$ 可以表示为在 $t$ 时刻获得的收益的期望值减去到目前为止收益的平均值


$$
\nabla f(x) = \sum \pi(x)R_t - \sum \pi(x) \bar{R}
$$