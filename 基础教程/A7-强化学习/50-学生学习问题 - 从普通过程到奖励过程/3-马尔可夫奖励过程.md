
## 5.3 马尔可夫奖励过程

### 5.3.1 马尔可夫奖励过程（Markov Reward Process）

一个完整的奖励过程如图 3 所示。就是在状态转移图中，给每个状态都定义一个奖励值，当到达这个状态时，强化学习过程就会获得相应的奖励值，使得整个过程向着获得最大收益的方向优化和运行。

<center>
<img src="./img/Reward-2.png">

图 3 马尔科夫奖励过程
</center>

很明显，这是用**面向结果**的方式来定义奖励。如果是**面向过程**的话，图 3 的 $S_1$ 状态应该没有奖励值，因为看上去它是**起始**状态，没有任何**过程**可以定义它的奖励。

在图 3 中：
- 到达$S_1$状态时，会得到$R_2$的奖励；
- 到达$S_2$状态时，会得到$R_3$的奖励；
......
- 到达$S_t$状态时，会得到$R_{t+1}$的奖励；
......
- 到达终点 $T$ 时，会得到$R_{T}$的奖励。

这些状态的下标值，只表示前后发生的顺序，即**时刻**，而与状态的序号无关。比如一个状态集中有 4 个状态 $[s_a,s_b,s_c,s_T]$，马尔可夫链的顺序有可能是 $s_a,s_b,s_a,s_c,s_T$，那么有：$S_1=s_a,\ S_2=s_b,\ S_3=s_a,\ S_4=s_c,\ S_5=s_T$。

*注：在本书中使用大写的 $S$ 表示某个时刻的状态，如 $S_1,S_t$，分别表示时刻 1 和时刻 t 的状态。用小写的 $s$ 表示具体问题的状态，如 $s_S,s_N$，表示安全驾驶问题中的 **出发** 状态和 **正常行驶** 状态。那么 $S_3=s_N$* 的含义就是在时刻 t=3 时处于状态 $s_N$。

*注：在本书中使用这种定义方式：整个序列的序号是 $S_1,R_2,S_2,R_3,\cdots,S_t,R_{t+1},\cdots$ 的过程，即 $R$ 的序号是 $S$ 的序号+1。而在有些资料中，用这种定义方式：$S_1,R_1,S_2,R_2,\cdots,S_t,R_{t},\cdots$，需要读者事先注意。*


用一个文字公式来表示 MRP（Markov Reward Process，马尔可夫奖励过程）：

$$
马尔可夫奖励过程 = 马尔可夫链 + 奖励函数
$$

根据上面学习的知识，再结合上面的状态转移图 1，我们给每个状态定义一个奖励，如图 5 所示。

<center>
<img src="./img/Drive-2.png" width="600">

图 5 安全驾驶问题的马尔可夫奖励过程
</center>

显然，这里使用了图 2 中的第一种方式（**面向结果**）来定义奖励，举例来说，无论状态“发生事故”是通过什么路径到达的（可以通过“拨打手机”到达，也可以通过“路口闯灯”到达），都可以得到 -12 的奖励（实际上是惩罚）。

奖励函数（值）的设计一般是人工设定的，是通过分析目标问题的实际科学意义或者人文意义来决定的。比如，在图 5 中，通过交通规则的学习，给出制定奖励的过程如下：

1. 按交规，超速行驶扣 3 分，开车打电话扣 3 分，闯红灯扣 6 分。
2. 发生事故扣 1 分。有的读者会有疑问：为什么出了事故只扣 1 分？因为在交规中，出了事故后，不会因为事故本身（比如有无人员伤亡，车俩的损失程度等）而扣分，而是分析出事故的原因，对原因扣分。所以，这里只是象征性地扣 1 分。至于后期要做经济赔偿等等，是属于附带的责任，而不是奖励或惩罚。
3. 礼让行人在交规上不加分，但是在强化学习系统中可以加 1 分，以鼓励自动驾驶的智能体强化此状态，保证安全。
4. 正常行驶是一个常见状态，得 0 分；
5. 闹市减速、小区内减速，和礼让行人一样，都给 1 分奖励。
6. 安全抵达给 5 分奖励。之所以给的奖励值很高，是要强化安全驾驶行为/状态，让智能体更倾向于安全驾驶的习惯。
7. 出发和结束都是 0 分。

根据状态转移可以得到一些完整的分幕采样，从而可以计算出每个采样的回报值，列在表 1 中。

表 1 分幕采样和回报计算

||分幕采样序列|回报值计算|
|-|-|-|
|1|S - N - L - G - E|$G_{S}=0+0+1+5+0=6$|
|2|S - N - P - N - L - G - E|$G_{S}=0+0+1+0+1+5+0=7$|
|3|S - N - R - C - E|$G_{S}=0+0-6-1+0=-7$|
|4|S - X - R - N - R - C - E|$G_{S}=0-3-6+0-6-1+0=-16$|

读者可能会产生怀疑：为什么表 1 中都是同样计算 $G_{S}$ 的回报值，但是有不同的结果？这是因为采样不同，路径不同，造成的回报值不同，这种情况是正常的。

与马尔可夫过程的二元组 $<S,P>$ 相比，马尔可夫奖励过程是一个元组的数据序列：$<S,P,R>$，分别表示状态 $S$、转移概率 $P$、奖励 $R$。



### 5.3.2 折扣因子 $\gamma$

如果把**回报**定义为**奖励**的简单相加的话，整个学习框架就会失去一些“灵动”，没有可以调节收益信号大小的“开关”，甚至带来如图 7 所示的灾难。

如何避免这个问题呢？

这里要引入一个**折扣因子**（简称**折扣**，discount factor）的概念，通常用 $\gamma$ 符号来表示，其取值范围是 [0,1]。具体定义是：

$$
\begin{aligned}
G_t &= R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+ \cdots +\gamma^{T-t-1} R_{T} 
\\
&= \sum_{k=0}^{T-t-1} \gamma^k R_{t+1+k}
\\
&=R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+ \cdots +\gamma^{T-t-2} R_{T})
\\
&=R_{t+1}+ \gamma G_{t+1}, \qquad (0 \le \gamma \le 1)
\end{aligned}
\tag{2}
$$

式 2 有一个有趣的现象，可以递归地用下一个时刻的回报来定义当前时刻的回报，这一点在我们后面的算法研究上非常有帮助。

引入折扣因子的原因如下：

- 有些马尔可夫过程是带环的，需要避免这种无穷的奖励。当过程很长时，$\gamma^t$ 的值越来越小，那么后面的奖励乘以 $\gamma^t$ 的值就可以忽略不计了。
- 因为模型不完备的原因，对未来的评估不一定是准确的。因为这种不确定性，所以对未来的预估增加一个折扣。
- 对于某些行业问题，如金融领域，希望尽可能快地得到奖励，而不是在未来得到奖励。可以想象为通货膨胀的例子，现在是10块钱和十年后的10块钱，是会相差好几倍的。
- 人们总是希望得到即时的奖励，比如一个孩子在成长过程中，对于任何行为随时给与奖励或者惩罚，可以给出强烈的学习信号，智能体也是如此。
- 当关注即时奖励时，可以设置系数 $\gamma=0$。
- 当想要未来获得的奖励跟当前获得的奖励是一样的，可以设为 $\gamma=1$。

折扣作为一种超参存在于强化学习系统中，不同的折扣值会带来智能体的不同的学习效果。


表 2 带折扣的分幕采样和回报计算

||分幕采样序列|回报值计算（$\gamma=0.9$）|
|-|-|-|
|1|Start - N - L - G - End|$G_{S}=0+0.9*0+0.9^2*1+0.9^3*10+0.9^4*0=8.1$|
|2|Start - N - P - N - L - G - End|$G_{S}=0+0.9*0+0.9^2*1+0.9^3*0+0.9^4*1+0.9^5*10+0.9^6*0=7.371$|
|3|Start - N - R - C - End|$G_{S}=0+0.9*0-0.9^2*6-0.9^3*12+0.9^4*0=-13.608$|
|4|Start - X - R - N - R - C - End|$G_{S}=0-0.9*3-0.9^2*6+0.9^3*0-0.9^4*6-0.9^5*12+0.9^6*0=-18.58$|


如果折扣为 0，则回报值 $G$ 就等于当前状态的奖励值 $R$，即 $G_t = R_{t+1}$。

在式 2 中，如果 $T$ 很大的话，似乎 $G$ 就会变得很大，从而无法计算。但由于限定 $\gamma \le 1$，所以回报值还是不会大得离谱的。特别低，如果奖励值为常数 +1，则回报是：

$$
G_t = \sum_k^{\infty} \gamma^k = \frac{1}{1-\gamma} \tag{3}
$$

所以，马尔可夫奖励过程的元组的数据序列中增加了折扣因子：$<S,P,R,\gamma>$。

### 参考资料

- David Silver RL course
- Sutton 强化学习
- 奖励函数设计 https://cloud.tencent.com/developer/article/1693899