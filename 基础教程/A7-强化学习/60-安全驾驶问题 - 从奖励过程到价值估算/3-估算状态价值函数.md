## 6.3 估算状态价值函数

### 6.3.1 搭建模型环境

#### 定义状态集

根据表 6.1.1 定义的状态，在代码中用枚举的方式来定义各个状态。

【代码位置：SafetyDrive_DataModel.py】

```Python
# 状态
class States(Enum):
    Start = 0           # S:出发
    Normal = 1          # N:正常行驶
    Pedestrians = 2     # P:礼让行人
    DownSpeed = 3       # D:闹市减速
    ExceedSpeed = 4     # X:超速行驶
    RedLight = 5        # R:路口闯灯
    LowSpeed = 6        # L:小区慢行
    MobilePhone = 7     # M:拨打手机
    Crash = 8           # C:发生事故
    Goal = 9            # G:安全抵达
    End = 10            # E:结束
```
由于本问题中状态比较少，所以可以用枚举方式来定义状态集，name 部分也可以用单个字母（如 S,N,P 等）来表示。

如果状态太多，并且有规律，就可以直接用序号来定义，但是序号应该保证连续，避免程序处理起来过于复杂。

#### 定义奖励函数

奖励函数，在本问题中是直接给到状态上的，所以可以根据状态的顺序，定义一个列表。在此使用一个简单的向量来定义奖励值，按顺序对应到状态上。

```Python
# 奖励向量
# |出发|正常行驶|礼让行人|闹市减速|超速行驶|路口闯灯|小区减速|拨打手机|发生事故|安全抵达|结束|
R = [0,  0,      +1,       +1,     -3,      -6,     +1,     -3,      -1,   +5,    0]
```

当然，如果使用字典形式，代码的可读性更强。比如：

```python
R={
    States.Start: 0,
    States.Noraml: 0,
    States.Pedestrians: 1,
    ......
}
```

#### 定义状态转移矩阵

```Python
# 状态转移概率 from->to
P = np.array(
    [
        [0.0, 0.9, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        [0.0, 0.0, 0.2, 0.1, 0.1, 0.1, 0.3, 0.1, 0.0, 0.1, 0.0],
        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        [0.0, 0.7, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
        [0.0, 0.2, 0.0, 0.0, 0.0, 0.3, 0.0, 0.0, 0.5, 0.0, 0.0],
        [0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.0, 0.0],
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0],
        [0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0],
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]
    ]
)
```

这是完全按照表 6.2.1 来书写的，请读者自己核对一下。首先要确定每行的数值的和为 1.0，其次要确定 "from->to" 坐标位置是否正确。万一搞错的话，会给后面 debug 代码时带来困难。




#### 定义模型

按理说有了上面的状态集、奖励、转移矩阵，就可以开始估算 G 值了。但是定义一个统一的模型，把细节封装成一些标准的交互函数，会让代码可读性好，出错概率低，复用性高，并帮助读者加深对概念的理解。

```Python
class DataModel(object):
    def __init__(self):
        self.P = P                          # 状态转移矩阵
        self.R = R                          # 奖励
        self.S = States                     # 状态集
        self.nS = len(self.S)               # 状态数量
        self.end_states = [self.S.End]      # 终止状态集
    
    # 判断给定状态是否为终止状态
    def is_end(self, s):
        if (s in self.end_states):
            return True
        return False

    # 获得即时奖励，保留此函数可以为将来更复杂的奖励函数做准备
    def get_reward(self, s):
        return self.R[s.value]

    # 根据转移概率前进一步，返回（下一个状态、当前状态的奖励、下个状态是否为终止）
    def step(self, s):
        next_s = np.random.choice(self.S, p=self.P[s.value])
        return next_s, self.get_reward(s), self.is_end(next_s)
```

上面的代码中的注释已经足够丰富了，不再赘述。唯一要提醒的是，我们使用了枚举定义状态，在函数之间传值时都用枚举变量而非具体数值。在函数内部要注意使用 s.value 来做具体索引值。

### 6.3.2 模拟采样并估算状态价值函数

OK! 模型已经建立好了，现在可以开始根据式 1 来估算学生学习模型的状态价值函数了。

#### 算法描述

【算法 6.3】针对某个状态的价值函数估算。

----

输入：起始状态 $s, Episodes, \gamma$
初始化：$G_{sum} \leftarrow 0$  # 累计多幕的G值以便求平均
多幕 $Episodes$ 循环：
　　当前幕的 $G \leftarrow 0$
　　计数器 $t \leftarrow 0$
　　幕内循环直到终止状态：
　　　　从 $S$ 根据状态转移概率得到 $s',r$ 以及是否终止的标志
　　　　$G \leftarrow G + \gamma^t r$
　　　　$t \leftarrow t+1$
　　　　$s \leftarrow s'$
　　$G_{sum} \leftarrow G_{sum}+G$
$v(s) \leftarrow G_{sum} / Episodes$
输出：$v(s)$

---

#### 算法说明

这个算法有一个很神秘的名字，叫做**蒙特卡洛方法**，专门用于采样，然后估算概率或者数学期望。在后面我们还会更深入地学习其理论知识，读者先记住这个名字就可以。

以表 6.1.2 中的第 1 个采样序列为例，说明算法的执行过程。

<center>
<img src="./img/Sampling.png">

图 6.3.1 采样算法说明
</center>

图 6.3.1 中展示了以 5 个状态组成的序列为例的 $G$ 的计算过程：

1. 起始状态为 $s_S$，得到奖励 $R_1$，保存到 $G$ 中，其中的下标 $G_{[1]}$ 表示第一步；
2. 转移到状态 $s_N$，得到奖励 $R_2$，乘以 $\gamma$ 后与第一步的 $G$ 相加，仍然保存到 $G$ 中，原来的 $G$ 值就被替换掉了；
3. 以此类推，一直到最后的 $s_E$ 状态，得到 $R_T$，与第 $[4]$ 步的 $G$ 相加，终止幕内循环，得到状态 $s_S$ 的一个采样序列的回报值 $G_{S}$；
4. 多次重复上述过程，得到不同的采样序列的 $G_S$ 值，累计；
5. 最后的累计值除以幕数，就可以认为是 $G$ 的数学期望，因而得到状态价值函数值 $v(S)$。

这个算法的特点是使用了最少的变量，在算法过程中，一共只用了 $G_{sum},G,R,t,s,s'$ 等几个变量，没有使用任何列表或数组来存储历史数据，而且完全是按照回报的定义以及状态价值函数的定义来实现的，便于读者理解。

#### 算法实现

【代码位置：Algo_Sampling.py】

```Python
# 多次采样获得回报 G 的数学期望，即状态价值函数 v(start_state)
def Sampling(dataModel, start_state, episodes, gamma):
    G_sum = 0  # 定义最终的返回值，G 的平均数
    # 循环多幕
    for episode in tqdm.trange(episodes):
        s = start_state # 把给定的起始状态作为当前状态
        G = 0           # 设置本幕的初始值 G=0
        t = 0           # 步数计数器
        is_end = False
        while not is_end:
            s_next, reward, is_end = dataModel.step(s)
            G += math.pow(gamma, t) * reward
            t += 1
            s = s_next
        # end while
        G_sum += G # 先暂时不估算平均值，而是简单地累加
    # end for
    v = G_sum / episodes   # 最后再一次性估算平均值，避免增加估算开销
    return v
```

上述代码可以通过多次循环（由 Episodes）指定，估算指定状态 start_state 的多个回报值 $G$ 的平均值，作为理论上的数学期望值。

这段代码单独作为一个模块，可以被任何其它应用场景调用，需要关于 dataModel 的接口匹配即可。

那么 Episodes 的具体数值是多少合适呢？后面再解释。

#### 主过程调用

【代码位置：SafetyDrive_1_Sampling.py】

```Python
if __name__=="__main__":
    start = time.time()
    episodes = 10000        # 估算 10000 次的试验的均值作为数学期望值
    gammas = [0, 0.9, 1]    # 折扣因子
    dataModel = data.DataModel()    
    for gamma in gammas:
        V = {}
        for s in dataModel.S:   # 遍历每个状态
            v = Sampling(dataModel, s, episodes, gamma) # 采样估算价值函数
            V[s] = v            # 保存到字典中
        # 打印输出
        print("gamma =", gamma)
        for key, value in V.items():
            print(str.format("{0}:\t{1}", key.name, value))
```

读者可以自己运行 SafetyDrive_1_Sampling.py 以得到输出，为了方便阅读，我们把输出结果整理在表 6.3.1 中。

#### 估算结果

表 6.3.1 各个状态的奖励值和价值函数（分幕采样数为 1000）

|状态|R|$\gamma = 0$|$\gamma = 0.9$|$\gamma = 1$|
|-|-:|-:|-:|-:|
|出发 Start|            0|  0.0 |0.48|1.08|
|正常行驶 Normal|       0|  0.0 |1.26|1.71|
|礼让行人 Pedestrians| +1|  1.0 |2.15|2.89|
|闹市减速 DownSpeed|   +1|  1.0 |2.31|3.02|
|超速行驶 ExceedSpeed| -3| -3.0 |-5.02|-5.17|
|路口闯灯 RedLight|    -6|  -6.0|-6.69|-6.71|
|小区慢行 LowSpeed|    +1|  1.0 |5.50|6.00|
|拨打电话 MobilePhone| -3|  -3.0|-2.64|-2.34|
|发生事故 Crash|       -1|  -1.0|-1.00|-1.00|
|安全抵达 Goal|        +5|  5.0 |5.00|5.00|
|终止 End|          0|  0.0 |0.00|0.00|

表 6.3.1 集中展示了不同折扣值下的各个状态的价值函数值。从中我们可以得到一些基本的概念：

- $\gamma=0$ 时，状态价值函数值等于奖励值 $R$。
- $\gamma=0.9$ 时的值比 $\gamma=1$ 时要低一些，因为折扣的原因。但是靠近终点的几个状态值没有减少，是因为 $t=0$，所以 $\gamma^t=1$，相当于没有折扣。
- 处于安全驾驶状态的价值函数都是正数，相反，处于危险驾驶状态的价值函数都是负数，这符合我们的预期。
- “出发”状态的价值函数为正，但是是其中最小的一个正数值，这是因为处于该状态时，不知道后面会发生什么情况。理想状态下，应该调整奖励函数，使得该状态的价值函数趋近于 0。不过目前这个值在折扣 0.9 下可以接受。
- **终止状态的状态价值函数定义为 0**，因为后续不会再有任何过程和奖励发生。

#### 多进程并发估算

在本问题的状态集中一共有 11 个状态。根据上面的算法，首先要指定起始状态，可以遍历状态集中的每个状态作为起始状态。在估算两个状态的状态函数值时互相不干扰，所以，可以考虑使用多进程来并发估算每个指定的起始状态。

读者可以根据自己的估算机的 CPU 数量修改 processes=4 的值，但是一定要注意，这个值如果大于你的估算机的 CPU 数量，程序运行速度反而会变慢，因为要在进程间不断切换。

在单进程时，代码 SafetyDrive_1_Sampling.py 在笔者的笔记本上运行了 49 秒；而多进程代码 SafetyDrive_2_MultiProcess.py 运行了 26 秒，快了一倍。这样我们可以把 Episodes 的数量增加一些，以提高采样估算的准确性。

表 6.3.2 各个状态的奖励值和价值函数（分幕采样数为 5000）

|状态|R|$\gamma = 0$|$\gamma = 0.9$|$\gamma = 1$|
|-|-:|-:|-:|-:|
|出发 Start|            0|  0.0 |0.56|1.02|
|正常行驶 Normal|       0|  0.0 |1.21|1.69|
|礼让行人 Pedestrians| +1|  1.0 |2.11|2.67|
|闹市减速 DownSpeed|   +1|  1.0 |2.32|3.06|
|超速行驶 ExceedSpeed| -3| -3.0 |-5.04|-5.19|
|路口闯灯 RedLight|    -6|  -6.0|-6.69|-6.71|
|小区慢行 LowSpeed|    +1|  1.0 |5.50|6.00|
|拨打电话 MobilePhone| -3|  -3.0|-2.69|-2.37|
|发生事故 Crash|       -1|  -1.0|-1.00|-1.00|
|安全抵达 Goal|        +5|  5.0 |5.00|5.00|
|终止 End|          0|  0.0 |0.00|0.00|

表 6.3.2 的数据与表 6.3.1 的比较，相差不多，说明 1000 次分幕采样的数据已经比较准确了。

### 问题与讨论

1. 为什么在不同的折扣因子情况下，“安全抵达”的状态的值永远是 10.0，而“终止” 状态的值永远是 0.0 ？
2. 假设 $Episodes=10000,\gamma=0.9$，多次估算状态值，哪个状态的方差最大？为什么？
3. 为什么 *安全抵达* 的状态值和奖励值相等，而其它的具有正奖励值的状态值都比奖励值大？
4. 使用 RMSE 为计算差的方法，衡量分幕数为500,700,900,1000 时，两组相邻数据的误差，以确定理想的分幕数。
