## 6.2 状态价值函数（State-Value Function）

### 6.2.1 状态价值函数的定义

上个小节中，我们建立了模型，给其中的各个状态以奖励值，并通过采样生成马尔科夫奖励过程，计算了“出发”状态的回报值。读者可能会会产生几个问题：

1. 有了回报值后，怎么使用？
2. 不同的分幕数据序列产生了不同的回报，如何处理？
3. 折扣值 $\gamma$ 设置成什么数值才算合理？
4. 这个奖励值可以真正表示这个状态的好坏吗？
5. 具有相同奖励值的状态，哪个更好？


先回忆一下回报的定义：

$$
G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+ \cdots +\gamma^{T-t-1} R_{T} 
\tag{由式5.3.1}
$$


读者可以看到，这里面只定义了时间步 $t$ 和奖励 $R$，没有与状态挂钩。考虑到以上几个疑问，定义状态价值函数如下：

$$
\begin{aligned}
v(s) &= \mathbb E [G_t | S_t = s]
\\
&=\mathbb E [ R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+ \gamma^3 R_{t+4}+ \cdots]
\end{aligned}
\tag{6.2.1}
$$

关于状态价值函数，我们用小写的 $v(s)$ 来表示状态 $s$ 的价值函数 $v$，用大写的 $V$ 表示所有状态的价值函数值数组，所以有时候会把状态价值函数叫做 V 函数。

式（6.2.1）的含义是，状态价值函数 $v$ 是以状态 $s$ 为起点的回报 $G$ 的**数学期望**。时刻 $t$ 在这里只起到一个按顺序串联状态 $S$ 从而得到奖励 $R$ 的作用。


### 6.2.2 数学期望

简单地回忆一下**数学期望**的概念。

一个正常的六面的骰子，投出去后可以得到 $\{1,2,3,4,5,6\}$ 六种结果，而且概率相等，那么这个骰子的期望值是 $\mathbb E\big[正常的骰子\big] = (1+2+3+4+5+6)/6=3.5$。哈哈，读者可能会发现 3.5 这个数子，骰子无法投出来，所以它只是一种定义。

但是，一个不正常的骰子，比如 $[4,5,6]$ 出现的概率 $p$ 都是 $\frac{1}{5}$，而 $[1,2,3]$ 出现的概率 $p$ 都是 $\frac{2}{15}$，那么它的数学期望是：

$$
\begin{aligned}
\mathbb E\big[不正常的骰子\big]&=\sum_{i=1}^6 p_i v_i
\\
&= \frac{2}{15} \times 1+\frac{2}{15} \times 2+\frac{2}{15} \times 3+\frac{1}{5} \times 4+\frac{1}{5} \times 5+\frac{1}{5} \times 6
\\
&=3.8
\end{aligned}
$$

观察式（6.2.1），在定义状态价值函数时，数学期望对于 $G_t$ 没有定义权重或概率，所以每一幕的 $G_t$ 值都是同等价值的，因此，状态价值函数就是若干幕的 $G_t$ 的算术平均值。

以表 6.1.2 中的数据为例：$v(Start)=[6+7+(-7)+(-13)]/4=-1.75$

以表 6.1.3 中的数据为例：$v(Start)=[4.455+4.42-5.59-10.12)]/4=-6.835$

但是，只有 4 幕采样并不能准确计算出真正的期望值，正如投骰子，投 6 次的平均值也基本不可能是 3.5，因此，我们需要更多的采样。一般情况下，采样的数量级应该是成千上万的，才会得到一个比较稳定的数学期望值。

### 6.2.3 获得更多的采样 - 大数定律

如何获得更多的采样来模拟成千上万的司机的不同驾驶行为呢？

大数定律（Law of Large Numbers），或者称为大数定理、大数法则等等，是概率论与数理统计学的基本定理之一，是关于随机变量序列的算术平均值向常数收敛的一系列极限定理的统称。其表达方式主要有：切比雪夫（Cheby—shev）大数法则、贝努利（Bernoulli）大数法则和泊松（Poisson）大数法法则。

- 弱大数定律

条件：

1. 设 $\{X_1,X_2,\cdots\}$ 为独立同分布（相互之间没关系，但是都遵从与同一个分布）的随机序列，并且 $\mathbb E[X_i] = \mu$ 存在（即 $X_i$ 的期望值为 $\mu$），以及 $var(X_i)=\sigma^2$ 存在；

2. 样本序列 $[X_1,X_2,\cdots,X_n]$ 的平均值 $\overline {X}_n=\frac{1}{n}(X_1+X_2+\cdots+X_n)$；

3. 对于任意正数 $\varepsilon$（一般指的是很小的一个正数）；

则有：

$$
\lim_{n \to \infty} \mathbb P\{|\overline {X}_n - \mu|\lt \varepsilon\}= 1
$$

意思是当样本序列的数量 $n$ 越来越大时，其均值收敛到期望值的概率越来越大（趋近于 1）。

- 强大数定律

条件与弱大数定律相同，但表述为：

$$
\mathbb P\{ \lim_{n\to\infty} \overline X_n=\mu\}=1
$$

意思是当样本序列的数量 $n$ 越来越大时，其均值以概率 1 收敛到期望值。

有了大数定律，我们就可以放心地采样，当样本数量足够多时，就会有 $\mathbb E[G_t] \to v(s)$。

我们有了模型，其中包含了状态定义及状态转移概率，而且我们有了奖励函数，并且可以设置折扣值 $\gamma=1.0$ 或小于 1 的数值，这样马尔科夫奖励过程所需要的四要素 $\langle S,P,R,\gamma\rangle$ 就都具备了。接下来的事情就是用代码来模拟司机的行为了，而司机们的行为，在模型的定义下，不外乎就是根据状态转移概率来进行的。

表 6.2.1 中列出了状态转移矩阵，与租车还车问题中的形式相同。

表 6.2.1 状态定义和状态转移概率

<font size=2>

|P: 从$\rightarrow$到|出发|正常<br>行驶|礼让<br>行人|闹市<br>减速|超速<br>行驶|路口<br>闯灯|小区<br>减速|拨打<br>手机|发生<br>事故|安全<br>抵达|结束|
|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|S：出发|-|0.9|-|-|0.1|-|-|-|-|-|-|
|N：正常行驶|-|-|0.2|0.1|0.1|0.1|0.3|0.1|-|0.1|-|
|P：礼让行人|-|1.0|-|-|-|-|-|-|-|-|-|
|D：闹市减速|-|0.7|0.3|-|-|-|-|-|-|-|-|
|X：超速行驶|-|0.2|-|-|-|0.3|-|-|0.5|-|-|
|R：路口闯灯|-|0.1|-|-|-|-|-|-|0.9|-|-|
|L：小区慢行|-|-|-|-|-|-|-|-|-|1.0|-|
|M：拨打手机|-|0.6|-|-|-|-|-|-|0.4|-|-|
|C：发生事故|-|-|-|-|-|-|-|-|-|-|1.0|
|G：安全抵达|-|-|-|-|-|-|-|-|-|-|1.0|
|E：结束|-|-|-|-|-|-|-|-|-|-|1.0|
</font>
