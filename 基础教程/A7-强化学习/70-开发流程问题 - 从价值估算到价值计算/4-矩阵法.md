


### 矩阵法


观察式 10 方程组，可以把它变形为：

$$
\begin{bmatrix}
v_0
\\
v_1
\\
v_2
\\
v_3
\\
v_4
\\
v_5
\\
v_6
\end{bmatrix}
= \
\begin{bmatrix}
-3
\\
0
\\
+1
\\
+3
\\
+2
\\
-1
\\
0
\end{bmatrix}
+\gamma 
\begin{bmatrix}
0.7v_0+0.3v_1
\\
0.6v_0+0.4v_2
\\
0.9v_3+0.1v_6
\\
0.2v_4+0.8v_5
\\
0.2v_1+0.5v_2+0.3v_3
\\
v_6
\\
0
\end{bmatrix}
\tag{13}
$$


关于式 13：

- 等式左侧的部分，就是状态值的向量，可以写成 $V_s$。
- 等式右侧的第一项，就是状态上的奖励值组成的向量，可以写成 $R_s$。
- 等式右侧的第二个矩阵，又可以写成两个矩阵的乘积：

$$
\begin{bmatrix}
0.7 & 0.3 & 0 & 0 & 0 & 0 & 0
\\
0.6 & 0 & 0.4 & 0 & 0 & 0 & 0
\\
0 & 0 & 0 & 0.9 & 0 & 0 & 0.1
\\
0 & 0 & 0 & 0 & 0.2 & 0.8 & 0
\\
0 & 0.2 & 0.5 & 0.3 & 0 & 0 & 0
\\
0 & 0 & 0 & 0 & 0 & 0 & 1.0
\\
0 & 0 & 0 & 0 & 0 & 0 & 1.0
\end{bmatrix}
\begin{bmatrix}
v_0
\\
v_1
\\
v_2
\\
v_3
\\
v_4
\\
v_5
\\
v_6
\end{bmatrix}
\tag{14}
$$

其中，第一个矩阵就是该问题的状态转移矩阵 $P_{ss'}$，第二个矩阵是状态值向量 $V_s$，于是，式 9 可以变成：
$$
V_s = R_s+ \gamma P_{ss'}V_s \tag{15}
$$

从式 9 直接看过来，式 15 等式右侧的 $V_s$ 应该是 $V_{s'}$ 才对，即 $V_s = R_s+\gamma P_{ss'}V_{s'}$。但是经过上述的实例化推导，希望读者可以理解到：


- 所谓的 $s'$ 是在时间维度上的定义，表示下一步的状态 $s'$ 的状态值；
- 而在空间上，由于状态值一旦确定就不会变化，并没有 $V_{s'}$ 的概念。

比如：
- 式 10.4，$v_3=3+0.2v_4+0.8v_5$ 中，$V_s=v_3,V_{s'}=\{v_4,v_5\}$，$v_4$ 是 $v_3$ 的后续状态，即 $s=v_3,s'=v_4$。
- 式 10.6，$v_4=2+0.2v_1+0.5v_2+0.3v_3$ 中，$V_s=v_4,V_{s'}=\{v_1,v_2,v_3\}$，$v_3$ 是 $v_4$ 的后续状态，即 $s=v_4,s'=v_3$。

两者在不同的马尔可夫过程中互为后续状态，但是 $v_3,v_4$ 这两个值不论在式 15 的等式左侧还是右侧，都是前后一致的。

对于式 13 的一个泛化的形式是式 16：

$$
\begin{bmatrix}
V_1
\\
V_2
\\
\vdots
\\
V_n
\end{bmatrix}
=\
\begin{bmatrix}
R_1
\\
R_2
\\
\vdots
\\
R_n
\end{bmatrix}
+\gamma
\begin{bmatrix}
P_{11} & P_{12} & \cdots & P_{1n}
\\
P_{21} & P_{22} & \cdots & P_{2n}
\\
\vdots & \vdots & \ddots & \vdots
\\
P_{n1} & P_{n2} & \cdots & P_{nn}
\end{bmatrix}
\begin{bmatrix}
V_1
\\
V_2
\\
\vdots
\\
V_n
\end{bmatrix}
\tag{16}
$$


式 15 可以变形，并最终解出 $V_s$：

$$
\begin{aligned}
V_s &= R_s+ \gamma P_{ss'}V_s
\\
V_s - \gamma P_{ss'}V_s &= R_s
\\
(I-\gamma P_{ss'})V_s&=R_s, &(I \ {\footnotesize 是对角矩阵})
\\
V_s&=(I-\gamma P_{ss'})^{-1}R_s &(矩阵没有除法，但可以求逆)
\end{aligned}
\tag{17}
$$

式 16 中，等式右侧的值都是已知的，所以可以解出 $V_s$ 的数学解析解。

【代码位置：1_CodeLifeCycle_DataModel_P.py】

根据式 17 中所需元素，需要定义状态转移矩阵和奖励函数向量。

定义状态转移矩阵：

```python
# 状态转移概率
P = np.array(
    [   # B   C    T    R    F    M    E    
        [0.7, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0],    # Bug 
        [0.6, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0],    # Coding
        [0.0, 0.0, 0.0, 0.9, 0.0, 0.0, 0.1],    # Test (CI)
        [0.0, 0.0, 0.0, 0.0, 0.2, 0.8, 0.0],    # Review
        [0.0, 0.2, 0.5, 0.3, 0.0, 0.0, 0.0],    # reFactor
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],    # Merge
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     # End
    ]
)

```

定义奖励函数值向量：
```python
# 奖励向量 缺陷 编码 测试 审查 重构 合并 结束
Rewards = [-3, 0,   +1, +3,  +2, -1,  0]
```

式 17 的具体实现：
    
```python 
def SolveMatrix(dataModel, gamma):
    # 在对角矩阵上增加一个微小的值来解决奇异矩阵不可求逆的问题
    I = np.eye(dataModel.N) * (1+1e-7)
    # I = np.eye(dataModel.N) # 非奇异矩阵时使用此行代码以提高计算精度
    factor = I - gamma * dataModel.P
    inv_factor = np.linalg.inv(factor)  # 求矩阵的逆
    vs = np.dot(inv_factor, dataModel.R)
    return vs
```
在定义状态转移矩阵时，右下角的值，即从 $S_{End} \to S_{End}$ 的转移概率，即可以写成 0.0，也可以写成 1.0，从强化学习的概念角度出发，都没有错。但是却与代码处理逻辑有关。

- 写成 0.0 时，np.random.choice(p=) 函数由于概率之和不为 1，所以函数调用出错。
- 写成 1.0 时，由于矩阵的行列式为 0，是个奇异矩阵，不可求逆。此时可以在对角矩阵上增加一个微小的值来解决奇异矩阵不可求逆的问题，但是最终结果会有 1e-7 的误差，可以接受。

```
[-21.63390663 -11.63390663   3.36609337   2.62899263   2.14496314
  -1.           0.        ]
Bug:    -21.634
Coding: -11.634
Test:   3.366
Review: 2.629
Refactor:       2.145
Merge:  -1.0
End:    0.0
```

可能有读者好奇：用矩阵法得到的结果，与式 12 相比，哪一个更准确？

答案是：式 12 更准确。原因是用代码求矩阵的逆时，由于具体实现的问题有一些误差，否则的话两者应该完全相等。

