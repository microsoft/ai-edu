
## 9.7 价值迭代

- 在 9.2 节中，计算了在随机策略下穿越虫洞问题的 $V_\pi, Q_\pi$；
- 在 9.5 节中，手工计算了简化版穿越虫洞问题的 $V_*,Q_*$；
- 在 9.6 节中，有了在最优策略下计算 $V_*,Q_*$ 的贝尔曼最优方程；
- 所以，在本节中，可以尝试解决寻找穿越虫洞问题的最优策略了。

但是，手工计算太复杂了，遇到求最大值的问题，还需要做很多假设，所以，本节中我们学习一下是否可以通过动态规划的方式来解决问题。

### 9.7.1 迭代算法

#### 算法描述

把 8.6.2 节中的算法拿过来，稍微改动一下。

【算法 9.7】计算最优价值函数。

---

定义误差 $\varepsilon$
初始化 $V_*(s),Q_*(s,a) \leftarrow 0$
循环：
　　保存备份以检查收敛性 $V_{old}(s) \leftarrow V_*(s)$
　　对每一个非终止状态的 $s \in S$：
　　　　获得动作空间 $A(s)$
　　　　对于每个动作 $a \in A(s)$:
　　　　　　获得转移概率 $p$, 下游状态 $s'$，奖励 $r$
　　　　　　计算 $q_*(s,a)=\sum\limits_{s'} p \big [r+\gamma v_*(s') \big ]$
　　　　　　存放到数组 $Q_*(s,a) \leftarrow q_*(s,a)$
　　　　$v_*(s) \leftarrow \max\limits_a[Q_*(s,a)]$，即指定状态下所有 $a$ 的最大 $Q$ 值
　　　　存放到数组 $V_*(s) \leftarrow v_*(s)$
　　检查收敛性, 如果 $\max(|V_{old} - V_*|) < \varepsilon$ 则退出循环
返回 $V_*(s),Q_*(s,a)$

---

#### 计算单个 $q_*$

计算 $q_*$ 的公式与计算 $q_\pi$ 的一样，所以代码也一样。

```python
# 式 (9.6.1) 计算 q，假设已知 v*(s')
def q_star(p_s_r, gamma, V):
    q = 0
    # 遍历每个转移概率,以计算 q
    for p, s_next, r in p_s_r:
        # math: \sum_{s'} p_{ss'}^a [ r_{ss'}^a + \gamma *  v_{\pi}(s')]
        q += p * (r + gamma * V[s_next])
    return q
```

输入参数：

- p_s_r：是转移概率 P、下游状态 S、奖励 R 的缩写，是一种数据结构定义，在前面有说明。
- gamma：折扣值。
- V：最优状态价值函数数组 $V_*$。

返回值：指定状态 s 和动作 a 的动作价值函数 $q_*(s,a)$。

#### 计算单个 $v_*$

计算 $v_*$ 的公式与计算 $v_\pi$ 的不一样，最大的区别有两点：

1. 输入参数中不需要 policy 策略参数，因为我们可以从这个算法中直接得到最优策略。
2. 不需要累积 q 值来计算 v 值，而是直接返回 q 值中的最大者。

代码如下：

```python
# 式 (9.6.3) 计算 v*
def v_star(s, actions, gamma, V, Q):
    list_q = []                     # 准备列表记录所有下游的 q*
    for a, p_s_r in actions:        # 遍历每个动作以计算q值，进而计算v值
        q = q_star(p_s_r, gamma, V) # 计算 q*
        list_q.append(q)            # 加入列表
        Q[s,a] = q                  # 记录下每一个q(s,a)值,不需要再单独计算一次
    return max(list_q)              # 返回几个q*中的最大值,即 v=max(q)
```

输入参数：

- s：指定状态。
- actions：指定状态下的动作空间。
- gamma：折扣。
- V：最优状态价值函数数组 $V_*$。
- Q：最优动作价值函数数组 $Q_*$。

返回值：几个最优动作价值函数值中的最大值 $v_*(s)=\max[q_*(s,a)]$。

#### 用迭代法计算 $V_*,Q_*$

实现算法 9.7.1。

【代码位置】Algo_OptimalValueFunction.py

```python
def calculate_VQ_star(env, gamma, max_iteration):
    V = np.zeros(env.nS)            # 0初始化V*(s)
    Q = np.zeros((env.nS, env.nA))  # 0初始化Q*(s,a)
    count = 0                       # 记录迭代次数
    # 迭代
    while (count < max_iteration):
        V_old = V.copy()                            # 保存备份便于检查收敛性
        for s in range(env.nS):                     # 遍历所有状态s
            if env.is_end(s):                       # 终止状态v=0
                continue            
            actions = env.get_actions(s)            # 获得动作空间
            V[s] = v_star(s, actions, gamma, V, Q)  # 计算V*,Q*
        if abs(V-V_old).max() < 1e-4:               # 检查收敛性
            break
        count += 1
    # end while
    print("迭代次数 = ",count)
    return V, Q
```

### 9.7.2 

#### 数据与模型

数据定义与模型逻辑仍然使用和计算 $V_\pi,Q_\pi$ 时同样的代码，在 9.2.2 节也说过了，这种设计可以高度复用，这时就能看出它的好处了。

【代码位置】Wormhole_0_Data.py, GridWorld_Model.py

#### 主过程

【代码位置】Wormhole_2_VQ_star.py

```python
import numpy as np
import Wormhole_0_Data as data              # 数据定义
import GridWorld_Model as model             # 模型逻辑
import Algo_OptimalValueFunction as algo    # 算法实现
import DrawQpi as drawQ                     # 结果输出

if __name__=="__main__":
    env = model.GridWorld(
        # 关于状态的参数
        data.GridWidth, data.GridHeight, data.StartStates, data.EndStates,  
        # 关于动作的参数
        data.Actions, data.Policy, data.SlipProbs,                     
        # 关于奖励的参数
        data.StepReward, data.SpecialReward,                     
        # 关于移动的限制 
        data.SpecialMove, data.Blocks)                        
    model.print_P(env.P_S_R)
    gamma = 0.9 # 折扣，在本例中用1.0可以收敛，但是用0.9比较保险
    iteration = 1000    # 算法最大迭代次数
    V_star, Q_star = algo.calculate_VQ_star(env, gamma, iteration)  # 原地更新的迭代算法
    print("V*")
    V = np.reshape(np.round(V_star,2), (data.GridWidth, data.GridHeight))
    print(V)
    print("Q*")
    print(np.round(Q_star,2))
    # 字符图形化显示
    drawQ.draw(Q_star, (data.GridWidth, data.GridHeight))
```

#### 运行结果

$V_*$ 的结果如图 9.7.1 所示。

<center>
<img src="./img/wormhole-25-Vstar.png">

图 9.7.1 穿越虫洞问题的最优状态价值函数值
</center>

以 $s_{17}$ 为例，它本身的 $v_*$ 值为 21.2，“人往高处走”，下一步，智能体会从 $s_{17}$ 走到相邻方格的 $v_*$ 比较高的方向，也就是 $s_{16},s_{22}$。

笔者特意给具有相同值的方格画了连线，整体上看，这些连线形成了一种类似梯度的形状，表示最优状态价值函数的一种变化趋势。


$Q_*$ 的结果如图 9.7.1 所示。


<center>
<img src="./img/wormhole-25-Qstar.png">

图 9.7.2 穿越虫洞问题的最优动作价值函数值
（左图：$Q_*$函数值；右图：最佳动作方向）
</center>

- 仍以 $s_{17}$ 为例，其“左上右下”四个方向的最优动作价值为 21.2, 17.2, 17.2, 21.2，所以它会引导智能体向下或者向左移动。
- 靠近边界的方格都会有向内部移动的趋势。
- $s_1,s_{21}$ 向任意方向移动都会导致穿越，所以无所谓。
- $s_{10},s_{11},s_{12},s_{13},s_{14}$ 这一行处于中间地带，最佳动作都是向 $s_{21}$ 移动，而不是向奖励相对较小的 $s_1$ 移动。

### 9.7.3 二级回溯

同 $V_\pi,Q_\pi$ 一样（见 8.6.4 节），$V_*,Q_*$ 也有二级回溯，即，可以用 $V_*(s')$ 来表示 $V_*(s)$，也可以用 $Q_*(s',a')$ 来表示 $Q_*(s,a)$。


#### $v_*(s)$ 的迭代表达式

<center>
<img src="./img/v-star-backup.png">

图 9.7.3 最优状态价值函数 $V*$ 的二级回溯图
</center>

如图 9.7.3，$v_*(s)$ 会取左右两侧较大的那个 $q_*(s,a)$ 来作为自己的值，读者不妨假设是左侧的 $q_*$ 较大，然后将式（9.6.1）代入式（9.6.3），从而消去 $q_*(s,a)$，推导出式（9.7.1）：

$$
\begin{aligned}
v_*(s) &= \max_{a \in A(s)}\Big[ q_*(s,a) \Big]
\\
&=\max_{a \in A(s)} \Big[ \sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_*(s')] \Big] &(1)
\\
&=\max_{a \in A(s)} \Big[  R(s,a)+ \gamma \sum_{s'} p_{ss'}^a  v_*(s') \Big] &(2)
\end{aligned}
\tag{9.7.1}
$$

子式（1）和子式（2）两种形式，都可以使用，取决于问题中的奖励函数的设定。

#### $q_*(s,a)$ 的迭代表达式

<center>
<img src="./img/q-star-backup.png">

图 9.7.4 最优动作价值函数 $Q*$ 的二级回溯图
</center>

如图 9.7.4，$q_*(s)$ 会取左右两侧 $v_*(s')$ 的加权平均值，将式（9.6.3）代入式（9.6.1），从而消去 $v_*(s)$，推导出式（9.7.2）：


$$
\begin{aligned}
q_*(s,a) 
&=\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_*(s')]
\\
&=\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma \max q_*(s',a')] &(1)
\\
&=\sum_{s'} p^a_{ss'} r^a_{ss'} + \gamma \sum_{s'} p^a_{ss'} [\max_{a'} q_*(s',a')] &(2)
\\
&=R(s,a) + \gamma \sum_{s'} p_{ss'}^a [\max q_*(s',a')] &(3)
\end{aligned}
\tag{9.7.2}
$$

式（9.7.2）实际上是在每个状态 $s'$ 的下游动作上，都取了一次 $\max$ 操作，作为对应的 $s'$ 的 $v_*(s')$ 值，然后再做加权平均。

子式（1,2,3）三种形式都可以使用，取决于问题中的奖励函数的设定。

### 思考与练习
