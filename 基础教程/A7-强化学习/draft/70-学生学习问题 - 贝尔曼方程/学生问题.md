## 学生学习问题 - 贝尔曼方程

### 1 提出问题

在解决安全驾驶问题的过程中，我们学习了马尔可夫奖励过程、状态价值函数、蒙特卡洛采样法等强化学习的重要概念。蒙特卡洛法虽然是一种科学的方法，但是需要大量的采样才能得到**比较理想的结果**，并不能说是**准确的结果**。

蒙特卡洛法是针对**无模型**强化学习问题的，对于**有模型**的问题，我们有什么更好的方法可以解决吗？

在本节中，我们以在校大学生为例，描述学生学习（上课、复习、考试）的过程，来分析解决上述问题。

假设一门课只需要上三次课就可以结束，然后就可以通过考试而结课拿学分。当然，这其中也不是那么顺利的，学生可能会遇到各种挑战：

- 上课不专心听讲而去打手机游戏；
- 学到一半的时候觉得这门课索然无味，中途退课；
- 觉得离考试还远，不着急复习巩固知识，而是去休息；
......


### 2 建立模型



图 1 是一个有关学生的学习、考试等一些列状态的马尔可夫链，也可以叫做状态转移图，并且给每个状态都赋予了一个即时奖励值。

<center>
<img src="./img/Student-2.png" width="500">

图 1 学习问题的状态转移概率图
</center>

- Class 1,2,3
  上课/学习/复习状态，假设一门课是需要三次课的学习就可以结束。
  - 在上课 1 中，有 0.5 的概率跑到 Game 状态，在课堂上用手机偷偷摸摸打游戏，另外 0.5 的概率到上课 2 状态。
  - 在上课 2 中，有 0.2 的概率直接退课，另外 0.8 的概率到上课 3 状态。
  - 在上课 3 中，有 0.6 的概率去考试，另外 0.4 的概率以为考试还远，不着急准备呢，就去休息了。
- Pass
  考试通过状态。考试结束后，以 100% 概率到结课状态。
- Rest
  休息状态。休息结束后，发现学的内容都忘得差不多了，遂分别以不同的概率回到三次课的上课/学习/复习状态。
  在这里我们不区分上课和复习，如果是第一次到达 C1 状态，就认为是上课，否则就认为是复习。
- Game
  娱乐/打游戏状态。在游戏状态中很大可能不能自拔，以 0.9 概率继续打游戏，只有 0.1 的概率幡然悔悟回到学习状态。
- End
  结课/退课状态。进入此状态后将不再进行转移，或者是说以 100% 的概率转移到自己，叫做结束状态或者吸收状态。

表 1 中列出了状态转移矩阵，与租车问题中的矩阵形式相同。

表 1 状态转移矩阵

|P: 从$\rightarrow$到|Game|Class1|Class2|Class3|Pass|Rest|End|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|**Game**|0.9|0.1||||||
|**Class1**|0.5||0.5|||||
|**Class2**||||0.8|||0.2|
|**Class3**|||||0.6|0.4||
|**Pass**|||||||1.0|
|**Rest**||0.2|0.4|0.4||||
|**End**|||||||1.0|

有的读者可能有个疑问：打游戏上瘾，从 Game 到 Game 有 0.9 的高概率，那么当该学生打游戏 2 小时后良心发现，转到学习状态的概率会不会大于 0.1 呢？

这是一个简单的平稳环境的马尔科夫链，如果考虑更复杂的情况，可以在 Game 状态下增加一个计数器：
- 如果打游戏超过 1 小时了，则有 0.5 的概率回到学习状态；
- 如果没超过 1 小时，则有 0.1 的概率回到学习状态。


在学生学习的模型中有很多状态，如何确定某个状态比另一个状态好呢？或者说如何比较两个状态的好坏呢？因为从直觉上讲，学生在 C1,C2,C3 的状态明显要比 Game 状态好，但是如何能用数值的大小来体现这种好坏关系呢？


在上一节中，已经有了分幕、奖励、回报的概念，这一节中，将会利用这些基础概念来定义每个**状态价值函数**，从而可以比较状态之间的好坏。

需要再次说明的是，在图 1 中，我们使用了**注重结果**的奖励定义方式，直接给每个状态赋值一个奖励，意味只要达到这个状态，就可以立刻获得标注出的奖励值，而不管是从哪条路径达到的。


### 马尔可夫奖励过程中的贝尔曼方程

<center>
<img src="./img/Bellman.png">

图 1 贝尔曼公式推导
</center>

- 左图：在 $s_a$ 状态得到 $R(s)$ 的表达式。
  在使用**注重过程**的奖励函数定义方式时，从$s_a$ 转移到 $s_b,s_c$ 的过程中，分别可以得到 $r_1,r_2$ 的奖励，则 $s_a$ 的奖励函数定义为一种期望：$R(s_a)=\mathbb E[R_{Sa}|S_t=s_a] = p_1 \cdot r_1+p_2 \cdot r_2$。
- 右图：在 $s_a$ 状态得到 $G_{t+1}$ 的表达式。
  当 $S_t=s$ 时，即在 $s_a$状态下，只能确定 $G_{t}=G_a$，不能确定$G_{t+1}$，因为不知道下一步会转移到哪个状态，是 $s_b$ 还是 $s_c$？
  所以，在 $s_a$ 状态时，$G_{t+1}$ 只能用转移概率（即 $p_1,p_2$）与下层状态 $s_b,s_c$ 的 $G$ 值（即 $G_a,G_b$）的乘积来表示，相当于在 $S_t=s_a$ 时，对 $G_{t+1}$ 求一次期望（带权重的平均值）：$G_{t+1}=\mathbb E[G_{b,c}|S_t=s_a]=(p_1 \cdot G_b|S_{t+1}=s_b)+(p_2 \cdot G_c|S_{t+1}=s_c)$。一旦确定到达 $s_b,s_c$ 状态后，$S_t=s_a$ 的条件就可以去掉了，分别用 $S_{t+1}=s_b,S_{t+1}=s_c$ 代替。


做实例化推导之前，针对图 1，先给出一些必要的定义。

状态集定义：

$$
s_a \in s, \ (s_b,s_c) \in s'
$$

其中：$s$ 等同于 $S_t$，$s'$ 等同于 $S_{t+1}$。

由价值函数的定义：
$$
V(s)=\mathbb E [G_t|S_t=s] \tag{1}
$$
可以得到图 1 中状态 $S_a, S_b, S_c$ 的价值函数的实例化表示：
$$
\begin{aligned}
V(s_a)&=\mathbb E [G_a|S_t=s_a]  & (2.1)
\\
V(s_b)&=\mathbb E [G_b|S_{t+1}=s_b]  & (2.2)
\\
V(s_c)&=\mathbb E [G_c|S_{t+1}=s_c]  & (2.3)
\end{aligned}
\tag{2}
$$

在本例中，如果 $V(s)=V(s_a)$，则 $V(s_b),V(s_c) \in V(s')$。

图 1 中状态转移概率的实例化表示：

$$
\begin{aligned}
p_1 &= p(s_b|s_a)=P(s'|s), \ (s=s_a,s'=s_b) &(3.1)
\\
p_2 &= p(s_c|s_a)=P(s'|s), \ (s=s_a,s'=s_c) &(3.2)
\end{aligned}
\tag{3}
$$

图 1 中奖励函数的实例化表示：

$$
\begin{aligned}
r_1 &= r(s_a,s_b)=R(s,s'), \ (s=s_a,s'=s_b) &(4.1)
\\
r_2 &= r(s_a,s_c)=R(s,s'), \ (s=s_a,s'=s_c) &(4.2)
\end{aligned}
\tag{4}
$$

该奖励函数的定义属于**注重过程**的定义方式，即定义在状态转移过程中。而此时状态 $S_a$ 的奖励为：
$$
R_{t+1}=p_1 \cdot r_1+p_2 \cdot r_2 \tag{5}
$$


推导

$$
\begin{aligned}
V(s)&=\mathbb E [G_t|S_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots|S_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\cdots)|S_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma G_{t+1}|S_t=s]
\\
&= \underbrace{ \mathbb E[R_{t+1}|S_t=s]}_A + \gamma \underbrace{\mathbb E[G_{t+1}|S_t=s]}_B
\end{aligned}
\tag{6}
$$

式 6 的 $A$ 部分：

$$
\begin{aligned}
A & = \mathbb E[R_{t+1}|S_t=s]
\\
({\footnotesize 实例化}\to) &= \mathbb E[R_{Sa}|S_t=s_a]
\\
({\footnotesize 图 1 左图} \to )&= p_1 \cdot  r_1+p_2 \cdot r_2 
\\
({\footnotesize 式3,4} \to) &=p(s_b|s_a) \cdot r(s_a,s_b)+p(s_c|s_a) \cdot r(s_a,s_c)
\\
({\footnotesize 抽象化} \to) &=\sum_{s'} P(s'|s) \cdot R(s,s') \to R(s)
\end{aligned}
\tag{7}
$$

式 6 的 $B$ 部分：

首先要注意的一个问题是，B 不等于 $V(s')$，因为按式 6 的第一行，$V(s')=\mathbb E[G_{t+1}|S_{t+1}=s'] \ne \mathbb E[G_{t+1}|S_t=s]$。

$$
\begin{aligned}
B&=\mathbb E\big[G_{t+1}|S_t=s \big ] 
\\
({\footnotesize 实例化} \to)&=\mathbb E \big[\mathbb E[G_{b,c}|S_t=s_a] \big ] 
\\
({\footnotesize 图1右图} \to)&= \mathbb E\big[(p_1 \cdot G_{b}|S_{t+1}=s_b) + (p_2\cdot G_{c}|S_{t+1}=s_c)\big]
\\
({\footnotesize 期望加法变换} \to)&=\mathbb E\big[p_1\cdot G_{b}|S_{t+1}=s_b]+\mathbb E[p_2\cdot G_{c}|S_{t+1}=s_c\big]
\\
({\footnotesize 提出常数} p_1,p_2\to)&=p_1 \cdot \mathbb E[G_{b}|S_{t+1}=s_b]+ p_2 \cdot \mathbb E[G_{c}|S_{t+1}=s_c]
\\
({\footnotesize 式2} \to)&= p(s_b|s_a) \cdot V(s_b) + p(s_c|s_a) \cdot V(s_c)
\\
({\footnotesize 抽象化} \to)&= \sum_{s'} P(s'|s)V(s')
\end{aligned}
\tag{8}
$$

所以式 6 最终为：

$$
\begin{aligned}
V(s) &= \mathbb E[R_{t+1}|S_t=s] + \gamma \mathbb E[G_{t+1}|S_t=s]
\\
&=\sum_{s'} P(s'|s) R(s,s')+ \gamma \sum_{s'} P(s'|s)V(s') & (9.1)
\\
&=\sum_{s'} P(s'|s)[R(s,s')+\gamma V(s')] &(9.2)
\\
&= R(s)+ \gamma \sum_{s'} P(s'|s)V(s')=R_s+ \gamma \sum_{s'} P_{ss'}V(s') &(9.3)
\end{aligned}
\tag{9}
$$

- 在**针对过程定义奖励函数**的问题中，使用式 9.2 比较方便，因为 $R(s,s')$ 是定义在从 $s\to s'$ 的转移过程上。这是 Richard S. Sutton and Andrew G. Barto 书中的写法。
- 在**针对状态定义奖励函数**的问题中，使用式 9.3 比较方便，因为 $R(s)$ 是直接定义在状态 $s$ 上。这是 David Silver 课件中的写法。


如果针对图 1，状态 $s_a$ 的价值函数实例计算公式为：

$$
\begin{aligned}
V(s_a)&=(p_1 \cdot r_1 + p_2 \cdot r_2) + \gamma[p_1 \cdot V(s_b) + p_2 \cdot V(s_c)]
\\
&=R(s)+\gamma[p_1 \cdot V(s_b) + p_2 \cdot V(s_c)]
\end{aligned}
$$

也就是说，一个状态 $s$ 的价值函数 $V(s)$ 由它的下游状态 $s'$ 的价值函数 $V(s')$ 和转移概率 $P(s,s')$ 以及转移过程中的奖励 $R(s,s')$ 构成。

Bellman Equation for MRP

<center>
<img src="./img/student-3.png" width="500">

图 2
</center>

图 2 中，每个状态下方都用括号表示了该状态的序号，比如 C1(1) 表示 $v_1$。以状态 C3 为例，根据式 9.3，可以得到其价值函数为：

$$
\begin{aligned}
v_3&=R(C3)+\gamma[P_{C3,Pass} \cdot V(Pass) + P_{C3,Rest} \cdot V(Rest)]
\\
&=-2+ (0.6 v_4 + 0.4 v_5), &(\gamma=1)
\end{aligned}
$$

同理可以得到其它所有状态的价值函数表达式，列出方程组如下：

$$
\begin{cases}
v_0=-1+0.9v_0+0.1v_1 & (10.1)
\\
v_1=-2+0.5v_0+0.5v_2 & (10.2)
\\
v_2=-2+0.8v_3+0.2v_6 & (10.3)
\\
v_3=-2+0.6v_4+0.4v_5 & (10.4)
\\
v_4=10+v_6 & (10.5)
\\
v_5=1+0.2v_1+0.4v_2+0.4v_3 & (10.6)
\\
v_6=0 & (10.7)
\end{cases}
\tag{10}
$$

这是一个七元一次方程组，肯定有解。先简化式 10 中的各项，得到新的表达式：

$$
\begin{cases}
v_0=v_1-10 & (11.1)
\\
v_1=-2+0.5v_0+0.5v_2 & (11.2)
\\
v_2=0.8v_3-2 & (11.3)
\\
v_3=0.4v_5+4 & (11.4)
\\
v_4=10 & (11.5)
\\
v_5=1+0.2v_1+0.4v_2+0.4v_3 & (11.6)
\\
v_6=0 & (11.7)
\end{cases}
\tag{11}
$$

将 $(11.1)(11.2)(11.3)(11.4)$ 都变成 $v_3$ 的表达式，带入$(11.6)$ 的两侧，可以得到：

$$
2.5v_3-10=1+0.2(0.8v_3-16)+0.4(0.8v_3-2)+0.4v_3
$$

得到：$v_3=4.321$

所以，最终的结果为：

$$
\begin{cases}
v_0=-22.543 \approx -22.5
\\
v_1=-12.543 \approx -12.5
\\
v_2=1.457 \approx 1.5
\\
v_3=4.321 \approx 4.3
\\
v_4=10
\\
v_5=0.803 \approx 0.8
\\
v_6=0
\end{cases}
\tag{12}
$$

读者可以用式 12 的结果验证式 10 中的任意等式。


### 矩阵法

观察式 10 方程组，可以把它变形为：

$$
\begin{bmatrix}
v_0
\\
v_1
\\
v_2
\\
v_3
\\
v_4
\\
v_5
\\
v_6
\end{bmatrix}
= \
\begin{bmatrix}
-1
\\
-2
\\
-2
\\
-2
\\
10
\\
1
\\
0
\end{bmatrix}
+\gamma 
\begin{bmatrix}
0.9v_0+0.1v_1
\\
0.5v_0+0.5v_2
\\
0.8v_3+0.2v_6
\\
0.6v_4+0.4v_5
\\
v_6
\\
0.2v_1+0.4v_2+0.4v_3
\\
0
\end{bmatrix}
\tag{13}
$$


关于式 13：

- 等式左侧的部分，就是状态值的向量，可以写成 $V_s$。
- 等式右侧的第一项，就是状态上的奖励值组成的向量，可以写成 $R_s$。
- 等式右侧的第二个矩阵，又可以写成两个矩阵的乘积：

$$
\begin{bmatrix}
0.9 & 0.1 & 0 & 0 & 0 & 0 & 0
\\
0.5 & 0 & 0.5 & 0 & 0 & 0 & 0
\\
0 & 0 & 0 & 0.8 & 0 & 0 & 0.2
\\
0 & 0 & 0 & 0 & 0.6 & 0.4 & 0
\\
0 & 0 & 0 & 0 & 0 & 0 & 1.0
\\
0 & 0.2 & 0.4 & 0.4 & 0 & 0 & 0
\\
0 & 0 & 0 & 0 & 0 & 0 & 1.0
\end{bmatrix}
\begin{bmatrix}
v_0
\\
v_1
\\
v_2
\\
v_3
\\
v_4
\\
v_5
\\
v_6
\end{bmatrix}
\tag{14}
$$

其中，第一个矩阵就是该问题的状态转移矩阵 $P_{ss'}$，第二个矩阵是状态值向量 $V_s$，于是，式 9 可以变成：
$$
V_s = R_s+ \gamma P_{ss'}V_s \tag{15}
$$

从式 9 直接看过来，式 15 等式右侧的 $V_s$ 应该是 $V_{s'}$ 才对，即 $V_s = R_s+\gamma P_{ss'}V_{s'}$。但是经过上述的实例化推导，读者可以理解所谓的 $V_{s'}$ 是在时间维度上的定义，表示下一步的状态；而在空间上，由于状态值一旦确定就不会变化，并没有 $s'$ 的概念。

比如：
- 式 10.4，$v_3=-2+ (0.6 v_4 + 0.4 v_5)$ 中，$V_s=v_3,V_{s'}=\{v_4,v_5\}$，$v_5$ 是 $v_3$ 的后续状态。
- 式 10.6，$v_5=1+0.2v_1+0.4v_2+0.4v_3$ 中，$V_s=v_5,V_{s'}=\{v_1,v_2,v_3\}$，$v_3$ 是 $v_5$ 的后续状态。

两者在不同的马尔可夫过程中互为后续状态，所以实际上并没有 $V_{s'}$ 的概念，或者说 $V_s$ 和 $V_{s'}$ 对于具体的状态实例有区别，对于状态向量组没有区别。

式 15 可以变形，并最终解出 $V_s$：

$$
\begin{aligned}
V_s &= R_s+ \gamma P_{ss'}V_s
\\
V_s - \gamma P_{ss'}V_s &= R_s
\\
(I-\gamma P_{ss'})V_s&=R_s, &(I \ {\footnotesize 是对角矩阵})
\\
V_s&=(I-\gamma P_{ss'})^{-1}R_s
\end{aligned}
\tag{16}
$$

式 16 中，等式右侧的值都是已知的，所以可以解出 $V_s$ 的数学解析解。

定义状态转移矩阵：

```python
# 状态转移概率
P = np.array(
    [   #Game Cl1  Cl2  Cl3  Pass Rest End
        [0.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0], 
        [0.5, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0],
        [0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.2],
        [0.0, 0.0, 0.0, 0.0, 0.6, 0.4, 0.0],
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
        [0.0, 0.2, 0.4, 0.4, 0.0, 0.0, 0.0],
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0] 
    ]
)
```

定义奖励函数值向量：
```python
# 奖励向量
# [Game, Class1, Class2, Class3, Pass, Rest, End]
Rewards = [-1, -2, -2, -2, 10, 1, 0]
```
代码
    
```python 
def SolveMatrix(dataModel, gamma):
    # 在对角矩阵上增加一个微小的值来解决奇异矩阵不可求逆的问题
    I = np.eye(dataModel.N) * (1+1e-7)
    # I = np.eye(dataModel.N) # 非奇异矩阵时使用此行代码以提高计算精度
    factor = I - gamma * dataModel.P
    inv_factor = np.linalg.inv(factor)
    vs = np.dot(inv_factor, dataModel.R)
    return vs
```
在定义状态转移矩阵时，右下角的值，即从 $S_{End} \to S_{End}$ 的转移概率，即可以写成 0.0，也可以写成 1.0，从强化学习的概念角度出发，都没有错。

与代码处理逻辑有关。

写成 0.0 时，np.random.choice(p=) 函数由于概率之和不为 1，所以函数调用出错。
写成 1.0 时，由于矩阵的行列式为 0，是个奇异矩阵，不可求逆。此时可以在对角矩阵上增加一个微小的值来解决奇异矩阵不可求逆的问题，但是最终结果会有 1e-7 的误差，可以接受。

```
[-22.54320988 -12.54320988   1.45679012   4.32098765  10.   0.80246914   0.        ]
Game:   -22.543
Class1: -12.543
Class2: 1.457
Class3: 4.321
Pass:   10.0
Rest:   0.802
End:    0.0
```

可能有读者好奇：用矩阵法得到的结果，与式 12 相比，哪一个更准确？

答案是：式 12 更准确。原因是用代码求矩阵的逆时，由于具体实现的问题有一些误差，否则的话两者应该完全相等。

### 迭代法（动态规划）
