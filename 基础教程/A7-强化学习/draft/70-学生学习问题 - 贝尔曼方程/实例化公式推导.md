## 学生学习问题 - 贝尔曼方程

### 1 提出问题

在解决安全驾驶问题的过程中，我们学习了马尔可夫奖励过程、状态价值函数、蒙特卡洛采样法等强化学习的重要概念。蒙特卡洛法虽然是一种科学的方法，但是需要大量的采样才能得到**比较理想的结果**，并不能说是**准确的结果**。

蒙特卡洛法是针对**无模型**强化学习问题的，对于**有模型**的问题，我们有什么更好的方法可以解决吗？

在本节中，我们以在校大学生为例，描述学生学习（上课、复习、考试）的过程，来分析解决上述问题。

假设一门课只需要上三次课就可以结束，然后就可以通过考试而结课拿学分。当然，这其中也不是那么顺利的，学生可能会遇到各种挑战：

- 上课不专心听讲而去打手机游戏；
- 学到一半的时候觉得这门课索然无味，中途退课；
- 觉得离考试还远，不着急复习巩固知识，而是去休息；
......


### 2 建立模型



图 1 是一个有关学生的学习、考试等一些列状态的马尔可夫链，也可以叫做状态转移图，并且给每个状态都赋予了一个即时奖励值。

<center>
<img src="./img/Student-2.png" width="500">

图 1 学习问题的状态转移概率图
</center>

- Class 1,2,3
  上课/学习/复习状态，假设一门课是需要三次课的学习就可以结束。
  - 在上课 1 中，有 0.5 的概率跑到 Game 状态，在课堂上用手机偷偷摸摸打游戏，另外 0.5 的概率到上课 2 状态。
  - 在上课 2 中，有 0.2 的概率直接退课，另外 0.8 的概率到上课 3 状态。
  - 在上课 3 中，有 0.6 的概率去考试，另外 0.4 的概率以为考试还远，不着急准备呢，就去休息了。
- Pass
  考试通过状态。考试结束后，以 100% 概率到结课状态。
- Rest
  休息状态。休息结束后，发现学的内容都忘得差不多了，遂分别以不同的概率回到三次课的上课/学习/复习状态。
  在这里我们不区分上课和复习，如果是第一次到达 C1 状态，就认为是上课，否则就认为是复习。
- Game
  娱乐/打游戏状态。在游戏状态中很大可能不能自拔，以 0.9 概率继续打游戏，只有 0.1 的概率幡然悔悟回到学习状态。
- End
  结课/退课状态。进入此状态后将不再进行转移，或者是说以 100% 的概率转移到自己，叫做结束状态或者吸收状态。

表 1 中列出了状态转移矩阵，与租车问题中的矩阵形式相同。

表 1 状态转移矩阵

|P: 从$\rightarrow$到|Game|Class1|Class2|Class3|Pass|Rest|End|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|**Game**|0.9|0.1||||||
|**Class1**|0.5||0.5|||||
|**Class2**||||0.8|||0.2|
|**Class3**|||||0.6|0.4||
|**Pass**|||||||1.0|
|**Rest**||0.2|0.4|0.4||||
|**End**|||||||1.0|

有的读者可能有个疑问：打游戏上瘾，从 Game 到 Game 有 0.9 的高概率，那么当该学生打游戏 2 小时后良心发现，转到学习状态的概率会不会大于 0.1 呢？

这是一个简单的平稳环境的马尔科夫链，如果考虑更复杂的情况，可以在 Game 状态下增加一个计数器：
- 如果打游戏超过 1 小时了，则有 0.5 的概率回到学习状态；
- 如果没超过 1 小时，则有 0.1 的概率回到学习状态。


在学生学习的模型中有很多状态，如何确定某个状态比另一个状态好呢？或者说如何比较两个状态的好坏呢？因为从直觉上讲，学生在 C1,C2,C3 的状态明显要比 Game 状态好，但是如何能用数值的大小来体现这种好坏关系呢？


在上一节中，已经有了分幕、奖励、回报的概念，这一节中，将会利用这些基础概念来定义每个**状态价值函数**，从而可以比较状态之间的好坏。

需要再次说明的是，在图 1 中，我们使用了**注重结果**的奖励定义方式，直接给每个状态赋值一个奖励，意味只要达到这个状态，就可以立刻获得标注出的奖励值，而不管是从哪条路径达到的。


### 马尔可夫奖励过程中的贝尔曼方程

<center>
<img src="./img/Bellman.png">

图 1 贝尔曼公式推导
</center>

- 左图：在 $s_a$ 状态得到 $R(s)$ 的表达式。
  在使用**注重过程**的奖励函数定义方式时，从$s_a$ 转移到 $s_b,s_c$ 的过程中，分别可以得到 $r_1,r_2$ 的奖励，则 $s_a$ 的奖励函数定义为一种期望：$R(s_a)=\mathbb E[R_{Sa}|S_t=s_a] = p_1 \cdot r_1+p_2 \cdot r_2$。
- 右图：在 $s_a$ 状态得到 $G_{t+1}$ 的表达式。
  当 $S_t=s$ 时，即在 $s_a$状态下，只能确定 $G_{t}=G_a$，不能确定$G_{t+1}$，因为不知道下一步会转移到哪个状态，是 $s_b$ 还是 $s_c$？
  所以，在 $s_a$ 状态时，$G_{t+1}$ 只能用转移概率（即 $p_1,p_2$）与下层状态 $s_b,s_c$ 的 $G$ 值（即 $G_a,G_b$）的乘积来表示，相当于在 $S_t=s_a$ 时，对 $G_{t+1}$ 求一次期望（带权重的平均值）：$G_{t+1}=\mathbb E[G_{b,c}|S_t=s_a]=(p_1 \cdot G_b|S_{t+1}=s_b)+(p_2 \cdot G_c|S_{t+1}=s_c)$。一旦确定到达 $s_b,s_c$ 状态后，$S_t=s_a$ 的条件就可以去掉了，分别用 $S_{t+1}=s_b,S_{t+1}=s_c$ 代替。


做实例化推导之前，针对图 1，先给出一些必要的定义。

状态集定义：

$$
s_a \in s, \ (s_b,s_c) \in s'
$$

其中：$s$ 等同于 $S_t$，$s'$ 等同于 $S_{t+1}$。

由价值函数的定义：
$$
V(s)=\mathbb E [G_t|S_t=s] \tag{1}
$$
可以得到图 1 中状态 $S_a, S_b, S_c$ 的价值函数的实例化表示：
$$
\begin{aligned}
V(s_a)&=\mathbb E [G_a|S_t=s_a]  & (2.1)
\\
V(s_b)&=\mathbb E [G_b|S_{t+1}=s_b]  & (2.2)
\\
V(s_c)&=\mathbb E [G_c|S_{t+1}=s_c]  & (2.3)
\end{aligned}
\tag{2}
$$

在本例中，如果 $V(s)=V(s_a)$，则 $V(s_b),V(s_c) \in V(s')$。

图 1 中状态转移概率的实例化表示：

$$
\begin{aligned}
p_1 &= p(s_b|s_a)=P(s'|s), \ (s=s_a,s'=s_b) &(3.1)
\\
p_2 &= p(s_c|s_a)=P(s'|s), \ (s=s_a,s'=s_c) &(3.2)
\end{aligned}
\tag{3}
$$

图 1 中奖励函数的实例化表示：

$$
\begin{aligned}
r_1 &= r(s_a,s_b)=R(s,s'), \ (s=s_a,s'=s_b) &(4.1)
\\
r_2 &= r(s_a,s_c)=R(s,s'), \ (s=s_a,s'=s_c) &(4.2)
\end{aligned}
\tag{4}
$$

该奖励函数的定义属于**注重过程**的定义方式，即定义在状态转移过程中。而此时状态 $S_a$ 的奖励为：
$$
R_{t+1}=p_1 \cdot r_1+p_2 \cdot r_2 \tag{5}
$$


推导

$$
\begin{aligned}
V(s)&=\mathbb E [G_t|S_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots|S_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\cdots)|S_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma G_{t+1}|S_t=s]
\\
&= \underbrace{ \mathbb E[R_{t+1}|S_t=s]}_A + \gamma \underbrace{\mathbb E[G_{t+1}|S_t=s]}_B
\end{aligned}
\tag{6}
$$

式 6 的 $A$ 部分：

$$
\begin{aligned}
A & = \mathbb E[R_{t+1}|S_t=s]
\\
({\footnotesize 实例化}\to) &= \mathbb E[R_{Sa}|S_t=s_a]
\\
({\footnotesize 图 1 左图} \to )&= p_1 \cdot  r_1+p_2 \cdot r_2 
\\
({\footnotesize 式3,4} \to) &=p(s_b|s_a) \cdot r(s_a,s_b)+p(s_c|s_a) \cdot r(s_a,s_c)
\\
({\footnotesize 抽象化} \to) &=\sum_{s'} P(s'|s) \cdot R(s,s') \to R(s)
\end{aligned}
\tag{7}
$$

式 6 的 $B$ 部分：

首先要注意的一个问题是，B 不等于 $V(s')$，因为按式 6 的第一行，$V(s')=\mathbb E[G_{t+1}|S_{t+1}=s'] \ne \mathbb E[G_{t+1}|S_t=s]$。

$$
\begin{aligned}
B&=\mathbb E\big[G_{t+1}|S_t=s \big ] 
\\
({\footnotesize 实例化} \to)&=\mathbb E \big[\mathbb E[G_{b,c}|S_t=s_a] \big ] 
\\
({\footnotesize 图1右图} \to)&= \mathbb E\big[(p_1 \cdot G_{b}|S_{t+1}=s_b) + (p_2\cdot G_{c}|S_{t+1}=s_c)\big]
\\
({\footnotesize 期望加法变换} \to)&=\mathbb E\big[p_1\cdot G_{b}|S_{t+1}=s_b]+\mathbb E[p_2\cdot G_{c}|S_{t+1}=s_c\big]
\\
({\footnotesize 提出常数} p_1,p_2\to)&=p_1 \cdot \mathbb E[G_{b}|S_{t+1}=s_b]+ p_2 \cdot \mathbb E[G_{c}|S_{t+1}=s_c]
\\
({\footnotesize 式2} \to)&= p(s_b|s_a) \cdot V(s_b) + p(s_c|s_a) \cdot V(s_c)
\\
({\footnotesize 抽象化} \to)&= \sum_{s'} P(s'|s)V(s')
\end{aligned}
\tag{8}
$$

所以式 6 最终为：

$$
\begin{aligned}
V(s) &= \mathbb E[R_{t+1}|S_t=s] + \gamma \mathbb E[G_{t+1}|S_t=s]
\\
&=\sum_{s'} P(s'|s) R(s,s')+ \gamma \sum_{s'} P(s'|s)V(s') & (9.1)
\\
&=\sum_{s'} P(s'|s)[R(s,s')+\gamma V(s')] &(9.2)
\\
&= R(s)+ \gamma \sum_{s'} P(s'|s)V(s')=R_s+ \gamma \sum_{s'} P_{ss'}V(s') &(9.3)
\end{aligned}
\tag{9}
$$

- 在**针对过程定义奖励函数**的问题中，使用式 9.2 比较方便，因为 $R(s,s')$ 是定义在从 $s\to s'$ 的转移过程上。这是 Richard S. Sutton and Andrew G. Barto 书中的写法。
- 在**针对状态定义奖励函数**的问题中，使用式 9.3 比较方便，因为 $R(s)$ 是直接定义在状态 $s$ 上。这是 David Silver 课件中的写法。


如果针对图 1，状态 $s_a$ 的价值函数实例计算公式为：

$$
\begin{aligned}
V(s_a)&=(p_1 \cdot r_1 + p_2 \cdot r_2) + \gamma[p_1 \cdot V(s_b) + p_2 \cdot V(s_c)]
\\
&=R(s)+\gamma[p_1 \cdot V(s_b) + p_2 \cdot V(s_c)]
\end{aligned}
$$

也就是说，一个状态 $s$ 的价值函数 $V(s)$ 由它的下游状态 $s'$ 的价值函数 $V(s')$ 和转移概率 $P(s,s')$ 以及转移过程中的奖励 $R(s,s')$ 构成。

Bellman Equation for MRP

<center>
<img src="./img/student-3.png" width="500">

图 2
</center>

图 2 中，每个状态下方都用括号表示了该状态的序号，比如 C1(1) 表示 $v_1$。以状态 C3 为例，根据式 9.3，可以得到其价值函数为：

$$
\begin{aligned}
v_3&=R(C3)+\gamma[P_{C3,Pass} \cdot V(Pass) + P_{C3,Rest} \cdot V(Rest)]
\\
&=-2+ (0.6 v_4 + 0.4 v_5), &(\gamma=1)
\end{aligned}
$$

同理可以得到其它所有状态的价值函数表达式，列出方程组如下：

$$
\begin{cases}
v_0=-1+0.9v_0+0.1v_1 & (10.1)
\\
v_1=-2+0.5v_0+0.5v_2 & (10.2)
\\
v_2=-2+0.8v_3+0.2v_6 & (10.3)
\\
v_3=-2+0.6v_4+0.4v_5 & (10.4)
\\
v_4=10+v_6 & (10.5)
\\
v_5=1+0.2v_1+0.4v_2+0.4v_3 & (10.6)
\\
v_6=0 & (10.7)
\end{cases}
\tag{10}
$$

这是一个七元一次方程组，肯定有解。先简化式 10 中的各项，得到新的表达式：

$$
\begin{cases}
v_0=v_1-10 & (11.1)
\\
v_1=-2+0.5v_0+0.5v_2 & (11.2)
\\
v_2=0.8v_3-2 & (11.3)
\\
v_3=0.4v_5+4 & (11.4)
\\
v_4=10 & (11.5)
\\
v_5=1+0.2v_1+0.4v_2+0.4v_3 & (11.6)
\\
v_6=0 & (11.7)
\end{cases}
\tag{11}
$$

将 $(11.1)(11.2)(11.3)(11.4)$ 都变成 $v_3$ 的表达式，带入$(11.6)$ 的两侧，可以得到：

$$
2.5v_3-10=1+0.2(0.8v_3-16)+0.4(0.8v_3-2)+0.4v_3
$$

得到：$v_3=4.321$

所以，最终的结果为：

$$
\begin{cases}
v_0=-22.543 \approx -22.5
\\
v_1=-12.543 \approx -12.5
\\
v_2=1.457 \approx 1.5
\\
v_3=4.321 \approx 4.3
\\
v_4=10
\\
v_5=0.803 \approx 0.8
\\
v_6=0
\end{cases}
\tag{12}
$$

读者可以用式 12 的结果验证式 10 中的任意等式。

