
贝尔曼方程

<center>
<img src="./img/Bellman.png">

图 1 贝尔曼公式推导
</center>

- 左图：在 $s_a$ 状态得到 $R(s)$ 的表达式。
  在使用**注重过程**的奖励函数定义方式时，从$s_a$ 转移到 $s_b,s_c$ 的过程中，分别可以得到 $r_1,r_2$ 的奖励，则 $s_a$ 的奖励函数定义为一种期望：$R(s_a)=\mathbb E[R_{Sa}|s_t=s_a] = p_1 \cdot r_1+p_2 \cdot r_2$。
- 右图：在 $s_a$ 状态得到 $G_{t+1}$ 的表达式。
  当 $s_t=s$ 时，即在 $s_a$状态下，只能确定 $G_{t}=G_a$，不能确定$G_{t+1}$，因为不知道下一步会转移到哪个状态，是 $s_b$ 还是 $s_c$？
  所以，在 $s_a$ 状态时，$G_{t+1}$ 只能用转移概率（即 $p_1,p_2$）与下层状态 $S_b,S_c$ 的 $G$ 值（即 $G_a,G_b$）的乘积来表示，相当于在 $s_t=s_a$ 时，对 $G_{t+1}$ 求一次期望（带权重的平均值）：$G_{t+1}=\mathbb E[G_{b,c}|s_t=s_a]=(p_1 \cdot G_b|s_{t+1}=s_b)+(p_2 \cdot G_c|s_{t+1}=s_c)$。一旦确定到达 $s_b,s_c$ 状态后，$s_t=s_a$ 的条件就可以去掉了，分别用 $s_{t+1}=s_b,s_{t+1}=s_c$ 代替。


做实例化推导之前，针对图 1，先给出一些必要的定义。

状态集定义：

$$
s_a \in s, \ (s_b,s_c) \in s'
$$

其中：$s$ 等同于 $s_t$，$s'$ 等同于 $s_{t+1}$。

由价值函数的定义：
$$
V(s)=\mathbb E [G_t|s_t=s] \tag{1}
$$
可以得到图 1 中状态 $S_a, S_b, S_c$ 的价值函数的实例化表示：
$$
\begin{aligned}
V(s_a)&=\mathbb E [G_a|s_t=s_a]  & (2.1)
\\
V(s_b)&=\mathbb E [G_b|s_{t+1}=s_b]  & (2.2)
\\
V(s_c)&=\mathbb E [G_c|s_{t+1}=s_c]  & (2.3)
\end{aligned}
\tag{2}
$$

在本例中，如果 $V(s)=V(s_a)$，则 $V(s_b),V(s_c) \in V(s')$。

图 1 中状态转移概率的实例化表示：

$$
\begin{aligned}
p_1 &= p(s_b|s_a)=P(s'|s), \ (s=s_a,s'=s_b) &(3.1)
\\
p_2 &= p(s_c|s_a)=P(s'|s), \ (s=s_a,s'=s_c) &(3.2)
\end{aligned}
\tag{3}
$$

图 1 中奖励函数的实例化表示：

$$
\begin{aligned}
r_1 &= r(s_a,s_b)=R(s,s'), \ (s=s_a,s'=s_b) &(4.1)
\\
r_2 &= r(s_a,s_c)=R(s,s'), \ (s=s_a,s'=s_c) &(4.2)
\end{aligned}
\tag{4}
$$

该奖励函数的定义属于**注重过程**的定义方式，即定义在状态转移过程中。而此时状态 $S_a$ 的奖励为：
$$
R_{t+1}=p_1 \cdot r_1+p_2 \cdot r_2 \tag{5}
$$


推导

$$
\begin{aligned}
V(s)&=\mathbb E [G_t|s_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots|s_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+\cdots)|s_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma G_{t+1}|s_t=s]
\\
&= \underbrace{ \mathbb E[R_{t+1}|s_t=s]}_A + \gamma \underbrace{\mathbb E[G_{t+1}|s_t=s]}_B
\end{aligned}
\tag{6}
$$

式 6 的 $A$ 部分：

$$
\begin{aligned}
A & = \mathbb E[R_{t+1}|s_t=s]
\\
(实例化\to) &= \mathbb E[R_{Sa}|s_t=s_a]
\\
(图 1 左图 \to )&= p_1 \cdot  r_1+p_2 \cdot r_2 
\\
(式3,4 \to) &=p(s_b|s_a) \cdot r(s_a,s_b)+p(s_c|s_a) \cdot r(s_a,s_c)
\\
(抽象化 \to) &=\sum_{s'} P(s'|s) \cdot R(s,s') \to R(s)
\end{aligned}
\tag{7}
$$

式 6 的 $B$ 部分：

$$
\begin{aligned}
B&=\mathbb E\big[G_{t+1}|s_t=s \big ] 
\\
(实例化 \to)&=\mathbb E \big[\mathbb E[G_{b,c}|s_t=s_a] \big ] 
\\
(图1右图 \to)&= \mathbb E\big[(p_1 \cdot G_{b}|s_{t+1}=s_b) + (p_2\cdot G_{c}|s_{t+1}=s_c)\big]
\\
(期望加法变换\to)&=\mathbb E\big[p_1\cdot G_{b}|s_{t+1}=s_b]+\mathbb E[p_2\cdot G_{c}|s_{t+1}=s_c\big]
\\
(提出常数p_1,p_2\to)&=p_1 \cdot \mathbb E[G_{b}|s_{t+1}=s_b]+ p_2 \cdot \mathbb E[G_{c}|s_{t+1}=s_c]
\\
(式2 \to)&= p(s_b|s_a) \cdot V(s_b) + p(s_c|s_a) \cdot V(s_c)
\\
(抽象化\to)&= \sum_{s'} P(s'|s)V(s')
\end{aligned}
\tag{8}
$$

所以式 6 最终为：

$$
\begin{aligned}
V(s) &= \mathbb E[R_{t+1}|s_t=s] + \gamma \mathbb E[G_{t+1}|s_t=s]
\\
&=\sum_{s'} P(s'|s) R(s,s')+ \gamma \sum_{s'} P(s'|s)V(s') & (9.1)
\\
&=\sum_{s'} P(s'|s)[R(s,s')+\gamma V(s')] &(9.2)
\\
&= R(s)+ \gamma \sum_{s'} P(s'|s)V(s') &(9.3)
\end{aligned}
\tag{9}
$$

- 在针对状态定义奖励函数的问题中，使用式 9.3 比较方便；
- 在针对过程定义奖励函数的问题中，使用式 9.2 比较方便。

如果针对图 1，状态 $s_a$ 的价值函数实例计算公式为：

$$
\begin{aligned}
V(s_a)&=(p_1 \cdot r_1 + p_2 \cdot r_2) + \gamma[p_1 \cdot V(s_b) + p_2 \cdot V(s_c)]
\\
&=R(s)+\gamma[p_1 \cdot V(s_b) + p_2 \cdot V(s_c)]
\end{aligned}
$$

也就是说，一个状态 $s$ 的价值函数 $V(s)$ 由它的下游状态 $s'$ 的价值函数 $V(s')$ 和转移概率 $P(s,s')$ 以及转移过程中的奖励 $R(s,s')$ 构成。

Bellman Equation for MRP

<center>
<img src="./img/student-3.png" width="500">

图 2
</center>

图 2 中，每个状态下方都用括号表示了该状态的序号，比如 C1(1) 表示 $v_1$。以状态 C3 为例，根据式 9.3，可以得到其价值函数为：

$$
\begin{aligned}
v_3&=R(C3)+\gamma[P_{C3,Pass} \cdot V(Pass) + P_{C3,Rest} \cdot V(Rest)]
\\
&=-2+ (0.6 v_4 + 0.4 v_5), &(\gamma=1)
\end{aligned}
$$

同理可以得到其它所有状态的价值函数表达式，列出方程组如下：

$$
\begin{cases}
v_0=-1+0.9v_0+0.1v_1 & (10.1)
\\
v_1=-2+0.5v_0+0.5v_2 & (10.2)
\\
v_2=-2+0.8v_3+0.2v_6 & (10.3)
\\
v_3=-2+0.6v_4+0.4v_5 & (10.4)
\\
v_4=10+v_6 & (10.5)
\\
v_5=1+0.2v_1+0.4v_2+0.4v_3 & (10.6)
\\
v_6=0 & (10.7)
\end{cases}
\tag{10}
$$

这是一个七元一次方程组，肯定有解。先简化式 10 中的各项，得到新的表达式：

$$
\begin{cases}
v_0=v_1-10 & (11.1)
\\
v_1=-2+0.5v_0+0.5v_2 & (11.2)
\\
v_2=0.8v_3-2 & (11.3)
\\
v_3=0.4v_5+4 & (11.4)
\\
v_4=10 & (11.5)
\\
v_5=1+0.2v_1+0.4v_2+0.4v_3 & (11.6)
\\
v_6=0 & (11.7)
\end{cases}
\tag{11}
$$

将 $(11.1)(11.2)(11.3)(11.4)$ 都变成 $v_3$ 的表达式，带入$(11.6)$ 的两侧，可以得到：

$$
2.5v_3-10=1+0.2(0.8v_3-16)+0.4(0.8v_3-2)+0.4v_3
$$

得到：$v_3=4.321$

所以，最终的结果为：

$$
\begin{cases}
v_0=-22.543 \approx -22.5
\\
v_1=-12.543 \approx -12.5
\\
v_2=1.457 \approx 1.5
\\
v_3=4.321 \approx 4.3
\\
v_4=10
\\
v_5=0.803 \approx 0.8
\\
v_6=0
\end{cases}
\tag{12}
$$


### 矩阵法


### 迭代法（动态规划）