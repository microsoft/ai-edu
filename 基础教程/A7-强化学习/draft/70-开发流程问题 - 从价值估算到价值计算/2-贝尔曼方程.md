
## 贝尔曼方程 Bellman Equation

这个问题，如果像安全驾驶问题中一样用蒙特卡洛采样方法，仍然可以得到答案。但是在本章，我们会学习一种更科学简便的方法，被称作贝尔曼方程。

### 对状态价值函数的进一步分解

前面已经学习了状态价值函数的定义，其公式为：

$$
v(s) = \mathbb E [G_t | S_t = s]
\tag{1}
$$

由于

$$
\begin{aligned}
G_t &= R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \cdots &(2.1)
\\
G_{t+1} &= R_{t+2}+ \gamma R_{t+3}+ \gamma^2 R_{t+4} + \cdots &(2.2)
\end{aligned}
\tag{2}
$$


结合式 1,2，可以做进一步的分解和推导：

$$
\begin{aligned}
v(s)&=\mathbb E [G_t|S_t=s]
\\
(带入式2.1)&=\mathbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+ \gamma^3 R_{t+4}+\cdots|S_t=s]
\\
&=\mathbb E[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}+ \gamma^2 R_{t+4}+\cdots)|S_t=s]
\\
(带入式2.2)&=\mathbb E[(R_{t+1}+\gamma G_{t+1})|S_t=s]
\\
&= \underbrace{ \mathbb E[R_{t+1}|S_t=s]}_A + \gamma \underbrace{\mathbb E[G_{t+1}|S_t=s]}_B
\end{aligned}
\tag{3}
$$

到了这一步，把 $V(s)$ 分成了 $A,B$ 两部分，**$A$ 可以看作是即时奖励，$B$ 可以看作是带折扣的未来奖励**。

下面分析一下这两部分如何分别得到。


### $A$ 部分：关于 $R_{t+1}$ 的数学期望

<center>
<img src="./img/Bellman-Rs.png">

图 2 关于 $R_{t+1}$ 的数学期望
</center>

#### 状态集定义

大写的 $S_t,S_{t+1}$，表示当前状态 $t$ 和下一个状态 $t+1$ 的**状态变量**，在不同的马尔可夫过程中会有不同的实例，所以只是一种通用表达方式，并不知道具体是哪个状态。

小写的 $s,s'$，表示具体的**状态实例**，具体到图 2 中：

$$
s = [s_a], \quad s' = [s_b,s_c] \tag{4}
$$


#### 转移概率定义

$P_{ss'}$ 是状态转移概率的集合，即转移矩阵。在图 2 中从 $s_a$ 出发有两个转移方向，分别是 $s_b,s_c$，对应的概率是 $p_1,p_2$，并且 $p_1+p_2=1$。

$$
P_{ss'}=[p_1,p_2] \tag{5}
$$

实际上 $p_1$ 可以写作 $p_{s_a s_b}$，含义是从 $s_a$ 到 $s_b$ 的转移概率，因为有式 4，所以 $p_{s_a s_b}$ 又可以写作 $p_{ss'}$，这样一来 $p_1,p_2$ 就可以有统一的符号了，这在后面的数学推导中很重要。

#### 奖励函数定义

在图 2 中，$R_{t+1}$ 指的是到达状态 $s_a$ 后可以得到的奖励 $R(s_a)$，可以看作是 $s_a$ 的函数。

在使用**面向过程**的奖励函数定义方式时，从$s_a$ 转移到 $s_b$ **或** $s_c$，分别可以得到 $r_1,r_2$ 的奖励，但不能同时得到。

$$
R_{ss'}=[r_1,r_2] \tag{6}
$$

和 $p_1,p_2$ 一样，$r_1,r_2$ 也可以统一写成 $r_{ss'}$。

图 2 中从 $s_a$ 出发并不能确定具体转移到哪个状态，所以也无法确定得到的奖励 $R_{t+1}$ 是 $r_1$ 还是 $r_2$。但是幸好有转移概率 $P_{ss'}$ 存在，给了我们一个机会，可以通过数学期望（带权重的平均值）来定义 $R(s_a)$：

$$R(s_a)=\mathbb E[R_{t+1}|S_t=s_a] = \underbrace {p_1 \cdot r_1}_{S_{t+1}=s_b} + \underbrace{p_2 \cdot r_2}_{S_{t+1}=s_c}=\sum_{s'} p_{ss'}r_{ss'} = P_{ss'} R_{ss'} \tag{7}$$

式 7 实际上统一了**面向过程**的奖励方式（$r_1,r_2$）和**面向结果**的奖励方式 $R(s_a)$，把过程的期望看作是结果。因为在有的资料中，会直接给出 $R(s_a)$ 的值，但是又不解释它是怎么来的。这一点很重要，希望读者牢记，避免在以后的学习中遇到概念上的疑问。

式 7 中求和符号下面的 $s'$，表示要遍历 $s'$ 求和，在本例中就是要遍历 [$s_b,s_c$]。

#### 结论

式 3 的 $A$ 部分可以定义为 $R(s)$：

$$
\begin{aligned}
R(s) & = \mathbb E[R_{t+1}|S_t=s]
\\
(根据图2实例化\ s \to s_a) &= \mathbb E[R_{t+1}|S_t=s_a]
\\
(带入式7\to)&= p_1 \cdot  r_1+p_2 \cdot r_2
\\
(抽象\to) &= \sum_{s'}p_{ss'} r_{ss'}=P_{ss'} R_{ss'}
\end{aligned}
\tag{8}
$$

### $B$ 部分：关于 $G_{t+1}$ 的数学期望

<center>
<img src="./img/Bellman-G.png">

图 3 关于 $G_{t+1}$ 的数学期望
</center>

#### 回报的定义

具体到图 3 中，回报 $G$ 在不同时刻的取值为：

$$
G_t=[G_a],\quad G_{t+1}=[G_b,G_c]
$$

和 $A$ 部分中遇到的问题一样：当 $S_t=s$ 时，即在 $s_a$ 状态下，只能确定 $G_{t}=G_a$，不能确定$G_{t+1}$ 是 $G_b,G_c$ 的哪一个，因为不知道下一步会转移到哪个状态，是 $s_b$ 还是 $s_c$？

但是，由于 $P_{ss'}$ 的存在，我们仍然可以用数学期望的方式

$$
G_{t+1}=\mathbb E\big[[G_b,G_c]|S_{t+1}=[s_b,s_c]\big]= \underbrace{p_1 \cdot G_b}_{S_{t+1}=s_b} + \underbrace {p_2 \cdot G_c}_{S_{t+1}=s_c} \tag{9}
$$


#### 价值函数定义

首先要注意的一个问题是，$B$ 部分不等于 $V(s')$，因为按式 1 的定义：

$$
v(s) = \mathbb E [G_t | S_t = s]
$$

则有：

$$
v(s') = \mathbb E [G_{t+1} | S_t = s']\ne \mathbb E[G_{t+1}|S_t=s]
$$

具体到图 3 中，状态 $s_a, s_b, s_c$ 的价值函数的实例化表示：

$$
\begin{aligned}
v(s_a)&=\mathbb E [G_a|S_t=s_a] & \in v(s)
\\
v(s_b)&=\mathbb E [G_b|S_{t+1}=s_b] & \in v(s')
\\
v(s_c)&=\mathbb E [G_c|S_{t+1}=s_c] & \in v(s')
\end{aligned}
\tag{10}
$$

不同时刻状态的价值函数的取值范围是：

$$
V(s)=[v(s_a)], \quad V(s')=[v(s_b),v(s_c)] \tag{11}
$$

#### 结论

$$
\begin{aligned}
B&=\mathbb E\big[G_{t+1}|S_t=s \big ] 
\\
(实例化\to)&=\mathbb E\big[G_{t+1}|S_t=s_a \big ] 
\\
(带入式9\to)&= \mathbb E\big[(p_1 \cdot G_{b}|S_{t+1}=s_b) + (p_2\cdot G_{c}|S_{t+1}=s_c)\big|S_t=s_a]
\\
(期望求和\to)&=\mathbb E\big[p_1\cdot G_{b}|S_{t+1}=s_b,S_t=s_a]+\mathbb E[p_2\cdot G_{c}|S_{t+1}=s_c,S_t=s_a\big]
\\
(马尔可夫性质\to)&=\mathbb E\big[p_1\cdot G_{b}|S_{t+1}=s_b]+\mathbb E[p_2\cdot G_{c}|S_{t+1}=s_c\big]
\\
(提出常数\ p_1,p_2\to)&=p_1 \cdot \mathbb E[G_{b}|S_{t+1}=s_b]+ p_2 \cdot \mathbb E[G_{c}|S_{t+1}=s_c]
\\
(带入式10\to)&= p_1 \cdot v(s_b) + p_2 \cdot v(s_c)
\\
(抽象\to)&=\sum_{s'} p_{ss'}v(s')= P_{ss'}V(s')
\end{aligned}
\tag{12}
$$


### 价值函数



所以式 3 最终为：

$$
\begin{aligned}
v(s) &= \mathbb E [G_t | S_t = s]
\\
&=\mathbb E [R_{t+1}|S_t=s] + \gamma \mathbb E[G_{t+1}|S_t=s]
\\
&=\sum_{s'} p_{ss'} r_{ss'}+ \gamma \sum_{s'} p_{ss'}v(s') =\sum_{s'} p_{ss'} [r_{ss'}+\gamma v(s')]  &(9.1)
\\
&= P_{ss'} R_{ss'} + \gamma P_{ss'} V(s') &(9.2)
\\
&= R(s)+ \gamma P_{ss'}V(s') &(9.3)
\end{aligned}
\tag{9}
$$

**也就是说，一个状态 $s$ 的价值函数 $v(s)$ 由它的下游状态（可能不止一个）$s'$ 的价值函数 $v(s')$ 和转移概率 $P_{ss'}$ 以及转移过程中的奖励 $R_{ss'}$ 构成。**

- 在**面向结果定义奖励函数**的问题中，使用式 9.3 比较方便，因为 $R(s)$ 是直接定义在状态 $s$ 上。这是 David Silver 课件中的写法。

- 在**面向过程定义奖励函数**的问题中，使用式 9.1 或 9.2 比较方便，因为 $R_{ss'}$ 是定义在从 $s\to s'$ 的转移过程上。这是 Richard S. Sutton and Andrew G. Barto 书中的写法，但不完全相同，我们后面再讲。

如果针对图 1，状态 $s_a$ 的价值函数实例计算公式为：

$$
\begin{aligned}
v(s_a)&=(p_1 \cdot r_1 + p_2 \cdot r_2) + \gamma[p_1 \cdot v(s_b) + p_2 \cdot v(s_c)]
\\
&=R(s)+\gamma[p_1 \cdot v(s_b) + p_2 \cdot v(s_c)]
\end{aligned}
$$
