## 8.4 V 函数与 Q 函数

有了模型和奖励，就可以进一步地研究价值问题了。如同马尔可夫奖励过程中的状态价值函数一样，在马尔可夫奖励过程过程中，同样会有价值函数，而且概念进一步地深化和扩展了。


### 8.4.1 奖励过程和决策过程的比较

我们首先用图 8.4.1 来比较一下两种过程，避免概念混淆。

<center>
<img src="./img/mdp-3.png">

图 8.4.1 马尔可夫奖励过程和马尔可夫决策过程的比较
（左侧：马尔可夫奖励过程模型；右侧：马尔可夫决策过程模型。）
</center>

表 8.4.1 比较图 8.4.1 中的左右两部分

||左侧|右侧|
|-|-|-|
|名称|马尔科夫奖励过程 MRP|马尔可夫决策过程 MDP
|模型|两层节点，一层过程|三层节点，两层过程|
|上层节点|源状态|源状态|
|中层节点|无|动作节点|
|下层节点|目标状态|目标状态|
|过程|实线箭头为状态转移 $P_{ss'}$ 以及过程奖励 $R_{ss'}$|实线箭头为策略选择$\pi(a \mid s)$，虚线箭头为状态转移$P^a_{ss'}$以及过程奖励$R^a_{ss'}$|
|解法|贝尔曼方程|贝尔曼期望方程|
|状态价值函数定义| $ v(s)= \mathbb E [G_t \mid S_t=s]$ | $v_\pi(s,a)=\mathbb E[G_t \mid S_t=s]$|
|动作价值函数定义|无|$q_\pi(s,a)=\mathbb E[G_t \mid S_t=s,A_t=a]$|

$v_\pi(s)$ 比 $v(s)$ 多了一个下标 $\pi$，是为了区分二者，没有实际的数学含义。

在有的资料中，给贝尔曼期望方程的 $\mathbb E$ 写作 $\mathbb E_\pi$，也是这个意思，没有实际的数学含义。


表 8.4.2 比较图 8.4.1 中的左右两部分的虚线框内的部分

||左侧|右侧|
|-|-|-|
|顶端节点|源状态 $s_0$，需要计算状态价值函数$v(s_0)$|源动作 $a_1$，需要计算动作价值函数$q_{\pi}(s_0,a_1)$|
|中间过程|状态转移概率$P_{ss'}$，过程奖励向量$R_{ss'}$|状态转移概率$P_{ss'}^{a_1}$，过程奖励向量$R_{ss'}^{a_1}$|
|底端节点|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v(s')$|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v_{\pi}(s')$|

比较图 8.4.1 中左侧和右侧虚线框内的部分，可以说除了符号不同，其它都是相同的，包括位置和含义。

### 8.4.2 动作价值函数 $q_\pi$

动作价值函数，可以简称为 **Q 函数**，用 $q_\pi(s,a)$ 表示在策略 $\pi(a|s)$ 下的动作 $a$ 的价值。

先回忆一下在马尔可夫奖励过程中学习过的状态价值函数，温故而知新。

$$
\begin{aligned}
v(s) &\doteq \mathbb E [G_t \mid S_t = s]
\\
&=\mathbb E [R_{t+1}\mid S_t=s] + \gamma \mathbb E[G_{t+1}\mid S_t=s]
\\
&=\sum_{s'} p_{ss'} r_{ss'}+ \gamma \sum_{s'} p_{ss'}v(s') =\sum_{s'} p_{ss'} [r_{ss'}+\gamma v(s')] 
\\
&= P_{ss'} R_{ss'} + \gamma P_{ss'} V(s')
\\
&= R(s)+ \gamma P_{ss'}V(s') 
\end{aligned}
\tag{8.4.1}
$$

观察贝尔曼方程的推导，其本质是：某个状态的价值函数 $v(s)$ 由三部分组成：
1. 其下游状态的价值 $v(s')$；
2. 转移概率 $p$；
2. 转移过程中的奖励 $r$。

无巧不成书，在下面推导动作价值函数 $q_\pi$ 的公式时，我们也遇到了和式 8.4.1 同样的表达。所以，可以大胆地预测，计算动作价值函数 $q_\pi(s,a)$ 的**贝尔曼期望方程**与计算状态价值函数 $v(s)$ 的**贝尔曼方程**完全一致。

<center>
<img src="./img/mdp-Q.png">

图 8.4.2 马尔科夫决策过程的 Q 函数模型
</center>

在图 8.4.2 中，我们绘制出了 Q 函数模型所需要的所有元素，定义如下：

$$
\begin{aligned}
q_\pi(s,a) & \doteq \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=\mathbb E [R_{t+1}\mid S_t=s, A_t=a] + \gamma \mathbb E[G_{t+1}\mid S_t=s, A_t=a]
\\
&=\sum_{s'} p_{ss'}^a r_{ss'}^a+ \gamma \sum_{s'} p_{ss'}^a v_\pi(s') =\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_\pi(s')] &(1)
\\
&=P^a_{ss'} R^a_{ss'} + \gamma P^a_{ss'} V_\pi(s')=P^a_{ss'}[R^a_{ss'}+\gamma V_\pi(s')] &(2)
\\
&= R^a(s)+ \gamma P_{ss'}^a V_\pi(s')  &(3)
\end{aligned}
\tag{8.4.2}
$$


比较式 8.4.1 和式 8.4.2，对于本章的动作价值函数来说，除了在条件部分多出来一个 $A_t=a$ 以外，其它的部分完全相同，所以它们的表达式也应该相同，但是含义不同，前者是简单的状态价值函数，后者是有策略下的动作价值函数。

那么多出来的这个 $A_t=a$ 会造成什么不同吗？答案是不会。因为这个条件相当于在图 8.4.1 中右侧的部分确定了是选择 $a_1$ 还是 $a_2$，正是因为有了这个条件存在，才会让黄色虚线框部分的模型结构高度相似。

最后计算得到的所有状态-动作的 Q 函数是一个表格形式，如表 8.4.3 所示。

表 8.4.3 表格形式的 Q 函数

|状态 $\to$ 动作|$a_0$|$a_1$|$a_2$|$\cdots$|
|:-:|-|-|-|-|
|$s_0$|||||
|$s_1$|||||
|$s_2$|||||
|$s_3$|||||
|$\vdots$|||||

该表中会填满数字，从而可以比较在某个状态 $s$ 下的各个动作之间的价值，进而评价策略 $\pi$ 的优略。不同的 $s$ 下的动作是不可比的。


### 8.4.3 状态价值函数 $v_\pi$

状态价值函数，可以简称为 **V 函数**，用 $v_\pi(s)$ 表示在策略 $\pi(a|s)$ 下的状态 $s$ 的价值。


函数名称定义为 $v_\pi$ 的原因是为了和马尔可夫过程的状态价值函数 $v$ 区分开来，其定义是：

$$
v_\pi(s) \doteq \mathbb E [G_t \mid S_t=s] \tag{8.4.3}
$$

同前面一样，式 8.4.3 仍然是要求回报 $G_t$ 的数学期望。在图 8.4.1 的右侧，我们考虑以 $v_\pi(s_0)$ 为例推出通用的价值函数公式。

状态 $s$ 并不直接接触到奖励机制，而是通过策略 $\pi$ 与下游的两个动作 $a_1,a_2$ 连接，所以，一旦知道了 $a_1,a_2$ 的动作价值函数 $q_\pi$，那么 $s$ 的状态价值函数就可以表示为 $q_\pi$ 的期望了，即：

<center>
<img src="./img/mdp-V.png">

图 8.4.3 马尔科夫决策过程的 V 函数模型
</center>

在图 8.4.3 中，我们绘制出了 V 函数模型所需要的所有元素，定义如下：

$$
\begin{aligned}
v_\pi(s) &=\pi(a_1|s) q_\pi(s,a_1) + \pi(a_2|s) q_\pi(s,a_2)
\\
&=\sum_{a \in A(s)} \pi(a | s) q_\pi(s,a)
\end{aligned}
\tag{8.4.4}
$$
