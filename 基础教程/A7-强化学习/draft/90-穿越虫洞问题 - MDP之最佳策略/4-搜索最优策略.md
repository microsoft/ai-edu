
## 9.4 搜索最优策略

###  9.4.1 策略选择与策略单元

对于射击气球问题来说，如果只有一支箭，则只有一次策略选择，有两支箭的话，一共要做两次策略选择，但是要牵涉到 6 个策略单元（每个状态一个策略单元）。所以，策略数量与状态数量保持一致，但是与选择策略的次数完全有可能不相等。

- 如射击气球问题，是一个有向无环的图，所以策略选择次数 $\le$ 策略单元数量，因为有些状态可能很少有机会到达。
- 对于有向有环问题，如代码生命周期问题，策略选择次数有可能大于或小于策略单元数量。
- 对于无向有环问题，如穿越虫洞问题，策略选择次数 $\ge$ 策略单元数量，因为一个状态可能被访问多次。


<center>
<img src="./img/Policy.png">

图 9.4.1 策略选择与策略单元
</center>

在图 9.4.1 中，我们提出了策略单元与策略组的概念，比其它文献更详细地说明了策略的全方位概念。

1. 首先，每个状态（除了终止状态以外）肯定有一个策略单元

    一旦到达该状态，就需要做一次选择。但是不是每个状态都必须被访问到。
    我们用 $\pi(s_t)$ 来表示状态 $s_t$ 的策略单元。

2. 每个策略单元中的动作空间有可能不同。

    比如在射击气球问题中，如果老板规定击中一个气球后，下一箭只能选择另外一只气球，则第一箭的状态空间为 2，第二箭的状态空间为 1。

    在比如在迷宫行走问题中，在某个格子内可以向四个方向行走，在另外一些靠墙的格子内，只能向三个方向行走。但是为了简化问题，我们一般会假设仍然可以向四个方向走，但是撞墙后留在原地不动。
    
    对于穿越虫洞问题，其实在虫洞入口处，飞船只有一个选择（就是开始穿越），但是我们还是定义飞船可以向四个方向行驶。

3. 所有的策略单元组成一个策略组，根据策略单元的选择形成唯一的组合。

    比如图 9.4.1 中的四个策略单元，在某个马尔科夫中选择动作序号是 $[0, 2, 1, ..., 0]$，而在另外一次过程中选择动作序号是 $[0, 1, 1, ..., 0]$，有一个单元的差别，算作两个策略组，即两个策略。
    我们用 $\pi(S)$ 表示策略组，其中 $s \in S$。


观察表 9.3.1，在 6 个状态上的策略修改，都会影响对应的局部状态价值函数值，进而影响最终的状态价值函数值。

在 9.3 节中，对不同的策略单元的的独立修改会提高或降低 $v_\pi(s_0)$ 的值，那么一共有多少种组合可以帮助游客提高收益呢？其最大值又是多少呢？本节的任务就是给每个状态都找到最好的策略，从而使整体状态值达到最佳。

但是在具体操作中会遇到这样的困难：假设我们知道了一个状态上的策略是 [0.4,0.6]，但是不可能把一支箭分成 0.4:0.6，所以你必须二选一，用公式表示为：

$$
\pi_*(a_i \mid s)=
\begin{cases}
1, & if \ \pi(a_i|s)=\argmax \limits_{a \in A(s)} \pi(a|s)
\\
0, & 其它动作
\end{cases}
\tag{9.4.1}
$$

当 $\pi=[0.4,0.6], A(s)=[a_0,a_1]$ 时，即 $\pi(a_0|s)=0.4, \pi(a_1|s)=0.6$，式 9.4.1 的结果是：$\pi_*(a_0|s)=0,\pi_*(a_1|s)=1$，因为 0.6 > 0.4 且处于数组中第 1 个位置（从 0 开始计算）。

从另一个角度看，以两个动作为例，假设原始策略为 [0.4, 0.6]，如果新策略 [0.2, 0.8] 会提高整体状态函数值的话，那么另一个新策略 [0.1, 0.9] 甚至 [0.0, 1.0] 也一定会更大地提高状态函数值。读者只需要把上一小节的策略改一改就知道了，比如这一行代码：
```python
[0.2,0.8],  # 修改状态 0 的策略，可以尝试 [0.1, 0.9] or [0.0, 1.0]
```
因此，当$\pi'=[0.0,1.0]$时， $\pi(a_0|s)=0,\pi(a_1|s)=1$ 也就自然成立了。

当两个概率值相等时，我们可以选择任意一个，总选第一个的话，可能会失去其它机会。

在图 9.3.1 中，一共有 6 个有效的状态（$s_0,\cdots,s_5$) ，在每个状态上的策略有两种动作选择（红球或蓝球），这样一共有 $2\times2\times2\times2\times2\times2=2^6$ 种组合。推广到一般情况：如果状态空间为 $S$，动作空间为 $A$，则策略组合是 $|A|^{|S|}$ 种。

对于射击气球这个简单的问题来说，计算一次 $v_\pi,q_\pi$ 只需要迭代两次，即使用遍历的方法，也很容易快速得到结果。所以，我们先用最笨但是最准确的遍历法来得到评估的基准。

### 9.4.2 策略遍历搜索

知道了一共有 $2^6=64$ 种组合，用手写出来还是比较累的。作为一个合格的程序员，我们可以用代码生成 64 个组合策略，实际上就是用二进制表示的 [0, 63]：
```
[[0 0 0 0 0 0]  # 红 红 红 红 红 红
 [0 0 0 0 0 1]  # 红 红 红 红 红 蓝
 [0 0 0 0 1 0]  # 红 红 红 红 蓝 红
 ......
 [1 1 1 1 0 1]  # 蓝 蓝 蓝 蓝 红 蓝
 [1 1 1 1 1 0]  # 蓝 蓝 蓝 蓝 蓝 红
 [1 1 1 1 1 1]] # 蓝 蓝 蓝 蓝 蓝 蓝
```
上面的输出中，0 表示在该状态选择红色球，1 表示在该状态选择蓝色球。

下面要根据式 9.4.1，把上述的二进制形式翻译一下，变成模型可以认识的策略数据结构：用 onehot（热独）编码来表示选择红球或者蓝球，比如：[1, 0] 表示选择红球，[0, 1] 表示选择蓝球。

【代码位置】Shoot_3_OptimalSearch.py

```python
# 创建onehot编码形式的policy，如 [[0,1],[1,0],[1,0]...]
def create_onehot_policy(actions):
    policy = {}
    for s in range(len(actions)):  # onehot
        policy[s] = [1,0] if actions[s]==0 else [0,1]
    return policy
```
以策略组-0为例，输出格式如下：

```
策略组-0：{0: [1, 0], 1: [1, 0], 2: [1, 0], 3: [1, 0], 4: [1, 0], 5: [1, 0]}
```

注意我们用策略组这个词，表示在 $[s_0,\cdots,s_5]$ 六个状态下的各自的**策略单元**动作选择，策略组-0中的策略是都选红色球。

好了，准备工作完成，接下来可以把每一种策略组代入环境中，计算价值函数了：

```python
if __name__=="__main__":
    all_policy_in_binary = create_binary_policy()   # 二进制形式
    print("二进制形式的策略组 : ")
    print("-"*30)
    print(all_policy_in_binary)
    print("="*30)
    # 遍历所有策略组合
    gamma = 1
    max_iteration = 1000
    V_values = []
    print("OneHot形式的策略组与状态价值函数 : ")
    print("-"*20)
    for id, actions in enumerate(all_policy_in_binary):
        policy = create_onehot_policy(actions)      # onehot形式
        print(str.format("策略组-{0}:\t{1}", id, policy))
        env = dataModel.Env(policy)     # 创建环境，代入策略组合
        V, Q = algo.calculate_Vpi_Qpi(env, gamma, max_iteration)    # 迭代法计算V,Q
        V_values.append(V)              # 保存每个策略组合的价值函数结果,便于比较
        print(str.format("状态价值函数:\t{0}",V))
        print("-"*10)
        ......
```

上述代码将会输出所有策略组合及其状态价值函数：

```python
策略组-0:   {0: [1, 0], 1: [1, 0], 2: [1, 0], 3: [1, 0], 4: [1, 0], 5: [1, 0]}
价值函数：  [1.048 0.5   0.56  0.8   0.5   0.7   0.   ]
策略组合-1: {0: [1, 0], 1: [1, 0], 2: [1, 0], 3: [1, 0], 4: [1, 0], 5: [0, 1]}
价值函数：  [1.048 0.5   0.56  0.8   0.5   0.75  0.   ]
......
策略组-62:  {0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [0, 1], 5: [1, 0]}
价值函数：  [1.26 0.6  0.55 0.8  0.6  0.7  0.  ]
策略组-63:  {0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [0, 1], 5: [0, 1]}
价值函数：  [1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
```

一共 64 组，为了节省篇幅，中间的部分省略，请读者自己运行代码观察全部结果。

接下来，要通过比较 $v_\pi(s_0)$ 的大小，来搜索最优策略组。$v_\pi(s_0)$ 的值就是在上面输出的价值函数列表中的第一个单元的值。

```python
# 搜索最优策略组合
def find_best_policy(V_values, all_policy_in_binary):
    v = np.array(V_values)                    # 列表变成数组
    v0_best = np.max(v[:,0])                  # 获得所有策略组合中 v(s0) 的最大值
    print("v(s0)的最优价值函数 :", v0_best)
    print("="*40)
    best_ids = np.argwhere(v[:,0] == v0_best) # 获得所有的最大值的策略组合序号
    return best_ids

if __name__=="__main__":
    ......
    print("="*40)
    # 输出最优策略的价值函数
    best_ids = find_best_policy(V_values, all_policy_in_binary)
    print("二进制形式的最优策略组与最优价值函数 :")
    print("-"*20)
    for id in best_ids:
        policy = create_onehot_policy(all_policy_in_binary[id[0]])      # onehot形式
        print(str.format("最优策略组-{0}:\t\t{1}", id[0], all_policy_in_binary[id[0]]))
        env = dataModel.Env(policy)     # 创建环境，代入策略组合
        V, Q = algo.calculate_Vpi_Qpi(env, gamma, max_iteration)    # 迭代法计算V,Q
        print(str.format("最优状态价值函数:\t{0}", V))
        print("最优动作价值函数:")
        print(Q)
        print("-"*10)
```

上述代码会输出最大值，以及达到该最大值的策略组合序号、二进制形式的策略组，以及对应的状态价值函数：

```
v(s0)的最优价值函数 : 1.29
========================================
OneHot形式的最优策略组与最优价值函数 :
--------------------
最优策略组-35:  [1 0 0 0 1 1]
最优状态价值函数:       [1.29 0.5  0.56 0.8  0.6  0.75 0.  ]
----------
最优策略组-39:  [1 0 0 1 1 1]
最优状态价值函数:       [1.29 0.5  0.56 0.8  0.6  0.75 0.  ]
----------
最优策略组-43:  [1 0 1 0 1 1]
最优状态价值函数:       [1.29 0.5  0.55 0.8  0.6  0.75 0.  ]
----------
最优策略组-47:  [1 0 1 1 1 1]
最优状态价值函数:       [1.29 0.5  0.55 0.8  0.6  0.75 0.  ]
----------
最优策略组-51:  [1 1 0 0 1 1]
最优状态价值函数:       [1.29 0.6  0.56 0.8  0.6  0.75 0.  ]
----------
最优策略组-55:  [1 1 0 1 1 1]
最优状态价值函数:       [1.29 0.6  0.56 0.8  0.6  0.75 0.  ]
----------
最优策略组-59:  [1 1 1 0 1 1]
最优状态价值函数:       [1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
----------
最优策略组-63:  [1 1 1 1 1 1]
最优状态价值函数:       [1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
```

$v_\pi(s_0)$ 的最大值为 1.29，可以达到该值的策略组有 8 个，序号为：[35, 39, 43, 47, 51, 55, 59, 63]。

### 9.4.3 结果分析

#### 策略解读

在总共 64 个策略组合中，有 8 个可以令达 $v_\pi(s_0)$ 达到最大值。这说明针对本例，最优策略并不是唯一的。在其它强化学习问题中，也是如此：

1. 可以保证有最优策略；
2. 可以保证找到最优策略；
3. 不能保证找到所有最优策略。

观察策略组序号的话，对数字敏感的读者可能会发现，它是一个等差数列，相邻数字之间相差 4。这个现象很有趣。进一步，我们可以把策略组的情况（二进制表示形式）做一个反异或（先异或，再求反）操作：

表 9.4.1 最优策略组合中的动作选择（1表示选择蓝球，0表示选择红球）

|策略组合序号|$\pi(s_0)$|$\pi(s_1)$|$\pi(s_2)$|$\pi(s_3)$|$\pi(s_4)$|$\pi(s_5)$|
|:-:|:-:|:-:|:-:|:-:|:-:|:-:|
|35|1|0|0|0|1|1|
|39|1|0|0|1|1|1|
|43|1|0|1|0|1|1|
|47|1|0|1|1|1|1|
|51|1|1|0|0|1|1|
|55|1|1|0|1|1|1|
|59|1|1|1|0|1|1|
|63|1|1|1|1|1|1|
|NOT-XOR|1|0|0|0|1|1|

从表 9.4.1 中的反异或结果可以看到，上述 8 个策略组中的共同点是在 $\pi(s_0),\pi(s_4),\pi(s_5)$ 三个策略上都选择 1（蓝色球），而在 $\pi(s_1),\pi(s_2),\pi(s_3)$ 三个策略上选择什么无所谓。

为什么会是这样呢？我们可以通过输出最优策略组合的状态/动作价值函数来分析。

以策略组合(51)为例，其最优状态价值函数和动作价值函数的结果如下：

```
策略组合(51):  [1 1 0 0 1 1]
最优状态价值函数:   [1.29 0.6  0.56 0.8  0.6  0.75 0.  ]
最优动作价值函数:
[[1.128 1.29 ]  # [q(s0,a0), q(s0,a1)]
 [0.5   0.6  ]  # [q(s1,a0), q(s1,a1)]
 [0.56  0.55 ]  # [q(s2,a0), q(s2,a1)]
 [0.8   0.8  ]  # [q(s3,a0), q(s3,a1)]
 [0.5   0.6  ]  # [q(s4,a0), q(s4,a1)]
 [0.7   0.75 ]  # [q(s5,a0), q(s5,a1)]
 [0.    0.   ]] # [q(s6,a0), q(s6,a1)]
```

数据解读：

- 在 $s_0$ 状态，选择射击蓝色气球；
- 射击蓝色气球后，会转移到 $s_4,s_5$：
    - 如果状态转移到 $s_4$，继续射击蓝色气球；
    - 如果状态转移到 $s_5$，也是继续射击蓝色气球。
    
虽然从逻辑上说，射击蓝色气球的动作已经决定了不可能再走左半分支（这就是表 9.4.1 反异或的结果的说明），但是该策略还是告诉了我们：
- 如果在 $s_0$ 状态，选择射击红色气球；
   - 在 $s_1$ 状态，选择蓝色气球；
    - 在 $s_2$ 状态，选择红色气球；
    - 在 $s_3$ 状态，选择红色气球。

#### 价值函数解读

把 9.4.2 节中的价值函数输出的晦涩数字标注在图 9.4.2 中，一目了然：

<center>
<img src="./img/shoot-result-search.png">

图 9.4.2 搜索最优策略的结果
</center>

绿色数字仍然是状态转移概率。其它图例说明如下：

- 状态价值函数（空心圆）

    - 最优状态价值函数的数值，普遍比图 8.5.3 中在通用策略 [0.4, 0.6] 下的状态价值函数要高。
    - $v_\pi(s_1),v_\pi(s_2)$ 分别有两个值，是因为上述 8 个策略的返回在这里略有不同。
    - 但目前只需要关心 $v_\pi(s_0)$ 即可，因为它反应了策略的整体表现。

- 动作价值函数（实心圆）

    - 最底层的动作价值函数没有变化，因为下面紧接着终止状态，在图中没有画出。
    - 上层的动作价值函数比通用策略下的高，因为下游的价值函数值提高了。

- 策略选择（蓝色数字）

    用 0 表示不选择，1 表示被选择。可以看到：
    - 在 $s_0,s_4,s_5$ 状态都选择了蓝色球。
    - 在 $s_1,s_2,s_3$ 状态，由于 8 个最优策略的选择各不相同，所以没有标出来。

- 虚线箭头

    在每个状态下的两个动作价值函数，相对较大的那个 $q_\pi$ 值与上游状态的价值函数 $v_\pi$ 相等，如虚线箭头所示。这并不是巧合，在下一节会讲解其原理。

### 思考与练习


