
## 9.4 搜索最优策略组合

###  9.4.1 最优策略

对 $\pi$ 的修改会提高或降低 $v_\pi(s_0)$ 的值，这还只是独立修改一个策略的影响，那么一共有多少种组合可以帮助游客提高收益呢？其最大值又是多少呢？本节的任务就是给每个状态都找到最好的策略，从而使整体状态值达到最佳。

但是在具体操作中会遇到这样的困难：假设我们知道了一个状态上的最佳策略是 [0.4,0.6]，但是不可能把一支箭分成 0.4:0.6，所以你必须二选一，用公式表示为：

$$
\pi_*(a_i \mid s)=
\begin{cases}
1, & if \ \pi(a_i|s)=\argmax \limits_{a \in A(s)} \pi(a|s)
\\
0, & 其它动作
\end{cases}
\tag{9.4.1}
$$

当 $\pi=[0.4,0.6], A(s)=[a_0,a_1]$ 时，即 $\pi(a_0|s)=0.4, \pi(a_1|s)=0.6$，式 9.4.1 的结果是：$\pi_*(a_0|s)=0,\pi_*(a_1|s)=1$，因为 0.6 > 0.4 且处于数组中第 1 个位置（从 0 开始计算）。

从另一个角度看，以两个动作为例，假设原始策略为 [0.4, 0.6]，如果新策略 [0.2, 0.8] 会提高整体状态函数值的话，那么另一个新策略 [0.1, 0.9] 甚至 [0.0, 1.0] 也一定会更大地提高状态函数值。读者只需要把上一小节的策略改一改就知道了，比如这一行代码：
```python
[0.2,0.8],  # 修改状态 0 的策略，可以尝试 [0.1, 0.9] or [0.0, 1.0]
```
因此，当$\pi'=[0.0,1.0]$时， $\pi(a_0|s)=0,\pi(a_1|s)=1$ 也就自然成立了。

当两个概率值相等时，我们可以选择任意一个，总选第一个的话，可能会失去其它机会。

在图 9.3.1 中，一共有 6 个有效的状态（$s_0,\cdots,s_5$) ，在每个状态上的策略有两种动作选择（红球或蓝球），这样一共有 $2\times2\times2\times2\times2\times2=2^6$ 种组合。推广到一般情况：如果状态空间为 $S$，动作空间为 $A$，则策略组合是 $|A|^{|S|}$ 种。

对于射击气球这个简单的问题来说，因为计算一次 $v_\pi,q_\pi$ 只需要迭代两次，即使用遍历的方法，也很容易快速得到结果。所以，我们先用最笨但是最准确的遍历来得到评估的基准。

### 9.4.2 策略遍历搜索

知道了一共有 $2^6=64$ 种组合，用手写出来还是比较累的，但是作为一个合格的程序员，我们可以用代码生成 64 个组合策略，实际上就是用二进制表示的 [0, 63]：
```
[[0 0 0 0 0 0]  # 红 红 红 红 红 红
 [0 0 0 0 0 1]  # 红 红 红 红 红 蓝
 [0 0 0 0 1 0]  # 红 红 红 红 蓝 红
 ......
 [1 1 1 1 0 1]  # 蓝 蓝 蓝 蓝 红 蓝
 [1 1 1 1 1 0]  # 蓝 蓝 蓝 蓝 蓝 红
 [1 1 1 1 1 1]] # 蓝 蓝 蓝 蓝 蓝 蓝
```
上面的输出中，0 表示在该状态选择红色球，1 表示在该状态选择蓝色球。

下面要根据式 9.4.1，把上述的二进制形式翻译一下，变成模型可以认识的策略数据结构：用 onehot（热独）编码来表示选择红球或者蓝球，比如：[1, 0] 表示选择红球，[0, 1] 表示选择蓝球。

【代码位置】Shoot_3_OptimalSearch.py

```python
# 创建onehot编码形式的policy
def create_onehot_policy(actions):
    policy = {}
    for s in range(len(actions)):  # onehot
        policy[s] = [1,0] if actions[s]==0 else [0,1]
    return policy
```
以**策略组合**(0)为例，输出格式如下：

```
策略组合(0)：{0: [1, 0], 1: [1, 0], 2: [1, 0], 3: [1, 0], 4: [1, 0], 5: [1, 0]}
```

注意我们用**策略组合**这个词，表示在 $[s_0,\cdots,s_5]$ 六个状态下的各自的**策略**动作选择，策略组合 0 中的策略是都选红色球。

好了，准备工作完成，接下来可以把各种策略组合代入环境中，计算价值函数了：

```python
if __name__=="__main__":
    all_policy_in_binary = create_binary_policy()   # 二进制形式
    print(all_policy_in_binary)
    # 遍历所有策略组合
    gamma = 1
    max_iteration = 1000
    V_values = []
    for id, actions in enumerate(all_policy_in_binary):
        policy = create_onehot_policy(actions)      # onehot形式
        print(str.format("策略组合（{0}）: {1}", id, policy))
        env = dataModel.Env(policy)     # 创建环境，代入策略组合
        V, Q = algo.calculate_Vpi_Qpi(env, gamma, max_iteration)    # 迭代法计算V,Q
        V_values.append(V)              # 保存每个策略组合的价值函数结果,便于比较
        print("价值函数 ：", V)
```

上述代码将会输出所有策略组合及其状态价值函数：

```python
策略组合(0): {0: [1, 0], 1: [1, 0], 2: [1, 0], 3: [1, 0], 4: [1, 0], 5: [1, 0]}
价值函数：[1.048 0.5   0.56  0.8   0.5   0.7   0.   ]
策略组合(1):{0: [1, 0], 1: [1, 0], 2: [1, 0], 3: [1, 0], 4: [1, 0], 5: [0, 1]}
价值函数：[1.048 0.5   0.56  0.8   0.5   0.75  0.   ]
......
策略组合 62:{0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [0, 1], 5: [1, 0]}
价值函数：[1.26 0.6  0.55 0.8  0.6  0.7  0.  ]
策略组合 63:{0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [0, 1], 5: [0, 1]}
价值函数：[1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
```

一共 64 组，为了节省篇幅，中间的部分省略，请读者自己运行代码观察全部结果。

接下来，要通过比较 $v_\pi(s_0)$ 的大小，来搜索最优策略组合。$v_\pi(s_0)$ 的值就是在上面输出的价值函数列表中的第一个单元的值。

```python
# 搜索最优策略组合
def find_best_policy(V_values, all_policy_in_binary):
    v = np.array(V_values)                    # 列表变成数组
    v0_best = np.max(v[:,0])                  # 获得所有策略组合中 v(s0) 的最大值
    print("v(s0)的最优价值函数 :", v0_best)
    best_ids = np.argwhere(v[:,0] == v0_best) # 拥有最大值的策略组合可能不止一个
    for id in best_ids:
        print(str.format("策略组合({0}):{1}", id[0], all_policy_in_binary[id[0]]))
        print("状态价值函数 :", v[id][0])
        print("====")
```

上述代码会输出最大值，以及达到该最大值的策略组合序号、策略组合情况（二进制形式），以及对应的状态价值函数：

```
v(s0)的最优价值函数 : 1.29
====
策略组合(35):[1 0 0 0 1 1]  # 二进制形式
状态价值函数 : [1.29 0.5  0.56 0.8  0.6  0.75 0.  ]
====
策略组合(39):[1 0 0 1 1 1]
状态价值函数 : [1.29 0.5  0.56 0.8  0.6  0.75 0.  ]
====
策略组合(43):[1 0 1 0 1 1]
状态价值函数 : [1.29 0.5  0.55 0.8  0.6  0.75 0.  ]
====
策略组合(47):[1 0 1 1 1 1]
状态价值函数 : [1.29 0.5  0.55 0.8  0.6  0.75 0.  ]
====
策略组合(51):[1 1 0 0 1 1]
状态价值函数 : [1.29 0.6  0.56 0.8  0.6  0.75 0.  ]
====
策略组合(55):[1 1 0 1 1 1]
状态价值函数 : [1.29 0.6  0.56 0.8  0.6  0.75 0.  ]
====
策略组合(59):[1 1 1 0 1 1]
状态价值函数 : [1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
====
策略组合(63):[1 1 1 1 1 1]
状态价值函数 : [1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
```

$v_\pi(s_0)$ 的最大值为 1.29，可以达到该值的策略组合有 8 个：[35, 39, 43, 47, 51, 55, 59, 63]。

### 9.4.3 结果分析

#### 策略解读

在总共 64 个策略组合中，有 8 个可以达到最大值。这说明针对本例，最优策略并不是唯一的。在其它强化学习问题中，也是如此：

1. 可以保证有最优策略；
2. 可以保证找到最优策略；
3. 不能保证找到所有最优策略。

观察策略组合序号的话，对数字敏感的读者可能会发现，它是一个等差数列，相邻数字之间相差 4。这个现象很有趣。进一步，我们可以把策略组合的情况（二进制表示形式）做一个反异或（先异或，再求反）操作：

表 9.4.1 最优策略组合中的动作选择

|策略组合序号|$\pi(s_0)$|$\pi(s_1)$|$\pi(s_2)$|$\pi(s_3)$|$\pi(s_4)$|$\pi(s_5)$|
|:-:|-|-|-|-|-|-|
|35|1|0|0|0|1|1|
|39|1|0|0|1|1|1|
|43|1|0|1|0|1|1|
|47|1|0|1|1|1|1|
|51|1|1|0|0|1|1|
|55|1|1|0|1|1|1|
|59|1|1|1|0|1|1|
|63|1|1|1|1|1|1|
|NOT-XOR|1|0|0|0|1|1|

从表 9.4.1 中可以看到，上述 8 个策略组合中的共同点是在 $\pi(s_0),\pi(s_4),\pi(s_5)$ 三个策略上都选择 1（蓝色球），而在 $\pi(s_1),\pi(s_2),\pi(s_3)$ 三个策略上选择什么无所谓。

为什么会是这样呢？我们可以通过输出最优策略组合的状态/动作价值函数来分析。

```python
    # 输出最优策略的价值函数
    best_ids = find_best_policy(V_values, all_policy_in_binary)
    for id in best_ids:
        policy = create_onehot_policy(all_policy_in_binary[id[0]])      # onehot形式
        print(str.format("策略组合({0}): {1}", id[0], policy))
        env = dataModel.Env(policy)     # 创建环境，代入策略组合
        V, Q = algo.calculate_Vpi_Qpi(env, gamma, max_iteration)    # 迭代法计算V,Q
        print("最优状态价值函数 :", V)
        print("最优动作价值函数 :\n", Q)
```

以策略组合(51)为例，其最优状态价值函数和动作价值函数的结果如下：

```
策略组合(51): {0: [0, 1], 1: [0, 1], 2: [1, 0], 3: [1, 0], 4: [0, 1], 5: [0, 1]}
最优状态价值函数 : [1.29 0.6  0.56 0.8  0.6  0.75 0.  ]
最优动作价值函数 :
 [[1.128 1.29 ] # [q(s0,a0), q(s0,a1)]
 [0.5   0.6  ]  # [q(s1,a0), q(s1,a1)]
 [0.56  0.55 ]  # [q(s2,a0), q(s2,a1)]
 [0.8   0.8  ]  # [q(s3,a0), q(s3,a1)]
 [0.5   0.6  ]  # [q(s4,a0), q(s4,a1)]
 [0.7   0.75 ]  # [q(s5,a0), q(s5,a1)]
 [0.    0.   ]] # [q(s6,a0), q(s6,a1)]
```

首先看策略组合：

- 在 $s_0$ 状态，选择射击蓝色气球；

    射击蓝色气球后，会转移到 $s_4,s_5$：

    - 如果状态转移到 $s_4$，继续射击蓝色气球；
    - 如果状态转移到 $s_5$，也是继续射击蓝色气球。
    
    虽然从逻辑上说，射击蓝色气球的动作已经决定了不可能再走左半分支（这就是表 9.4.1 反异或的结果的说明），但是该策略还是告诉了我们：

    - 在 $s_1$ 状态，选择蓝色气球；
    - 在 $s_2$ 状态，选择红色气球；
    - 在 $s_3$ 状态，选择红色气球。

#### 价值函数解读

把上面的价值函数输出中的晦涩的数字标注在图 9.4.1 中，一目了然：

<center>
<img src="./img/shoot-result-search.png">

图 9.4.1 搜索最优策略的结果
</center>

绿色数字仍然是状态转移概率。其它图例说明如下：

- 状态价值函数（空心方框）

    - 最优状态价值函数的数值，普遍比图 8.5.3 中在通用策略 [0.4, 0.6] 下的状态价值函数要高。
    - 但我们只需要关心 $v_\pi(s_0)$ 即可，因为它反应了策略的整体表现。

- 动作价值函数（实心圆）

    - 最底层的动作价值函数没有变化，因为下面紧接着终止状态，在图中没有画出。
    - 上层的动作价值函数比通用策略下的高，因为下游的价值函数值提高了。

- 策略选择（蓝色数字与虚线箭头）

    用 0 表示不选择，1 表示被选择。可以看到：
    - 在 $s_0,s_4,s_5$ 状态都选择了蓝色球。
    - 在 $s_1,s_2,s_3$ 状态，由于 8 个最优策略的选择各不相同，所以没有标出来。
    - 在每个状态下的两个动作价值函数，相对较大的那个 $q_\pi$ 值与上游状态的价值函数 $v_\pi$ 相等，如虚线箭头所示。这并不是巧合，在下一节会讲解其原理。

### 思考与练习


