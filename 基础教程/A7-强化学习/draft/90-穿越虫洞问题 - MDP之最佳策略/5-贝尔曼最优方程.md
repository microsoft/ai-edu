
## 9.5 贝尔曼最优方程

### 9.5.1 最优

先给出两个定义


$$
\begin{aligned}
v_*(s) &\doteq \max_\pi v_\pi(s) 
\\
&=\max\big[v_{\pi_1}(s),v_{\pi_2}(s),\cdots,v_{\pi_n}(s)\big]
\end{aligned}
\tag{9.5.1}
$$

对于任意状态 $s \in S$，假设采用了 $n$ 个不同的策略 $\pi_i$ 时，得到了 $n$ 个相同或不同的 $v_{\pi_i}(s)$，其中最大的那个值，定义为状态 $s$ 下的最优状态价值函数 $v_*(s)$，对应的 $\pi_i$ 称为最优策略 $\pi_*$。

举例来说，上一小节中的输出

```
策略组合(59):   {0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [1, 0], 4: [0, 1], 5: [0, 1]}
状态价值函数:   [1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
----------
策略组合(60):   {0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [1, 0], 5: [1, 0]}
状态价值函数:   [1.22 0.6  0.55 0.8  0.5  0.7  0.  ]
----------
策略组合(61):   {0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [1, 0], 5: [0, 1]}
状态价值函数:   [1.25 0.6  0.55 0.8  0.5  0.75 0.  ]
----------
策略组合(62):   {0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [0, 1], 5: [1, 0]}
状态价值函数:   [1.26 0.6  0.55 0.8  0.6  0.7  0.  ]
----------
策略组合(63):   {0: [0, 1], 1: [0, 1], 2: [0, 1], 3: [0, 1], 4: [0, 1], 5: [0, 1]}
状态价值函数:   [1.29 0.6  0.55 0.8  0.6  0.75 0.  ]
```
我们只比较 $v_\pi(s_0)$，即输出中的状态价值函数列表的第一个值，做出如下判断：

1. 在此片段中一共有 $v_{\pi_i}(s_0)=[1.29, 1.22, 1.25, 1.26, 1.29]$ 五个值，$i=[59,\cdots,63]$；
2. 对应着策略 $\pi_{59}(s_0),\pi_{60}(s_0),\pi_{61}(s_0),\pi_{62}(s_0),\pi_{63}(s_0)$；
3. 其中有两个相同的最大值 1.29；
4. 则策略 $\pi_{59}(s_0),\pi_{63}(s_0)$ 同为最优策略。

$$
\begin{aligned}
q_*(s,a) &\doteq \max_\pi q_\pi(s,a) 
\\
&=\max\big[q_{\pi_1}(s,a),q_{\pi_2}(s,a),\cdots,q_{\pi_n}(s,a)\big]
\end{aligned}
\tag{9.5.2}
$$

对于任意状态 $s \in S$ 和动作 $a \in A(s)$，假设采用了 $n$ 个不同的策略 $\pi_i$ 时，得到了 $n$ 个相同或不同的 $q_{\pi_i}(s,a)$，其中最大的那个值，定义为状态 $s$ 下采用动作 $a$ 的最优动作价值函数 $q_*(s,a)$，对应的 $\pi_i$ 称为最优策略 $\pi_*$。




$$
v_*(s)= \max_a q_*(s,a)
$$



$$
\pi_*=\argmax_\pi v_\pi(s)
$$

$$
v_*(s)= \max_{a} \Big(\sum_{s'} p^a_{ss'} r^a_{ss'} + \gamma \sum_{s'} p^a_{ss'} v_*(s') \Big )
$$

$$
q_*(s,a) = \sum_{s'} p^a_{ss'} r^a_{ss'} + \gamma \sum_{s'} p^a_{ss'} v_*(s')
$$

$$
q_*(s,a) = \sum_{s'} p^a_{ss'} r^a_{ss'} + \gamma \sum_{s'} p^a_{ss'} \max_{a'} q_*(s',a')
$$


比较 贝尔曼方程，贝尔曼期望方程，贝尔曼最优方程


|过程名称|组成元素|数据序列|计算|
|-|-|-|-|
|马尔可夫过程 MP|$<S,P>$|$S_0,S_1,\cdots,S_t$||
|马尔可夫奖励过程 MRP|$<S,P,R,\gamma>$|$S_0,R_1,S_1,R_2,\cdots,S_t,R_{t+1}$|$V$|
|马尔可夫决策过程 MDP|$<S,A,P,R,\gamma>$|$S_0,A_0,R_1,S_1,A_1,R_2,\cdots,S_t,A_t,R_{t+1}$|$V_\pi,Q_\pi$|
|马尔可夫决策过程 MDP|$<S,A,P,R,\gamma>$|$S_0,A_0,R_1,S_1,A_1,R_2,\cdots,S_t,A_t,R_{t+1}$|$V_*,Q_*$|


- 可以比较policy iteration and value iteration
- 可以绘图：结果图、曲线（趋势）图

- maze  https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html

