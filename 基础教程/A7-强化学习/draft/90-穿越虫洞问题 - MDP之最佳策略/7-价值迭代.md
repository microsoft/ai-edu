
要解决穿越虫洞问题

https://zhuanlan.zhihu.com/p/28868460

价值迭代可以从策略迭代推导得到

动态规划

$$
\pi_*=\argmax_\pi v_\pi(s)
$$

$$
v_*(s)= \max_{a} \Big(\sum_{s'} p^a_{ss'} r^a_{ss'} + \gamma \sum_{s'} p^a_{ss'} v_*(s') \Big )
$$

$$
q_*(s,a) = \sum_{s'} p^a_{ss'} r^a_{ss'} + \gamma \sum_{s'} p^a_{ss'} v_*(s')
$$

$$
q_*(s,a) = \sum_{s'} p^a_{ss'} r^a_{ss'} + \gamma \sum_{s'} p^a_{ss'} \max_{a'} q_*(s',a')
$$


比较 贝尔曼方程，贝尔曼期望方程，贝尔曼最优方程


|过程名称|组成元素|数据序列|计算|
|-|-|-|-|
|马尔可夫过程 MP|$<S,P>$|$S_0,S_1,\cdots,S_t$||
|马尔可夫奖励过程 MRP|$<S,P,R,\gamma>$|$S_0,R_1,S_1,R_2,\cdots,S_t,R_{t+1}$|$V$|
|马尔可夫决策过程 MDP|$<S,A,P,R,\gamma>$|$S_0,A_0,R_1,S_1,A_1,R_2,\cdots,S_t,A_t,R_{t+1}$|$V_\pi,Q_\pi$|
|马尔可夫决策过程 MDP|$<S,A,P,R,\gamma>$|$S_0,A_0,R_1,S_1,A_1,R_2,\cdots,S_t,A_t,R_{t+1}$|$V_*,Q_*$|


- 可以比较policy iteration and value iteration
- 可以绘图：结果图、曲线（趋势）图

- maze  https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html

