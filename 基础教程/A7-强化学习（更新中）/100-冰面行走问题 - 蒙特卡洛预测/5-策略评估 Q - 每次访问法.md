## 10.5 策略评估（预测）$Q_\pi$

### 10.5.1 策略

在 10.4 小节中，我们学习如何预测 $V_\pi$，而且也提到过，预测 $V_\pi$ 的意义其实不是很大，更有用的还是预测 $Q_\pi$。所以，在本小节中，我们将会使用每次访问法估算指定策略的 $Q_\pi$ 值。

在第八章的射击气球问题中，开始引入了策略，当时只有两个动作：射击红色气球或者蓝色气球，非常简单。第九章的穿越虫洞问题中，我们使用价值迭代直接找到了最优策略，但其实对策略的理解还不够深入。所以在本章中利用冰面行走问题，更深入地研究一下策略问题。

在 10.4 节中，使用了随机策略，即在四个方向上都有 0.25 的概率被选择执行。那么非随机（定向）策略应该如何定义呢？见图 10.5.1。

<center>
<image src="./img/MC-105-3Policy.png">

图 10.5.1 三种策略
（左图：正确策略；中图：随机策略；右图：错误策略）
</center>

在图 10.5.1 中，我们给出了三种策略定义，描述如下：

- 正确策略

    因为起点在左上角，终点在右下角，所以给出的策略是各有 0.3 的概率向下方和右侧移动，各有 0.2 的概率向错误方向移动。当然，这里的“错误方向”只是针对这个问题的设置而言，在有些设置中，智能体不得不向左侧或者上方移动来避免掉入冰洞。而“正确方向”也只是在大方向上是正确的，具体到某个状态上，比如 $s_4$，就不能向右侧移动，而是必须向下方甚至向上方移动才是安全的。但是智能体依然会在 $s_4$ 尝试向右侧移动，通过获得惩罚来学习如何找到安全路径。

- 随机策略

    向四个方向移动的概率各是 0.25。

- 错误策略

    每一步都有相对较大的概率 0.3 向与目标位置相反的方向移动，以较小的概率 0.2 向右下方移动。

用代码来描述的话是这样的：

```python
    policy_names = ["正确策略", "随机策略", "错误策略"]
    policies = [
        #left, down, right, up      四个值相加等于 1
        (0.20, 0.30, 0.30, 0.20),   # 正确策略
        (0.25, 0.25, 0.25, 0.25),   # 随机策略
        (0.30, 0.20, 0.20, 0.30)    # 错误策略
    ]
```

policies 数组的每一行都是一个策略，由于策略是一种概率表达，所以 4 个值相加要等于 1。

如果定义一个极端的策略，比如 (0.5, 0, 0, 0.5)，那么智能体将无法学习到安全路径。

### 10.5.2 算法

接下来要使用每次访问法来预测 $Q$ 值。


#### 算法说明

【算法 10.5】

---

输入：起始状态 $s$，策略 $\pi$，折扣 $\gamma$, 幕数 Episodes
初始化数组：$G(S,A) \leftarrow 0, N(S,A) \leftarrow 0$，$S$ 为状态空间，$A$ 为动作空间
多幕 Episodes 循环：
　　列表置空 $Episode = [\ ] $ 用于存储序列数据 $(s,a,r)$
　　幕内循环直到终止状态：
　　　　从 $s$ 根据策略 $\pi$ 得到动作 $a$
　　　　执行 $a$ 从环境得到 $s',r$ 以及是否终止的标志
　　　　$Episode \Leftarrow (s,a,r)$，相当于是 $(s_t,a_t,r_{t+1})$
　　　　$s \leftarrow s'$
　　$G_t \leftarrow 0$
　　对 $Episode$ 从后向前遍历, $t=\tau-1,\tau-2,...,0$
　　　　取出 $(s_t,a_t,r_{t+1})$
　　　　$G_t \leftarrow \gamma G_t+r_{t+1}$
　　　　$G(s_t,a_t) \leftarrow G(s_t,a_t)+G_t$
　　　　$N(s_t,a_t) \leftarrow N(s_t,a_t)+1$
$Q(S,A) \leftarrow G(S,A) / N(S,A)$
输出：$Q(S,A)$

---

与算法 10.4 的主要区别：

- 初始化的数组为二维 (S,A)，记录每个状态 $s$ 下的每个动作 $a$ 的 G 值。
- Episode 中保存 $s,a,r$，比算法 10.4 多一个动作 $a$。

#### 算法实现

【代码位置】Algo_MC_Policy_Evaulation.py

```python
# MC 策略评估（预测）：每次访问法估算 Q_pi
def MC_EveryVisit_Q_Policy(env, start_state, episodes, gamma, policy):
    nA = env.action_space.n                 # 动作空间
    nS = env.observation_space.n            # 状态空间
    Value = np.zeros((nS, nA))              # G 的总和
    Count = np.zeros((nS, nA))              # G 的数量
    for episode in tqdm.trange(episodes):   # 多幕循环
        s = env.reset() # 重置环境，开始新的一幕采样
        Episode = []    # 一幕内的(状态,动作,奖励)序列
        done = False
        while (done is False):              # 幕内循环
            action = np.random.choice(nA, p=policy[s])  # 根据策略选择动作
            next_s, reward, done, _ = env.step(action)  # 与环境交互得到反馈
            Episode.append((s, action, reward))         # 保存 s,a,r
            s = next_s

        num_step = len(Episode)
        G = 0
        # 从后向前遍历计算 G 值
        for t in range(num_step-1, -1, -1):
            s, a, r = Episode[t]
            G = gamma * G + r
            Value[s,a] += G     # 值累加
            Count[s,a] += 1     # 数量加 1

    Count[Count==0] = 1 # 把分母为0的填成1，主要是针对终止状态Count为0
    Q = Value / Count   # 求均值
    return Q  
```

上述代码中与预测 V 函数不同的地方在于 Value 和 Count 数值都是二维的，因为加入了“动作”这一维信息。

### 10.5.3 结果

#### 三种策略的误差比较

以下是主程序中的主要参数设置：

```
    gamma = 0.9
    episodes = 20000
```

折扣值 0.9，每个策略做 20000 幕采样。

但是我们从 10.4 节学习到，蒙特卡洛法是高偏差的，所以需要把三种策略各运行 10 次（每次都是 20000 幕采样），求其误差的平均值（不是平均值的误差），得到图 10.5.2。

<center>
<image src="./img/MC-105-3-Policy-error.png">

图 10.5.2 三种策略 10 次运行的误差的平均值变化比较
</center>

这个结果可能和读者的想象不太一样：三种策略的误差随着循环次数的增加并没有很大的区别。这说明在不同的策略下，算法都可以评估出正确的动作价值函数，而且收敛速度没有差别。

其它运行指标展示在表 10.5.1 中。

表 10.5.1 三种策略的结果比较

||正确策略|随机策略|错误策略|
|-|-|-|-|
|$\pi(s)$|[0.2, 0.3, 0.3, 0.2]|[0.25, 0.25, 0.25, 0.25]|[0.3, 0.2, 0.2, 0.3]|
|迭代次数(阈值=1e-3)|10|8|6|
|迭代次数(阈值=1e-4)|15|14|12|
|迭代次数(阈值=1e-5)|20|20|20|
|迭代次数(阈值=1e-6)|24|26|28|
|迭代次数(阈值=1e-7)|28|32|35|
|评估时长(秒)|5|7|9|
|平均每幕长度|6.1|7.7|10.4|

数据说明：

- 当阈值为 1e-4 时，用贝尔曼方程的迭代法都是 15 次以内可以收敛，这和图 10.5.2 是相符的，也就是策略不影响收敛速度。而且有趣的是，我们认为是正确的策略，迭代次数反而比错误的策略要多 3 次。

    但是当阈值精度提高时，正确策略用的迭代次数就会比错误策略少了。

- 但是用时方面就会有明显差异。单次运行时，正确策略用时最短，错误策略用时最长。

    为什么错误策略用时长呢？因为方向性的错误，所以导致智能体需要花费更多的步数达到目的地，正确策略最短，平均用 6.1 步，而错误策略最长，平均用 10.4 步（因为大概率会走反）。


#### 动作价值函数结果

图 10.5.3 显示了三种策略下的 16 个状态上的 4 个动作的价值，笔者特意用红色把 4 个动作的最大值标记出来，便于读者快速阅读。

<center>
<image src="./img/Q-Value.png">

图 10.5.3 三种策略的动作价值函数
（左图：正确策略；中图：随机策略；右图：错误策略）
</center>

读者可以发现，左图中的数值普遍较大，而右图中的数值普遍较小。这是为什么呢？

我们回忆一下第九章中的最优动作价值函数的定义：

$$
\begin{aligned}
q_*(s,a) &\doteq \max_\pi q_\pi(s,a) 
\\
&=\max\big[q_{\pi_1}(s,a),q_{\pi_2}(s,a),\cdots,q_{\pi_m}(s,a)\big]
\end{aligned}
\tag{10.5.1}
$$

通过式（10.5.1）可以知道，不同的策略 $\pi$ 通过策略评估会得到不同的动作价值函数值 $q_\pi$，值越大的越接近最优动作价值。在图 10.5.3 中，左图是正确的策略（但可能不是最优策略），而右图的策略肯定要比左图的差，所以右图的数值就会小。

但是不管总体值是大是小，在一个策略中的一个状态的四个动作中，总会有一个最大值，比如 $s_0$ 的第二个动作（向下方移动），而三个策略都是在相同位置上具有最大值，所以我们把这个最大值所代表的动作绘制在图 10.5.4 中。

<center>
<image src="./img/Q-MaxValue.png">

图 10.5.4 三种策略的最佳动作方向
</center>

读者会发现有一些状态的动作价值全为 0，是因为该状态是终止状态，位于 $s_5,s_7,s_{11},s_{12},s_{15}$，包括 4 个冰洞和一个目的地。

读者在这里可能会产生一个问题，为什么看似错误的策略，和一个正确的策略，都能得到同样的正确的结果呢？

我们所说的“错误策略”，实际上还有没有错到离谱的程度，比如读者可以试试（0.5, 0.0, 0.0, 0.5）这个策略，根本得不到正确路径。所以严格地说，上面三个策略并没有正确与错误的区别，只是花费的力气不同，一个具有好的 rule-based（基于规则）的策略，可以帮助智能体更快地学习到收敛状态。

比如，一个经验丰富的驾驶员，都知道有“让车不让道”的说法，意思是遇到前方或侧方的车或行人，第一反应是刹车，而不是躲避，因为躲避有可能带来更大的风险。在无人驾驶问题中，如果有上面这条规则，将会大大提高无人驾驶的安全性。

冰面行走问题是一个简单的问题，我们可以给出所谓的“正确策略”。但是遇到复杂问题时，不能明确什么是正确的，就用随机策略做初始化即可。
