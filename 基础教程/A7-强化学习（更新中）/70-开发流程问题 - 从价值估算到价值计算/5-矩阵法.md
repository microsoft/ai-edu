
## 7.5 用矩阵法解贝尔曼方程

### 7.5.1 从线性方程组到矩阵运算

观察式（7.4.1）方程组，可以把它变形为：

$$
\begin{bmatrix}
v_0
\\
v_1
\\
v_2
\\
v_3
\\
v_4
\\
v_5
\\
v_6
\end{bmatrix}
= \
\begin{bmatrix}
-3
\\
0
\\
+1
\\
+3
\\
+2
\\
-1
\\
0
\end{bmatrix}
+\gamma 
\begin{bmatrix}
0.2v_0+0.8v_1
\\
0.6v_0+0.4v_2
\\
0.1v_1+0.9v_3
\\
0.1v_1+0.2v_4+0.7v_5
\\
0.2v_1+0.5v_2+0.3v_3
\\
v_6
\\
0
\end{bmatrix}
\tag{7.5.1}
$$

关于式（7.5.1）的说明：

- 等式左侧的部分，就是所有状态的价值函数值组成的向量（数组），可以写成 $V(s)$。

- 等式右侧的第一项，就是状态上的奖励值组成的向量，可以写成 $R(s)$。

- 等式右侧的第二个矩阵，又可以写成两个矩阵的乘积：

$$
\begin{bmatrix}
0.2v_0+0.8v_1
\\
0.6v_0+0.4v_2
\\
0.1v_1+0.9v_3
\\
0.1v_1+0.2v_4+0.7v_5
\\
0.2v_1+0.5v_2+0.3v_3
\\
v_6
\\
0
\end{bmatrix}=
\begin{bmatrix}
0.2 & 0.8 & 0 & 0 & 0 & 0 & 0
\\
0.6 & 0 & 0.4 & 0 & 0 & 0 & 0
\\
0 & 0.1 & 0 & 0.9 & 0 & 0 & 0
\\
0 & 0.1 & 0 & 0 & 0.2 & 0.7 & 0
\\
0 & 0.2 & 0.5 & 0.3 & 0 & 0 & 0
\\
0 & 0 & 0 & 0 & 0 & 0 & 1.0
\\
0 & 0 & 0 & 0 & 0 & 0 & 0.0
\end{bmatrix}
\begin{bmatrix}
v_0
\\
v_1
\\
v_2
\\
v_3
\\
v_4
\\
v_5
\\
v_6
\end{bmatrix}
\tag{7.5.2}
$$

式（7.5.2）中，等式右侧的第一个矩阵就是该问题的状态转移矩阵 $P_{ss'}$，第二个矩阵是状态值向量 $V(s)$，于是，式（7.5.1）可以变成：

$$
V(s) = R(s)+ \gamma P_{ss'}V(s) \tag{7.5.3}
$$

其泛化形式是：

$$
\begin{bmatrix}
v_1
\\
v_2
\\
\vdots
\\
v_n
\end{bmatrix}
=\
\begin{bmatrix}
R_1
\\
R_2
\\
\vdots
\\
R_n
\end{bmatrix}
+\gamma
\begin{bmatrix}
P_{11} & P_{12} & \cdots & P_{1n}
\\
P_{21} & P_{22} & \cdots & P_{2n}
\\
\vdots & \vdots & \ddots & \vdots
\\
P_{n1} & P_{n2} & \cdots & P_{nn}
\end{bmatrix}
\begin{bmatrix}
v_1
\\
v_2
\\
\vdots
\\
v_n
\end{bmatrix}
\tag{7.5.4}
$$

但是从式（7.3.13）看，式（7.5.3）等式右侧的 $V(s)$ 应该是 $V(s')$ 才对，但是经过上述的实例化推导，希望读者可以理解到：

- 所谓的 $s'$ 是在时间维度上的定义，表示下一步的状态 $s'$ 的状态值；
- 而在空间上，由于状态值一旦确定就不会变化，$v(s)=v(s')$ 。

比如：
- 式（7.4.1.1），$v_0=-3+0.2v_0+0.8v_1$ 中：
    $V(s)=\{v_0\},V(s')=\{v_0,v_1\}$，$s_1$ 是 $s_0$ 的后续状态。
- 式（7.4.1.2），$v_1=0.6v_0+0.4v_2$ 中：
    $V(s)=\{v_1\},V(s')=\{v_0,v_2\}$，$s_0$ 是 $s_1$ 的后续状态。

两者在不同的马尔可夫过程中互为后续状态，但是 $v_0,v_1$ 这两个值不论在式（7.5.3）的等式左侧还是右侧，各自都应该代表同一个变量。

式（7.5.3）可以变形，并最终解出 $V(s)$：

$$
\begin{aligned}
V(s) &= R_s+ \gamma P_{ss'}V(s)
\\
V(s) - \gamma P_{ss'}V(s) &= R(s)
\\
(I-\gamma P_{ss'})V(s)&=R(s), &(I \ 是对角矩阵)
\\
V(s)&=(I-\gamma P_{ss'})^{-1}R(s) &(矩阵没有除法，但可以求逆)
\end{aligned}
\tag{7.5.5}
$$

式（7.5.5）中，等式右侧的值都是已知的，所以可以直接解出 $V(s)$ 的数学解析解。

### 7.5.2 代码实现

【代码位置：LifeCycle_0_DataModel.py】

定义状态转移矩阵：

```python
# 状态转移概率
P = np.array(
    [   # B   C    T    R    F    M    E    
        [0.2, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0],    # Bug 
        [0.6, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0],    # Coding
        [0.0, 0.1, 0.0, 0.9, 0.0, 0.0, 0.0],    # Test (CI)
        [0.0, 0.1, 0.0, 0.0, 0.2, 0.7, 0.0],    # Review
        [0.0, 0.2, 0.5, 0.3, 0.0, 0.0, 0.0],    # reFactor
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0],    # Merge
        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]     # End
    ]
)
```

定义奖励函数值向量：
```python
# 奖励向量 缺陷 编码 测试 审查 重构 合并 结束
Rewards = [-3, 0,   +1, +3,  +2, -1,  0]
```

式（7.5.5）的具体实现：
    
```python 
def solve_matrix(dataModel, gamma):
    # 在对角矩阵上增加一个微小的值来解决奇异矩阵不可求逆的问题
    I = np.eye(dataModel.N) * (1+1e-7)
    # I = np.eye(dataModel.N) # 非奇异矩阵时使用此行代码以提高计算精度
    factor = I - gamma * dataModel.P
    inv_factor = np.linalg.inv(factor)  # 求矩阵的逆
    vs = np.dot(inv_factor, dataModel.R)
    return vs
```
在定义状态转移矩阵时，右下角的值，即从 $S_{End} \to S_{End}$ 的转移概率，即可以写成 0.0，也可以写成 1.0，从强化学习的概念出发，都没有错。但是却与代码处理逻辑有关。

- 写成 0.0 时，np.random.choice(p=) 函数由于概率之和不为 1，所以函数调用出错。
- 写成 1.0 时，由于矩阵的行列式为 0，是个奇异矩阵，不可求逆。此时可以在对角矩阵上增加一个微小的值来解决奇异矩阵不可求逆的问题，但是最终结果会有 1e-7 的误差，可以接受。

以下是用矩阵法的计算结果：

```
状态价值函数计算结果(数组) : [-5.99 -2.24  3.38  2.9   4.11 -1.    0.  ]
Bug:        -5.99
Coding:     -2.24
Test:       3.38
Review:     2.9
Refactor:   4.11
Merge:      -1.0
End:        0.0
```

可能有读者好奇：如果保留更多的小数点后位数的话，用矩阵法得到的结果，与式（7.4.3）相比，哪一个更准确？

答案是：式（7.4.3）更准确。原因是用代码求矩阵的逆时，由于具体实现的问题有一些误差，否则的话两者应该完全相等。

