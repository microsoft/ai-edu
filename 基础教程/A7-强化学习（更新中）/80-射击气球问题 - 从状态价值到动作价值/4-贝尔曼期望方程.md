## 8.4 贝尔曼期望方程

有了模型和奖励，就可以进一步地研究价值问题了。如同马尔可夫奖励过程中的状态价值函数一样，在马尔可夫决策过程过程中，同样会有价值函数，而且概念进一步地深化和扩展了。

### 8.4.1 MRP 和 MDP 的比较

我们首先用图 8.4.1 来比较一下两种过程的模型，避免概念混淆。

<center>
<img src="./img/MDPvsMRP.png">

图 8.4.1 马尔可夫奖励过程和马尔可夫决策过程模型的比较
（左侧：马尔可夫奖励过程 MRP 模型；右侧：马尔可夫决策过程 MDP 模型。）
</center>

表 8.4.1 比较图 8.4.1 中的左右两部分

||左侧|右侧|
|-|-|-|
|名称|马尔科夫奖励过程 MRP|马尔可夫决策过程 MDP
|模型|两层节点，一层过程|三层节点，两层过程|
|上层节点|源状态|源状态|
|中层节点|无|动作节点|
|下层节点|目标状态|目标状态|
|过程|实线箭头为状态转移 $P_{ss'}$，以及过程奖励 $R_{ss'}$|实线箭头为策略选择$\pi(a \mid s)$，虚线箭头为状态转移$P^a_{ss'}$，<br>以及过程奖励$R^a_{ss'}$|
|解法|贝尔曼方程|贝尔曼期望方程|
|状态价值函数定义| $ v(s)= \mathbb E [G_t \mid S_t=s]$ | $v_\pi(s,a)=\mathbb E[G_t \mid S_t=s]$|
|动作价值函数定义|无|$q_\pi(s,a)=\mathbb E[G_t \mid S_t=s,A_t=a]$|

- MDP 中状态价值函数 $v_\pi(s)$ 比 MRP 中的 $v(s)$ 多了一个下标 $\pi$，是为了区分二者，没有实际的数学含义。

- 在有的资料中，给贝尔曼期望方程的 $\mathbb E$ 写作 $\mathbb E_\pi$，也是这个意思，没有实际的数学含义。

- 在 MRP 中，每个状态节点都有价值函数，在第 7 章中，我们用**贝尔曼方程**可以计算出这些节点的函数值。

- 在 MDP 中，每个状态节点和每个动作节点都有价值函数，在本章中，我们将学习用**贝尔曼期望方程**来计算它们。


### 8.4.2 动作价值函数 $q_\pi$

MDP 中的动作价值函数 $Q_\pi$，可以简称为 **Q 函数**。我们在这里用小写的 $q_\pi(s,a)$ 表示在策略 $\pi(a|s)$ 下的动作 $a$ 的价值的一个实例，用大写的 $Q_\pi$ 来表示所有所有实例的集合，即 Q 表格（表 8.4.3）。

它的实际含义和马尔科夫奖励过程过程 MRP 中的状态价值函数 $V$ 是可以类比的，其含义甚至是相等的。这个说法听上去比较奇怪：为什么是 $Q_\pi$ 函数和 MRP 中的 $V$ 函数类比，而不是 8.4.3 节中要讲的 $V_\pi$ 函数和 MRP 中的 $V$ 函数类比呢？这与它们在模型中的位置以及定义有关。

先回忆一下在马尔可夫奖励过程 MRP 中学习过的状态价值函数 $V$ 的表达式（7.3.13），温故而知新：

$$
\begin{aligned}
v(s) &\doteq \mathbb E [G_t \mid S_t = s]
\\
&=\mathbb E [R_{t+1}\mid S_t=s] + \gamma \mathbb E[G_{t+1}\mid S_t=s] &(0)
\\
&=\sum_{s'} p_{ss'} r_{ss'}+ \gamma \sum_{s'} p_{ss'}v(s') =\sum_{s'} p_{ss'} [r_{ss'}+\gamma v(s')] &(1)
\\
&= P_{ss'} R_{ss'} + \gamma P_{ss'} V(s')=P_{ss'}[R_{ss'} + \gamma V(s')] &(2)
\\
&= R(s)+ \gamma P_{ss'}V(s') &(3)
\end{aligned}
\tag{8.4.1}
$$

式（8.4.1）即式（7.3.13），它对应了图 8.4.1 中的左侧部分。再比较图 8.4.1 中左侧和右侧的黄色虚线框内的部分，可以说除了符号定义不同，其它都是相同的，包括位置和含义。

表 8.4.2 比较图 8.4.1 中的左右两部分的黄色虚线框内的部分

||左侧|右侧|
|-|-|-|
|顶端节点|源状态 $s_0$，需要计算状态价值函数$v(s_0)$|源动作 $a_1$，需要计算动作价值函数$q_{\pi}(s_0,a_1)$|
|中间过程|状态转移概率$P_{ss'}$，过程奖励向量$R_{ss'}$|状态转移概率$P_{ss'}^{a_1}$，过程奖励向量$R_{ss'}^{a_1}$|
|底端节点|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v(s')$|下游状态$s_1,s_2,s_3$，假设已知状态价值函数$v_{\pi}(s')$|

观察式（8.4.1）贝尔曼方程的推导，其本质是：在 MRP 中，某个状态的价值函数 $v(s)$ 由三部分组成：
1. 其下游状态的价值 $v(s')$；
2. 转移概率 $p$；
2. 转移过程中的奖励 $r$。

在下面推导 MDP 中的**贝尔曼期望方程**的动作价值函数 $q_\pi$ 的公式时，我们也遇到了和式 8.4.1 同样的表达。所以，可以大胆地预测，计算动作价值函数 $q_\pi(s,a)$ 的**贝尔曼期望方程**与计算状态价值函数 $v(s)$ 的**贝尔曼方程**完全一致。

<center>
<img src="./img/mdp-Q.png">

图 8.4.2 马尔科夫决策过程的 $Q$ 函数模型
</center>

在图 8.4.2 中，我们绘制出了 $Q$ 函数模型所需要的所有元素（实际上是图 8.3.4 状态转移模型的一个补充），其中的 $V_\pi(s')$ 也可以写成 $V_\pi(S_{t+1})$，折扣 $\gamma$ 没有在图中画出来。

$Q$ 函数定义如下：

$$
\begin{aligned}
q_\pi(s,a) & \doteq \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=\mathbb E[ R_{t+1}+\gamma v_\pi(S_{t+1}) \mid S_t=s, A_t=a]
\\
&=\mathbb E [R_{t+1}\mid S_t=s, A_t=a] + \gamma \mathbb E[G_{t+1}\mid S_t=s, A_t=a]
\end{aligned}
\tag{8.4.2}
$$

先暂停一下，看看式（8.4.2）和式（8.4.1.0）的区别，除了在条件部分多出来一个 $A_t=a$ 以外，其它的部分完全相同。那么多出来的这个 $A_t=a$ 会造成什么不同吗？答案是不会。因为这个条件相当于在图 8.4.1 中右侧的部分确定了是选择 $a_1$ 还是 $a_2$，正是因为有了这个条件存在，才会让左右两个黄色虚线框部分的模型结构高度相似。

所以，虽然这二者的含义不同（前者是简单的 MRP 状态价值函数，后者是策略下的 MDP 动作价值函数），但是它们的表达式应该相同。所以，我们直接把式（8.4.1）的结果拿来就行了。继续推导式（8.4.2）：

$$
\begin{aligned}
q_\pi(s,a) & \doteq \mathbb E [G_t \mid S_t = s, A_t=a]
\\
&=\mathbb E[ R_{t+1}+\gamma v_\pi(S_{t+1}) \mid S_t=s, A_t=a]
\\
&=\mathbb E [R_{t+1}\mid S_t=s, A_t=a] + \gamma \mathbb E[G_{t+1}\mid S_t=s, A_t=a]
\\
&=\sum_{s'} p_{ss'}^a r_{ss'}^a+ \gamma \sum_{s'} p_{ss'}^a v_\pi(s') =\sum_{s'} p_{ss'}^a [r_{ss'}^a+\gamma v_\pi(s')] &(1)
\\
&=P^a_{ss'} R^a_{ss'} + \gamma P^a_{ss'} V_\pi(s')=P^a_{ss'}[R^a_{ss'}+\gamma V_\pi(s')] &(2)
\\
&= R(s,a)+ \gamma P_{ss'}^a V_\pi(s')  &(3)
\end{aligned}
\tag{续8.4.2}
$$

式（8.4.2）中有三个子式：
- 子式（1），求和形式，用于给出了原始数据的计算方法；
- 子式（2），矩阵形式，便于数学描述；
- 子式（3），适用于奖励设置在动作节点上的情况，但是比较少见。

其中，$R(s,a)=P^a_{ss'}R^a_{ss'}=\sum_{s'} p_{ss'}^a r_{ss'}^a$。

最后计算得到的所有状态-动作的 Q 函数是一个表格形式，如表 8.4.3 所示。

表 8.4.3 表格形式的 Q 函数

|状态 $\to$ 动作|$a_0$|$a_1$|$a_2$|$\cdots$|
|:-:|:-:|:-:|:-:|-|
|$s_0$|$q_\pi(s_0,a_0)$|$q_\pi(s_0,a_1)$|$q_\pi(s_0,a_2)$|$\cdots$|
|$s_1$|$q_\pi(s_1,a_0)$|$q_\pi(s_1,a_1)$|$q_\pi(s_1,a_2)$|$\cdots$|
|$s_2$|$q_\pi(s_2,a_0)$|$q_\pi(s_2,a_1)$|$q_\pi(s_2,a_2)$|$\cdots$|
|$s_3$|$q_\pi(s_3,a_0)$|$q_\pi(s_3,a_1)$|$q_\pi(s_3,a_2)$|$\cdots$|
|$\vdots$|$\vdots$|$\vdots$|$\vdots$||

该表中会填满数字，从而可以进行横向比较，即在某个状态 $s$ 下的各个动作的价值大小，进而评价策略 $\pi$ 的优略。不同的 $s$ 下的动作是不可比的。


### 8.4.3 状态价值函数 $v_\pi$

状态价值函数，可以简称为 **V 函数**，用 $v_\pi(s)$ 表示在策略 $\pi(a|s)$ 下的状态 $s$ 的价值。

<center>
<img src="./img/mdp-V.png">

图 8.4.3 马尔科夫决策过程的 V 函数模型
</center>

在图 8.4.3 中，包含了计算 V 函数所需要的所有元素（实际上是图 8.3.3 策略选择模型的一个补充），定义如下：

$$
\begin{aligned}
v_\pi(s) &\doteq \mathbb E [G_t \mid S_t=s] 
\\
&=\mathbb E [R_{t+1}+\gamma G_{t+1} \mid S_t=s]
\end{aligned}
\tag{8.4.3}
$$

同前面一样，式（8.4.3）仍然是要求回报 $G_t$ 的数学期望。但是状态 $s$ 并不直接接触到奖励机制，而是通过策略 $\pi$ 与下游的两个动作 $a_1,a_2$ 连接，所以想直接得到 $R_{t+1}$ 和 $G_{t+1}$ 是不可能的。

不如我们换一个思路：

- 结合模型 8.4.2 和式（8.4.2）来看，Q 函数是由它的下游 V 函数组成的；
- 再看模型 8.4.3，那么 V 函数当然也是由它的下游 Q 函数组成的。


所以，一旦知道了 $a_1,a_2$ 的动作价值函数 $q_\pi$，那么 $s$ 的状态价值函数就可以表示为 $q_\pi$ 的期望了。

好，继续推导式（8.4.3）：

$$
\begin{aligned}
v_\pi(s) &\doteq \mathbb E [G_t \mid S_t=s] 
\\
&=\mathbb E [R_{t+1}+\gamma G_{t+1} \mid S_t=s]
\\
&=\pi(a_1|s) q_\pi(s,a_1) + \pi(a_2|s) q_\pi(s,a_2)
\\
&=\sum_{a \in A(s)} \pi(a | s) q_\pi(s,a)
\\
&=\pi(s) \cdot Q_\pi(s)
\end{aligned}
\tag{续8.4.3}
$$

其中 $\pi(s)$ 和 $Q_\pi(s)$ 都代表一个矢量，即适量点乘得到标量 $v_\pi(s)$。

到此为止，我们在假设已知下游 V 函数的情况下，得到了 Q 函数的表达式（8.4.2），又在假设已知下游 Q 函数的情况下，得到了 V 函数的表达式（8.4.3）。读者不禁会产生疑问，这不是就如同武侠小说中所描述的那样：只见那位大侠原地跳起一丈多高，然后在空中左脚（V 函数）一蹬右脚（Q 函数）面，使了一个梯云纵，又上窜了一丈多高......

这个，可以实现吗？这个问题我们在 8.6 节中解决。

